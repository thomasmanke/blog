[
  {
    "objectID": "lectures/Rintro2023/02_HighDimData.html",
    "href": "lectures/Rintro2023/02_HighDimData.html",
    "title": "02: Higher Dimensional Data",
    "section": "",
    "text": "Goal:\nReal data is much more complex than simple vectors of numbers or strings.\nHere we introduce new data structures, important functions, and some more jargon."
  },
  {
    "objectID": "lectures/Rintro2023/02_HighDimData.html#data",
    "href": "lectures/Rintro2023/02_HighDimData.html#data",
    "title": "02: Higher Dimensional Data",
    "section": "data()",
    "text": "data()\nR comes with many pre-defined data sets. They are often used to illustrate statistical problems, data science concepts and functionality of software packages. Just type data() to get an overview.\n\n\nCode\ndata()\n\n\nOne particularly famous data set is iris. We will use it throughout this course:\n\n\n\nFlower Measurements\n\n\n(Image from blog of mathieu.guillame-bert.com)\nTasks: Explore the iris data set\n\nTry ?iris to learn more about this data set.\nType iris or View(iris). Why is this not the best way to explore data?\n\nQuery:\nHow many samples and variables does this data set contain?"
  },
  {
    "objectID": "lectures/Rintro2023/02_HighDimData.html#data-frames",
    "href": "lectures/Rintro2023/02_HighDimData.html#data-frames",
    "title": "02: Higher Dimensional Data",
    "section": "Data Frames",
    "text": "Data Frames\nThis iris data object is more complex than simple vectors discussed before.\nTypical commands to explore such data objects are:\n\n\nCode\nstr(iris)      # the structure\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\nhead(iris)     # the first few lines\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nCode\ntail(iris,n=3) # last n lines\n\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\nCode\nclass(iris)    # data frame is a \"class\"\n\n\n[1] \"data.frame\"\n\n\nCode\nsummary(iris)  # works well with data_frames\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nCode\ndim(iris)      # data frames ar 2D objects: rows x columns\n\n\n[1] 150   5\n\n\nData Frames are built from vectors with identical lengths.\n\n\n\nImage adopted from Michael Stadler (FMI)"
  },
  {
    "objectID": "lectures/Rintro2023/02_HighDimData.html#data-frame-access",
    "href": "lectures/Rintro2023/02_HighDimData.html#data-frame-access",
    "title": "02: Higher Dimensional Data",
    "section": "Data Frame Access",
    "text": "Data Frame Access\nProgrammatic access to specific subsets of data is key to all analyses.\nThere are many basic ways to achieve this - and even more in later sections.\n\n\nCode\nd=iris    # just to reduce typing\n\nir=1:3    # vector of row indices\nic=c(2,5) # vector of col indices\nd[ir,ic]  # selecting specific elements in 2D data frame\n\n\n  Sepal.Width Species\n1         3.5  setosa\n2         3.0  setosa\n3         3.2  setosa\n\n\nCode\nir = c(7,20,19,69)   # just some arbitrary choice of indices\nd[ir,\"Species\"]      # access by index and column by name\n\n\n[1] setosa     setosa     setosa     versicolor\nLevels: setosa versicolor virginica\n\n\nCode\nd$Species[ir]        # the same, more common \n\n\n[1] setosa     setosa     setosa     versicolor\nLevels: setosa versicolor virginica\n\n\nCode\nd[,-5]               # unselecting column 5\n\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width\n1            5.1         3.5          1.4         0.2\n2            4.9         3.0          1.4         0.2\n3            4.7         3.2          1.3         0.2\n4            4.6         3.1          1.5         0.2\n5            5.0         3.6          1.4         0.2\n6            5.4         3.9          1.7         0.4\n7            4.6         3.4          1.4         0.3\n8            5.0         3.4          1.5         0.2\n9            4.4         2.9          1.4         0.2\n10           4.9         3.1          1.5         0.1\n11           5.4         3.7          1.5         0.2\n12           4.8         3.4          1.6         0.2\n13           4.8         3.0          1.4         0.1\n14           4.3         3.0          1.1         0.1\n15           5.8         4.0          1.2         0.2\n16           5.7         4.4          1.5         0.4\n17           5.4         3.9          1.3         0.4\n18           5.1         3.5          1.4         0.3\n19           5.7         3.8          1.7         0.3\n20           5.1         3.8          1.5         0.3\n21           5.4         3.4          1.7         0.2\n22           5.1         3.7          1.5         0.4\n23           4.6         3.6          1.0         0.2\n24           5.1         3.3          1.7         0.5\n25           4.8         3.4          1.9         0.2\n26           5.0         3.0          1.6         0.2\n27           5.0         3.4          1.6         0.4\n28           5.2         3.5          1.5         0.2\n29           5.2         3.4          1.4         0.2\n30           4.7         3.2          1.6         0.2\n31           4.8         3.1          1.6         0.2\n32           5.4         3.4          1.5         0.4\n33           5.2         4.1          1.5         0.1\n34           5.5         4.2          1.4         0.2\n35           4.9         3.1          1.5         0.2\n36           5.0         3.2          1.2         0.2\n37           5.5         3.5          1.3         0.2\n38           4.9         3.6          1.4         0.1\n39           4.4         3.0          1.3         0.2\n40           5.1         3.4          1.5         0.2\n41           5.0         3.5          1.3         0.3\n42           4.5         2.3          1.3         0.3\n43           4.4         3.2          1.3         0.2\n44           5.0         3.5          1.6         0.6\n45           5.1         3.8          1.9         0.4\n46           4.8         3.0          1.4         0.3\n47           5.1         3.8          1.6         0.2\n48           4.6         3.2          1.4         0.2\n49           5.3         3.7          1.5         0.2\n50           5.0         3.3          1.4         0.2\n51           7.0         3.2          4.7         1.4\n52           6.4         3.2          4.5         1.5\n53           6.9         3.1          4.9         1.5\n54           5.5         2.3          4.0         1.3\n55           6.5         2.8          4.6         1.5\n56           5.7         2.8          4.5         1.3\n57           6.3         3.3          4.7         1.6\n58           4.9         2.4          3.3         1.0\n59           6.6         2.9          4.6         1.3\n60           5.2         2.7          3.9         1.4\n61           5.0         2.0          3.5         1.0\n62           5.9         3.0          4.2         1.5\n63           6.0         2.2          4.0         1.0\n64           6.1         2.9          4.7         1.4\n65           5.6         2.9          3.6         1.3\n66           6.7         3.1          4.4         1.4\n67           5.6         3.0          4.5         1.5\n68           5.8         2.7          4.1         1.0\n69           6.2         2.2          4.5         1.5\n70           5.6         2.5          3.9         1.1\n71           5.9         3.2          4.8         1.8\n72           6.1         2.8          4.0         1.3\n73           6.3         2.5          4.9         1.5\n74           6.1         2.8          4.7         1.2\n75           6.4         2.9          4.3         1.3\n76           6.6         3.0          4.4         1.4\n77           6.8         2.8          4.8         1.4\n78           6.7         3.0          5.0         1.7\n79           6.0         2.9          4.5         1.5\n80           5.7         2.6          3.5         1.0\n81           5.5         2.4          3.8         1.1\n82           5.5         2.4          3.7         1.0\n83           5.8         2.7          3.9         1.2\n84           6.0         2.7          5.1         1.6\n85           5.4         3.0          4.5         1.5\n86           6.0         3.4          4.5         1.6\n87           6.7         3.1          4.7         1.5\n88           6.3         2.3          4.4         1.3\n89           5.6         3.0          4.1         1.3\n90           5.5         2.5          4.0         1.3\n91           5.5         2.6          4.4         1.2\n92           6.1         3.0          4.6         1.4\n93           5.8         2.6          4.0         1.2\n94           5.0         2.3          3.3         1.0\n95           5.6         2.7          4.2         1.3\n96           5.7         3.0          4.2         1.2\n97           5.7         2.9          4.2         1.3\n98           6.2         2.9          4.3         1.3\n99           5.1         2.5          3.0         1.1\n100          5.7         2.8          4.1         1.3\n101          6.3         3.3          6.0         2.5\n102          5.8         2.7          5.1         1.9\n103          7.1         3.0          5.9         2.1\n104          6.3         2.9          5.6         1.8\n105          6.5         3.0          5.8         2.2\n106          7.6         3.0          6.6         2.1\n107          4.9         2.5          4.5         1.7\n108          7.3         2.9          6.3         1.8\n109          6.7         2.5          5.8         1.8\n110          7.2         3.6          6.1         2.5\n111          6.5         3.2          5.1         2.0\n112          6.4         2.7          5.3         1.9\n113          6.8         3.0          5.5         2.1\n114          5.7         2.5          5.0         2.0\n115          5.8         2.8          5.1         2.4\n116          6.4         3.2          5.3         2.3\n117          6.5         3.0          5.5         1.8\n118          7.7         3.8          6.7         2.2\n119          7.7         2.6          6.9         2.3\n120          6.0         2.2          5.0         1.5\n121          6.9         3.2          5.7         2.3\n122          5.6         2.8          4.9         2.0\n123          7.7         2.8          6.7         2.0\n124          6.3         2.7          4.9         1.8\n125          6.7         3.3          5.7         2.1\n126          7.2         3.2          6.0         1.8\n127          6.2         2.8          4.8         1.8\n128          6.1         3.0          4.9         1.8\n129          6.4         2.8          5.6         2.1\n130          7.2         3.0          5.8         1.6\n131          7.4         2.8          6.1         1.9\n132          7.9         3.8          6.4         2.0\n133          6.4         2.8          5.6         2.2\n134          6.3         2.8          5.1         1.5\n135          6.1         2.6          5.6         1.4\n136          7.7         3.0          6.1         2.3\n137          6.3         3.4          5.6         2.4\n138          6.4         3.1          5.5         1.8\n139          6.0         3.0          4.8         1.8\n140          6.9         3.1          5.4         2.1\n141          6.7         3.1          5.6         2.4\n142          6.9         3.1          5.1         2.3\n143          5.8         2.7          5.1         1.9\n144          6.8         3.2          5.9         2.3\n145          6.7         3.3          5.7         2.5\n146          6.7         3.0          5.2         2.3\n147          6.3         2.5          5.0         1.9\n148          6.5         3.0          5.2         2.0\n149          6.2         3.4          5.4         2.3\n150          5.9         3.0          5.1         1.8"
  },
  {
    "objectID": "lectures/Rintro2023/02_HighDimData.html#conditional-access",
    "href": "lectures/Rintro2023/02_HighDimData.html#conditional-access",
    "title": "02: Higher Dimensional Data",
    "section": "Conditional Access",
    "text": "Conditional Access\nUsually we want to access (or subset) data if certain conditions are met\n\n\nCode\n# select only species \"setosa\"\nib = d$Species == \"setosa\" \nd[ib,]\n\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n11          5.4         3.7          1.5         0.2  setosa\n12          4.8         3.4          1.6         0.2  setosa\n13          4.8         3.0          1.4         0.1  setosa\n14          4.3         3.0          1.1         0.1  setosa\n15          5.8         4.0          1.2         0.2  setosa\n16          5.7         4.4          1.5         0.4  setosa\n17          5.4         3.9          1.3         0.4  setosa\n18          5.1         3.5          1.4         0.3  setosa\n19          5.7         3.8          1.7         0.3  setosa\n20          5.1         3.8          1.5         0.3  setosa\n21          5.4         3.4          1.7         0.2  setosa\n22          5.1         3.7          1.5         0.4  setosa\n23          4.6         3.6          1.0         0.2  setosa\n24          5.1         3.3          1.7         0.5  setosa\n25          4.8         3.4          1.9         0.2  setosa\n26          5.0         3.0          1.6         0.2  setosa\n27          5.0         3.4          1.6         0.4  setosa\n28          5.2         3.5          1.5         0.2  setosa\n29          5.2         3.4          1.4         0.2  setosa\n30          4.7         3.2          1.6         0.2  setosa\n31          4.8         3.1          1.6         0.2  setosa\n32          5.4         3.4          1.5         0.4  setosa\n33          5.2         4.1          1.5         0.1  setosa\n34          5.5         4.2          1.4         0.2  setosa\n35          4.9         3.1          1.5         0.2  setosa\n36          5.0         3.2          1.2         0.2  setosa\n37          5.5         3.5          1.3         0.2  setosa\n38          4.9         3.6          1.4         0.1  setosa\n39          4.4         3.0          1.3         0.2  setosa\n40          5.1         3.4          1.5         0.2  setosa\n41          5.0         3.5          1.3         0.3  setosa\n42          4.5         2.3          1.3         0.3  setosa\n43          4.4         3.2          1.3         0.2  setosa\n44          5.0         3.5          1.6         0.6  setosa\n45          5.1         3.8          1.9         0.4  setosa\n46          4.8         3.0          1.4         0.3  setosa\n47          5.1         3.8          1.6         0.2  setosa\n48          4.6         3.2          1.4         0.2  setosa\n49          5.3         3.7          1.5         0.2  setosa\n50          5.0         3.3          1.4         0.2  setosa\n\n\nCode\n# select species \"setosa\" and \"small Sepal.Width\")\nib=(d$Species==\"setosa\") & (d$Sepal.Width&lt;3)\nd[ib,]\n\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n9           4.4         2.9          1.4         0.2  setosa\n42          4.5         2.3          1.3         0.3  setosa\n\n\nTask:\n\nLook at the vector ib in the above examples and understand what is meant with “access by truth value”.\nModify the criteria and try your own selections."
  },
  {
    "objectID": "lectures/Rintro2023/02_HighDimData.html#adding-data",
    "href": "lectures/Rintro2023/02_HighDimData.html#adding-data",
    "title": "02: Higher Dimensional Data",
    "section": "Adding Data",
    "text": "Adding Data\n\n\nCode\nd$x = 1:3   # simple, but useless\nhead(d,10)\n\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species x\n1           5.1         3.5          1.4         0.2  setosa 1\n2           4.9         3.0          1.4         0.2  setosa 2\n3           4.7         3.2          1.3         0.2  setosa 3\n4           4.6         3.1          1.5         0.2  setosa 1\n5           5.0         3.6          1.4         0.2  setosa 2\n6           5.4         3.9          1.7         0.4  setosa 3\n7           4.6         3.4          1.4         0.3  setosa 1\n8           5.0         3.4          1.5         0.2  setosa 2\n9           4.4         2.9          1.4         0.2  setosa 3\n10          4.9         3.1          1.5         0.1  setosa 1"
  },
  {
    "objectID": "lectures/Rintro2023/05_DataModels.html",
    "href": "lectures/Rintro2023/05_DataModels.html",
    "title": "05: Data Modeling",
    "section": "",
    "text": "Recap: All-Against-All Correlations\nTask: remove the Species variable from “iris” and store the result in a new data.frame “niris”\n\n\nCode\nniris=iris[,-5]  # generate new data frame without species variable\nstr(niris)\n\n\n'data.frame':   150 obs. of  4 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n\n\nTask: Generate all-against-all correlation plot\n\n\nCode\n# assign species-colors to each observation \ncols = iris$Species                        # understand how color is defined\npairs(niris, col=cols, lower.panel=NULL)   # \"cols\" was defined in task above\n\n\n\n\n\n\n\n\nFrom Correlations to Models\nGoal:\nModel some dependent variable y as function of other explanatory variables x (features)\n\\[\ny = f(\\theta, x) = \\theta_1 x +  \\theta_0\n\\]\nFor \\(N\\) data points, choose parameters \\(\\theta\\) by ordinary least squares:\n\\[\nRSS=\\sum_{i=1}^{N} (y_i - f(\\theta, x_i))^2 \\to min\n\\]\nEasy in R:\n\n\nCode\nplot(Petal.Width ~ Petal.Length, data=iris, col=Species) # use model (\"formula\") notation\nfit=lm(Petal.Width ~ Petal.Length, data=iris)       # fit a linear model\nabline(fit, lwd=3, lty=2)                           # add regression line\n\n\n\n\n\nQuery: What class is the object fit?\nTask: Extract the coefficients of the fitted line.\n\n\n (Intercept) Petal.Length \n  -0.3630755    0.4157554 \n\n\n (Intercept) Petal.Length \n  -0.3630755    0.4157554 \n\n\n\n\nReporting the fit (model)\n\n\nCode\nsummary(fit)        # summary() behaves differently for fit objects\n\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncoefficients(fit)   # more functions for specific elements\n\n\n (Intercept) Petal.Length \n  -0.3630755    0.4157554 \n\n\nCode\nconfint(fit)        # Try to change the confidence level: ?confint\n\n\n                  2.5 %     97.5 %\n(Intercept)  -0.4416501 -0.2845010\nPetal.Length  0.3968193  0.4346915\n\n\nThis is a good fit as suggested by a\n\nsmall residual standard error\na large coefficient of variation \\(R^2\\)\na small p-value\nand by visualization\n\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_i(y_i - y(\\theta,x_i))^2}{\\sum_i(y_i-\\bar{y})^2}\n\\] There are manny more methods to access information for the lm class\n\n\nCode\nmethods(class='lm')\n\n\n [1] add1           alias          anova          case.names     coerce        \n [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n[11] drop1          dummy.coef     effects        extractAIC     family        \n[16] formula        hatvalues      influence      initialize     kappa         \n[21] labels         logLik         model.frame    model.matrix   nobs          \n[26] plot           predict        print          proj           qr            \n[31] residuals      rstandard      rstudent       show           simulate      \n[36] slotsFromS3    summary        variable.names vcov          \nsee '?methods' for accessing help and source code\n\n\n\n\nPredictions (with confidence intervals)\n\n\nCode\nx=iris$Petal.Length                       # explanatory variable from fit (here:Petal.Length)\nxn=seq(min(x), max(x), length.out = 100)  # define range of new explanatory variables\nndf=data.frame(Petal.Length=xn)           # put them into new data frame\n\np=predict(fit, ndf, interval = 'confidence' , level = 0.95)\nplot(Petal.Width ~ Petal.Length, data=iris, col=Species)\nlines(xn, p[,\"lwr\"] )\nlines(xn, p[,\"upr\"] )\n\n#some fancy filling\npolygon(c(rev(xn), xn), c(rev(p[ ,\"upr\"]), p[ ,\"lwr\"]), col = rgb(1,0,0,0.5), border = NA)\n\n\n\n\n\nCode\n## using ggplot2 - full introduction later\n#library(ggplot2)\n#g = ggplot(iris, aes(Petal.Length, Petal.Width, colour=Species))\n#g + geom_point() + geom_smooth(method=\"lm\", se=TRUE, color=\"red\") + geom_smooth(method=\"loess\", colour=\"blue\")\n\n\n\n\nPoor Fit\nJust replace “Petal” with “Sepal”\n\n\nCode\nplot(Sepal.Width ~ Sepal.Length, data=iris, col=cols)  \nfit1=lm(Sepal.Width ~ Sepal.Length, data=iris)     \nabline(fit1, lwd=3, lty=2)    \n\n\n\n\n\nCode\nconfint(fit1)                     # estimated slope is indistinguishable from zero\n\n\n                  2.5 %     97.5 %\n(Intercept)   2.9178767 3.92001694\nSepal.Length -0.1467928 0.02302323\n\n\nCode\nsummary(fit1)\n\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1095 -0.2454 -0.0167  0.2763  1.3338 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.41895    0.25356   13.48   &lt;2e-16 ***\nSepal.Length -0.06188    0.04297   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4343 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\n\nInterpretation: slope is not significantly distinct from 0.\nTask: Use the above template to make predictions for the new poor fit.\n\n\n\n\n\n\n\nFactorial variables as predictors\nIn the iris example the “Species” variable is a factorial (categorical) variable with 3 levels. Other typical examples: different experimental conditions or treatments.\n\n\nCode\nplot(Petal.Width ~ Species, data=iris)\n\n\n\n\n\nCode\nfit=lm(Petal.Width ~ Species, data=iris)\nsummary(fit)\n\n\n\nCall:\nlm(formula = Petal.Width ~ Species, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.626 -0.126 -0.026  0.154  0.474 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.24600    0.02894    8.50 1.96e-14 ***\nSpeciesversicolor  1.08000    0.04093   26.39  &lt; 2e-16 ***\nSpeciesvirginica   1.78000    0.04093   43.49  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2047 on 147 degrees of freedom\nMultiple R-squared:  0.9289,    Adjusted R-squared:  0.9279 \nF-statistic:   960 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\n“setosa” (1st species) has mean Petal.Width=0.246(29) - reference baseline\n“versicolor” (2nd species) has mean Petal.Width = Petal.Width(setosa) + 1.08(4)\n“virginica” (3rd species) has mean Petal.Width = Petal.Width(setosa) + 1.78(4)\n\n\n\nAnova\nsummary(fit) contains information on the individual coefficients. They are difficult to interpret\n\n\nCode\nanova(fit)    \n\n\nAnalysis of Variance Table\n\nResponse: Petal.Width\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals 147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: variable “Species” accounts for much variation in “Petal.Width”\n\n\n\nMore complicated models\nDetermine residual standard error sigma for different fits with various complexity\n\n\nCode\nfit=lm(Petal.Width ~ Petal.Length, data=iris)\npaste(toString(fit$call), sigma(fit))\n\n\n[1] \"lm, Petal.Width ~ Petal.Length, iris 0.206484348913609\"\n\n\nCode\nfit=lm(Petal.Width ~ Petal.Length + Sepal.Length, data=iris)  # function of more than one variable\npaste(toString(fit$call), sigma(fit))\n\n\n[1] \"lm, Petal.Width ~ Petal.Length + Sepal.Length, iris 0.204445704742963\"\n\n\nCode\nfit=lm(Petal.Width ~ Species, data=iris)                      # function of categorical variables\npaste(toString(fit$call), sigma(fit))\n\n\n[1] \"lm, Petal.Width ~ Species, iris 0.204650024805914\"\n\n\nCode\nfit=lm(Petal.Width ~ . , data=iris)                           # function of all other variable (numerical and categorical)\npaste(toString(fit$call), sigma(fit))\n\n\n[1] \"lm, Petal.Width ~ ., iris 0.166615943019283\"\n\n\n… more complex models tend to have smaller residual standard error (overfitting?)\n\n\n\nModel Checking: Diagnostic Plots\n“fit” is a large object of the lm-class which contains also lots of diagnostic informmation. Notice how the behaviour of “plot” changes.\n\n\nCode\nfit=lm(Petal.Width ~ ., data=iris)\nop=par(no.readonly=TRUE)   # safe only resettable graphical parameters, avoids many warnings\npar(mfrow=c(2,2))          # change graphical parameters: 2x2 images on device\nplot(fit,col=iris$Species) # four plots rather than one\n\n\n\n\n\nCode\npar(op)                    # reset graphical parameters\n\n\nmore examples here: http://www.statmethods.net/stats/regression.html\nLinear models \\(y_i=\\theta_0 + \\theta_1 x_i + \\epsilon_i\\) make certain assumptions (\\(\\epsilon_i \\propto N(0,\\sigma^2)\\))\n\nresiduals \\(\\epsilon_i\\) are independent from each other (non-linear patterns?)\nresiduals are normally distributed\nhave equal variance \\(\\sigma^2\\) (homoscedascity)\nare there outliers (large residuals) or observations with strong influence on fit\n\n\n\n\nReview\n\ndependencies between variable can often be modeled\nlinear model lm(): fitting, summary and interpretation\ncorrelation coefficients can be misleading\nlinear models may not be appropriate. &gt;example(anscombe)"
  },
  {
    "objectID": "lectures/Rintro2023/04_DataDescriptions.html",
    "href": "lectures/Rintro2023/04_DataDescriptions.html",
    "title": "04: Data Descriptions and Visualizations",
    "section": "",
    "text": "R Markdown\nR scripts (.R) help to define and run reproducible analysis workflows, but they lack documentation (other than comments).\nR Markdown files (*.Rmd) combine scripts with powerful text formatting. They can be rendered (=“knit”) to produce html and pdf.\nTask: In Rstudio,\n\nopen a new R markdown document with File &gt; New File &gt; R Markdown.... (This will open a template for an Rmd file that can be knit)\nTo convert this file into html press Knit - try it out! You may have to save it first as - e.g. “first.Rmd”\nThere is a YAML header that contains parameters which will affect the rendering process - customize them\nThe rest of the document are text blocks (with simple format instructions) and code blocks (with R code)\nIn R studio, the code blocks can also be run individually using the embedded Play button - try it out\nPlay time: Modify the yaml header, text blocks or code - or all of it. “knit” the documnt and observe changes.\n\n\n\n\nDescriptive Statistics\n\n\nCode\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nCode\nplot(iris$Petal.Length) # a plot at last. Simple, but many arguments: ?plot \n\n\n\n\n\nCode\nhist(iris$Petal.Length) # and a histogram\n\n\n\n\n\nTask hist() can have many arguments. Use help to find out how the histogram can be customized (e.g. number of break points, title, colors). Try some of it.\n\n\nBoxplot: a more condensed summary\n\n\nCode\nboxplot(iris$Petal.Length)\n\n\n\n\n\nTask: The boxplot above is for all data. Create a boxplot of petal length for species “setosa” only.\n\n\n\n\n\nBoxplot understands data frames\n\n\nCode\nboxplot(iris) \n\n\n\n\n\nQuery: What does the boxplot for Species mean?\n\n\nInterlude: Factors = categorical variables\nFactors denote a special class of R-objects that can be thought of as categories (here: species). They have a given number of levels which are internally represented as integers.\n\n\nCode\nclass(iris$Species)\n\n\n[1] \"factor\"\n\n\nCode\ntypeof(iris$Species)\n\n\n[1] \"integer\"\n\n\nCode\nts=table(iris$Species)  # returns a contigency table ~&gt; histogram for categorical data\nbarplot(ts, col=rainbow(3), ylab=\"observations\", cex.names=0.9)\n\n\n\n\n\nCode\npie(ts,col=rainbow(3))  # if you really must\n\n\n\n\n\n\nBoxplot understands factors in data frames\n\n\nCode\nboxplot( Petal.Length ~ Species, data = iris, las=2) # what does las=2 do ?\n\n\n\n\n\nTask: Use help to add three different colors:\n\n\n\n\n\n\n\nCorrelations\nIf a data set has many numerical variables we often want to understand their correlations structure\n\n\nCode\nx=iris$Petal.Length\ny=iris$Petal.Width\nplot(x,y)                              # again: this can be customized\nabline(v=mean(x),h=mean(y),col=\"red\")  # add vertical/horizontal lines\n\n\n\n\n\nCode\ncor(x,y)                               # a correlation coefficient: which one?\n\n\n[1] 0.9628654\n\n\n\n\nAll-Against-All Correlations\nTask: remove the Species variable from “iris” and store the result in a new data.frame “niris”\n\n\n'data.frame':   150 obs. of  4 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n\n\n\n\nCode\ncor(niris)  # correlation matrix. Which correlation coefficient?\n\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nCode\nplot(niris) # provide a visualization for data frames, try also pairs(iris)\n\n\n\n\n\nCode\n# assign species-colors to each observation \ncols = iris$Species                        # understand how color is defined\nplot(niris, col=cols, lower.panel=NULL)   # \"cols\" was defined in task above\n\n\n\n\n\n\n\n\nReview\n\nR markdown files extend scripts and can be converted to html/pdf\nsummary for descriptive statistics\nsimple plots with simple commands: plot(), hist(), …\nlots of parameters for customization: line width, colours, …\ncorrelation between variables: cor(), plot() - class specific behaviour\nNotice that the data used was extremely clean and structured: data()"
  },
  {
    "objectID": "lectures/Rintro2023/06_DataVisualization.html",
    "href": "lectures/Rintro2023/06_DataVisualization.html",
    "title": "06: Data Visualization",
    "section": "",
    "text": "It’s good practice to normalize the different variables\n\n\nCode\nd=iris[,-5]   # numerical iris data (without speciee) \nds=scale(d)   # scaled iris data (column-wise)\n\nbr=seq(-5,7,by=0.5)                                           # set common break points for the histograms below\nhist(d[,\"Sepal.Width\"], breaks = br)                          # illustrate scaling for specific column\nhist(ds[,\"Sepal.Width\"], breaks = br, add=TRUE, col=\"red\")    # add histogram to current plot\nlegend(\"topright\", c(\"orig\",\"scaled\"), fill=c(\"white\", \"red\"))\n\n\n\n\n\n\n\n\nHeatmaps are color-coded representations of numerical matrices.\nTypically the rows and columns are re-ordered according to some distance measure (default: Euclidean) and hierarchical clustering method (default: complete)\nThere are many tools to draw heatmaps in R. Here we use the pheatmap package to provide this powerful functionality\n\n\nCode\n#install.packages(\"pheatmap\")  # That's how we install new packages - more later\nlibrary(pheatmap)              # make packaged functions available\npaste('loaded pheatmap version:', packageVersion('pheatmap'))\n\n\n[1] \"loaded pheatmap version: 1.0.12\"\n\n\nCode\nds  = scale(iris[,-5])                # scaled data for heatmap\nann = data.frame(Species = iris[,5])  # meta data for annotations\n\n# explicitly set rownames to retain association between data and metadata\nrownames(ds)=rownames(iris)\nrownames(ann)=rownames(iris)\n\npheatmap(ds, \n         annotation_row = ann,\n         show_rownames = FALSE,\n         )\n\n\n\n\n\nThere is many more parameters for more control - if you have lots of time read “?pheatmap”\n\n\n\n\nIn Rstudio, we can export figures from the “Plots” tab. On the console we can define a pdf file as a new device for all subsequent figures. This is usually done only after the image is sufficiently optimized\n\n\nCode\npdf(\"output/heatmap.pdf\")                                        # similar for jpeg, png, ...\npheatmap(ds, annotation_row = ann, show_rownames = FALSE)\ndev.off()                                                 # close device = pdf file\n\n\npdf \n  3"
  },
  {
    "objectID": "lectures/Rintro2023/06_DataVisualization.html#scaling",
    "href": "lectures/Rintro2023/06_DataVisualization.html#scaling",
    "title": "06: Data Visualization",
    "section": "",
    "text": "It’s good practice to normalize the different variables\n\n\nCode\nd=iris[,-5]   # numerical iris data (without speciee) \nds=scale(d)   # scaled iris data (column-wise)\n\nbr=seq(-5,7,by=0.5)                                           # set common break points for the histograms below\nhist(d[,\"Sepal.Width\"], breaks = br)                          # illustrate scaling for specific column\nhist(ds[,\"Sepal.Width\"], breaks = br, add=TRUE, col=\"red\")    # add histogram to current plot\nlegend(\"topright\", c(\"orig\",\"scaled\"), fill=c(\"white\", \"red\"))"
  },
  {
    "objectID": "lectures/Rintro2023/06_DataVisualization.html#heatmaps",
    "href": "lectures/Rintro2023/06_DataVisualization.html#heatmaps",
    "title": "06: Data Visualization",
    "section": "",
    "text": "Heatmaps are color-coded representations of numerical matrices.\nTypically the rows and columns are re-ordered according to some distance measure (default: Euclidean) and hierarchical clustering method (default: complete)\nThere are many tools to draw heatmaps in R. Here we use the pheatmap package to provide this powerful functionality\n\n\nCode\n#install.packages(\"pheatmap\")  # That's how we install new packages - more later\nlibrary(pheatmap)              # make packaged functions available\npaste('loaded pheatmap version:', packageVersion('pheatmap'))\n\n\n[1] \"loaded pheatmap version: 1.0.12\"\n\n\nCode\nds  = scale(iris[,-5])                # scaled data for heatmap\nann = data.frame(Species = iris[,5])  # meta data for annotations\n\n# explicitly set rownames to retain association between data and metadata\nrownames(ds)=rownames(iris)\nrownames(ann)=rownames(iris)\n\npheatmap(ds, \n         annotation_row = ann,\n         show_rownames = FALSE,\n         )\n\n\n\n\n\nThere is many more parameters for more control - if you have lots of time read “?pheatmap”"
  },
  {
    "objectID": "lectures/Rintro2023/06_DataVisualization.html#sending-plots-to-files",
    "href": "lectures/Rintro2023/06_DataVisualization.html#sending-plots-to-files",
    "title": "06: Data Visualization",
    "section": "",
    "text": "In Rstudio, we can export figures from the “Plots” tab. On the console we can define a pdf file as a new device for all subsequent figures. This is usually done only after the image is sufficiently optimized\n\n\nCode\npdf(\"output/heatmap.pdf\")                                        # similar for jpeg, png, ...\npheatmap(ds, annotation_row = ann, show_rownames = FALSE)\ndev.off()                                                 # close device = pdf file\n\n\npdf \n  3"
  },
  {
    "objectID": "lectures/Rintro2023/06_DataVisualization.html#pca-goals",
    "href": "lectures/Rintro2023/06_DataVisualization.html#pca-goals",
    "title": "06: Data Visualization",
    "section": "PCA Goals",
    "text": "PCA Goals\n\nsimplify description of data matrix \\(M\\): data reduction & extract most important information\nmaximal variance: look for direction in which data shows maximal variation\nminimal error: allow accurate reconstruction of original data\n\n from amoeba @ https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues"
  },
  {
    "objectID": "lectures/Rintro2023/06_DataVisualization.html#pca-with-r",
    "href": "lectures/Rintro2023/06_DataVisualization.html#pca-with-r",
    "title": "06: Data Visualization",
    "section": "PCA with R",
    "text": "PCA with R\n\n\nCode\npca = prcomp(M, scale=TRUE)\n\n\nTask: What kind of object is pca?"
  },
  {
    "objectID": "lectures/Rintro2023/06_DataVisualization.html#covariance-structure",
    "href": "lectures/Rintro2023/06_DataVisualization.html#covariance-structure",
    "title": "06: Data Visualization",
    "section": "Covariance Structure",
    "text": "Covariance Structure\n\n\nCode\nS=pca$x          # score matrix = rotated and scaled data matrix\n\npairs(S, col=s)\n\n\n\n\n\nCode\npheatmap(cov(M)) # original covariance matrix\n\n\n\n\n\nCode\n# covariance matrix of scores is diagonal (by design)\n# --&gt; principal components are uncorrelated\npheatmap(cov(S)) \n\n\n\n\n\nNotice that the higher components do not add much to the variance, so we may as well represent the transformed data in only the first two dimensions:\n\n\nCode\nplot(S[,1:2],pch=21, bg=s)  # score-plot"
  },
  {
    "objectID": "lectures/ABS2022/Jupyter/Notebooks_000.html",
    "href": "lectures/ABS2022/Jupyter/Notebooks_000.html",
    "title": "00: Jupyter Notebooks Basics",
    "section": "",
    "text": "Jupyter Notebooks contain text cells (annotations in markdown format) and code cells (containing executable Python code).\nThe current cell is a text cell. The cell below is a code cell.\nIn order to run (=execute) a code cell you can move the cursor to the cell of interest and either\n\npress the play button in the tool bar or\npress Shift-Enter simultaneously.\n\nTry to execute the two cells below:\n\n\nCode\n# simple python.\nprint('Hello world')\n\n\nHello world\n\n\n\n\nCode\n# simple linux. Notice the exclamation mark\n!ls\n\n\nsample_data\n\n\n\n\n\nCode Cell Number: Executed code cell will be marked by an increasing number.\nRun All: It is possible to run the whole notebook at once (similar to scripts). In this teaching context this may give a false impression of training success. In this course, many cells still need editing for training purposes, and the default way will be to execute cells one by one.\n\n\n\n\nThe %%script … line prevents execution when the cell (or the whole notebook) is run.\n\n\nCode\n%%script echo Do not execute\nprint('Hello world')\n\n\nDo not execute\n\n\nTask (1 min):: Execute the cell above. Then comment out the first line and execute it again.\n\n\nThe temporal order of cell execution is important! Random order may cause unintended problems (Limits of reproducibility).\nThis and other arguments have been made against notebooks:\nhttps://www.youtube.com/watch?v=7jiPeIFXb6U\n\n\n\n\nExplore jupyterlab and this notebook.\nFeel free to create a new notebook or add content below: text cells and code cells."
  },
  {
    "objectID": "lectures/ABS2022/Jupyter/Notebooks_000.html#notice",
    "href": "lectures/ABS2022/Jupyter/Notebooks_000.html#notice",
    "title": "00: Jupyter Notebooks Basics",
    "section": "",
    "text": "Code Cell Number: Executed code cell will be marked by an increasing number.\nRun All: It is possible to run the whole notebook at once (similar to scripts). In this teaching context this may give a false impression of training success. In this course, many cells still need editing for training purposes, and the default way will be to execute cells one by one."
  },
  {
    "objectID": "lectures/ABS2022/Jupyter/Notebooks_000.html#controlled-execution",
    "href": "lectures/ABS2022/Jupyter/Notebooks_000.html#controlled-execution",
    "title": "00: Jupyter Notebooks Basics",
    "section": "",
    "text": "The %%script … line prevents execution when the cell (or the whole notebook) is run.\n\n\nCode\n%%script echo Do not execute\nprint('Hello world')\n\n\nDo not execute\n\n\nTask (1 min):: Execute the cell above. Then comment out the first line and execute it again.\n\n\nThe temporal order of cell execution is important! Random order may cause unintended problems (Limits of reproducibility).\nThis and other arguments have been made against notebooks:\nhttps://www.youtube.com/watch?v=7jiPeIFXb6U"
  },
  {
    "objectID": "lectures/ABS2022/Jupyter/Notebooks_000.html#time-to-play-10-min",
    "href": "lectures/ABS2022/Jupyter/Notebooks_000.html#time-to-play-10-min",
    "title": "00: Jupyter Notebooks Basics",
    "section": "",
    "text": "Explore jupyterlab and this notebook.\nFeel free to create a new notebook or add content below: text cells and code cells."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html",
    "title": "003 - An Application (Dishonest Casino)",
    "section": "",
    "text": "Motivated by: https://hmmlearn.readthedocs.io/en/latest/auto_examples/plot_casino_example.html\n\n\nA dishonest casino occassionally swaps their dice (fair &lt;-&gt; biased), but all we see is a sequence \\(X\\) of throws \\(X \\in \\{0,1,2,3,4,5\\}\\).\nIn this part we will:\n\nBe the casino: generate/sample such HMM data\nBe a clever player: learn the parameters (transition rates, emission rates) from observations\ntry to decode the game: predict the most likely sequence of states\n\n… all of this with hmmlearn.\n\n\n\n\nStates \\(Z\\): { 0 = fair, 1 = biased };\nObservations \\(X\\): { 0, 1, 2, 3, 4, 5 }\nParameters \\(\\Theta\\):\n\n\\[\\begin{align}\n    P(Z_0) &= \\begin{bmatrix} 0.5 & 0.5  \\end{bmatrix} \\\\ \\\\\n    P(Z_t | Z_{t-1}) & = \\begin{bmatrix} 0.95 & 0.05 \\\\ 0.25 & 0.75 \\end{bmatrix} \\\\ \\\\\n    P(X_t | Z_t) & =  \\begin{bmatrix} 1/6 & 1/6 &  1/6 & 1/6 & 1/6 & 1/6 \\\\ 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.5 \\end{bmatrix} \\\\\n\\end{align}\\]\n\n\n\n\n\nCode\n%%script echo install only once\n!pip install hmmlearn\n\n\ninstall only once\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom hmmlearn import hmm\n\n\n\n\n\n\n\nCode\npf = [1./6 ] *6           # fair emission probabilities\npb = [1./10] *5 + [1/2]   # biased emission probabilities\n\nnp.random.seed(42)\nmodel_gen = hmm.MultinomialHMM(n_components=2)\n\n# set parameters\nmodel_gen.startprob_ = np.array([1.0, 0.])                     # initial state prob\nmodel_gen.transmat_  = np.array([[0.95, 0.05], [0.25, 0.75]])   # transition prob\nmodel_gen.emissionprob_ =  np.array([pf, pb])                   # emission prob\n\nX,Z = model_gen.sample(5000)\n\n\nTask: Inspect the shapes for X and Z.\n\n\nHere is a simple function to plot the distribution of the obserrvations for the different states.\n\n\nCode\ndef HMM_inspect(Z,X,t=50):\n  z_str = ''.join(str(z) for z in Z.flatten()[0:t])\n  x_str = ''.join(str(x) for x in X.flatten()[0:t])\n  print('Z: ',z_str)\n  print('X: ',x_str)\n\n  bs=list(np.arange(0,6,0.5))\n  plt.hist(X[Z == 0], label='Z=0', alpha=0.5, bins=bs)\n  plt.hist(X[Z == 1], label='Z=1', alpha=0.5, bins=bs)\n  plt.xlabel('observation X')\n  plt.legend()\n  plt.show()\n\nHMM_inspect(Z,X,100)\n\n\nTask (5 min): Understand and interprete the results.\n\n\n\n\nTask (15 min)\n\nUse the Baum-Welch algorithm to fit a model with two states (“components”), given observations \\(X\\).\nObtain the score for this fit: \\(\\log P(X)\\)\nPredict the best possible path \\(Z\\)\n\n\n\nCode\n%%script echo edit before execution\nnp.random.seed(42)\nmodel_fit = hmm.MultinomialHMM(...)\n[ ... do the fit ..]\nscore = ...\nprint('scores: ', score)\nZv = ... predict path ...\n\nHMM_inspect(Zv,X)\n\n\nedit before execution\n\n\nTask & Discussion (5 min):\nWas the fit successful - does it reflect our expectation?\nRemove np.random.seed() and run the above code cell multiple times. What’s your conclusion?\nGroup Task & Explorations (45 min):\nHow could we improve the learning?\nNotice that there are many default parameters that can be adjusted. Consult the help pages for MultinomialHMM()!\n\nhow many iterations are run by default? try to increase it.\ninitialize parameters: maybe you can make an informed initial guess for the transitions probabilities\nloop over different initializations and keep only the model with best score\nwould it help to use another choice for path predcition (decoding) ?\nIf you have more time: you might want to generate longer sequences.\n\n\n\nCode\n%%script echo edit before execution\nbest_score = -1e81 # something really small\nbest_model = None\n\n[ ... ]\n\nHMM_inspect(Zv,X)\n\n\n\n\n\nHere I provide some convenience function to compare the HMM parameters (\\(P\\) and \\(E\\)) for two different models - for example the generating model and the fitted model.\n\n\nCode\n# some convenience functions\nfrom matplotlib.colors import Normalize\n\n# plot matrix (with title and numbers)\ndef plotMatrix(ax, mat, title, cm, normalizer):\n  ax.imshow(mat, cmap=cm, norm=normalizer)\n  for (j,i),label in np.ndenumerate(mat):\n    ax.text(i,j,np.round(label,2),ha='center',va='center')\n    ax.set_title(title)\n\n# compare two models\ndef compareHMM(model, model_fit):\n  ## Visualization ###\n  my_cm=plt.cm.Blues        # set color-map\n  normalizer=Normalize(0,1) # set common color code for trans and emission probs \n\n  fig, ax = plt.subplots(3, 2, \n      gridspec_kw={'width_ratios': [1, 3], 'height_ratios': [1, 1, 0.1]},\n      figsize=(10,7))\n\n  plotMatrix(ax[0,0], model.transmat_, 'trans - orig', my_cm, normalizer)\n  plotMatrix(ax[0,1], model.emissionprob_, 'emissions - orig', my_cm, normalizer)\n  plotMatrix(ax[1,0], model_fit.transmat_, 'trans - fit', my_cm, normalizer)\n  plotMatrix(ax[1,1], model_fit.emissionprob_, 'emissions - fit', my_cm, normalizer)\n\n  # add colorbar with common color scale (set by im)\n  im = plt.cm.ScalarMappable( cmap=my_cm, norm=normalizer) \n  fig.colorbar(im, cax=ax[2,0], orientation='horizontal')\n  fig.colorbar(im, cax=ax[2,1], orientation='horizontal')\n  plt.show()\n\n\n\n\nCode\n%%script echo ensure best_model is defined before execution\ncompareHMM(model_gen, best_model)\n\n\n\n\nCode\n%%script echo alternative representation of emission probabilities\nimport pandas as pd\npp=pd.DataFrame(best_model.emissionprob_).plot(kind='bar', stacked=True, title='emissions')\n\n\nMessages:\n\nLearning = Fitting is computationally the most demanding step\nThe Baum-Welch Algorithm only delivers local maxima - run it several times to explore other initial conditions\nBe aware of hyperparameters that may need fine-tuning: number of iterations, initial estimates and prior assumptions\nThe number of components needs to be chosen wisely (limits of unsupervised learning, c.f. k-means)\nMore data helps, but fitting will take longer\nThe state labels may swap\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\n\n\n\n\nCode\n%%script echo ensure that best_model exists\nZ_pred = best_model.predict(X)\nprint('Z - orig: ', *Z[0:50])\nprint('Z - pred: ', *Z_pred[0:50])\nprint('confusion matrix: \\n', confusion_matrix(Z, Z_pred))\n\n\nTask (10 min):\nPredict the path using the original (generative) model, and compare it with the path that was actually generated along with the observations.\n\n\nCode\n_,Z_pred = model_gen.decode(X)\nprint('Z - orig: ', *Z[0:50])\nprint('Z - pred: ', *Z_pred[0:50])\nprint('confusion matrix: \\n', confusion_matrix(Z, Z_pred))\n\n\nMessage:\nAlthough state transitions and emission probabilities may be predicted quite accurately, predicting the correct state path is much harder.\nEven using the true original model, the predicted path \\(Z\\) (the most likely path) is not necessarily the same that really gave rise to the observation \\(X\\)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#the-story",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#the-story",
    "title": "003 - An Application (Dishonest Casino)",
    "section": "",
    "text": "A dishonest casino occassionally swaps their dice (fair &lt;-&gt; biased), but all we see is a sequence \\(X\\) of throws \\(X \\in \\{0,1,2,3,4,5\\}\\).\nIn this part we will:\n\nBe the casino: generate/sample such HMM data\nBe a clever player: learn the parameters (transition rates, emission rates) from observations\ntry to decode the game: predict the most likely sequence of states\n\n… all of this with hmmlearn."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#the-hmm-formulation",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#the-hmm-formulation",
    "title": "003 - An Application (Dishonest Casino)",
    "section": "",
    "text": "States \\(Z\\): { 0 = fair, 1 = biased };\nObservations \\(X\\): { 0, 1, 2, 3, 4, 5 }\nParameters \\(\\Theta\\):\n\n\\[\\begin{align}\n    P(Z_0) &= \\begin{bmatrix} 0.5 & 0.5  \\end{bmatrix} \\\\ \\\\\n    P(Z_t | Z_{t-1}) & = \\begin{bmatrix} 0.95 & 0.05 \\\\ 0.25 & 0.75 \\end{bmatrix} \\\\ \\\\\n    P(X_t | Z_t) & =  \\begin{bmatrix} 1/6 & 1/6 &  1/6 & 1/6 & 1/6 & 1/6 \\\\ 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.5 \\end{bmatrix} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#setting-up-the-software",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#setting-up-the-software",
    "title": "003 - An Application (Dishonest Casino)",
    "section": "",
    "text": "Code\n%%script echo install only once\n!pip install hmmlearn\n\n\ninstall only once\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom hmmlearn import hmm"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#playing-the-casino-generating-states-and-observations",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#playing-the-casino-generating-states-and-observations",
    "title": "003 - An Application (Dishonest Casino)",
    "section": "",
    "text": "Code\npf = [1./6 ] *6           # fair emission probabilities\npb = [1./10] *5 + [1/2]   # biased emission probabilities\n\nnp.random.seed(42)\nmodel_gen = hmm.MultinomialHMM(n_components=2)\n\n# set parameters\nmodel_gen.startprob_ = np.array([1.0, 0.])                     # initial state prob\nmodel_gen.transmat_  = np.array([[0.95, 0.05], [0.25, 0.75]])   # transition prob\nmodel_gen.emissionprob_ =  np.array([pf, pb])                   # emission prob\n\nX,Z = model_gen.sample(5000)\n\n\nTask: Inspect the shapes for X and Z.\n\n\nHere is a simple function to plot the distribution of the obserrvations for the different states.\n\n\nCode\ndef HMM_inspect(Z,X,t=50):\n  z_str = ''.join(str(z) for z in Z.flatten()[0:t])\n  x_str = ''.join(str(x) for x in X.flatten()[0:t])\n  print('Z: ',z_str)\n  print('X: ',x_str)\n\n  bs=list(np.arange(0,6,0.5))\n  plt.hist(X[Z == 0], label='Z=0', alpha=0.5, bins=bs)\n  plt.hist(X[Z == 1], label='Z=1', alpha=0.5, bins=bs)\n  plt.xlabel('observation X')\n  plt.legend()\n  plt.show()\n\nHMM_inspect(Z,X,100)\n\n\nTask (5 min): Understand and interprete the results."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#fit-the-data",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#fit-the-data",
    "title": "003 - An Application (Dishonest Casino)",
    "section": "",
    "text": "Task (15 min)\n\nUse the Baum-Welch algorithm to fit a model with two states (“components”), given observations \\(X\\).\nObtain the score for this fit: \\(\\log P(X)\\)\nPredict the best possible path \\(Z\\)\n\n\n\nCode\n%%script echo edit before execution\nnp.random.seed(42)\nmodel_fit = hmm.MultinomialHMM(...)\n[ ... do the fit ..]\nscore = ...\nprint('scores: ', score)\nZv = ... predict path ...\n\nHMM_inspect(Zv,X)\n\n\nedit before execution\n\n\nTask & Discussion (5 min):\nWas the fit successful - does it reflect our expectation?\nRemove np.random.seed() and run the above code cell multiple times. What’s your conclusion?\nGroup Task & Explorations (45 min):\nHow could we improve the learning?\nNotice that there are many default parameters that can be adjusted. Consult the help pages for MultinomialHMM()!\n\nhow many iterations are run by default? try to increase it.\ninitialize parameters: maybe you can make an informed initial guess for the transitions probabilities\nloop over different initializations and keep only the model with best score\nwould it help to use another choice for path predcition (decoding) ?\nIf you have more time: you might want to generate longer sequences.\n\n\n\nCode\n%%script echo edit before execution\nbest_score = -1e81 # something really small\nbest_model = None\n\n[ ... ]\n\nHMM_inspect(Zv,X)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#inspections",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#inspections",
    "title": "003 - An Application (Dishonest Casino)",
    "section": "",
    "text": "Here I provide some convenience function to compare the HMM parameters (\\(P\\) and \\(E\\)) for two different models - for example the generating model and the fitted model.\n\n\nCode\n# some convenience functions\nfrom matplotlib.colors import Normalize\n\n# plot matrix (with title and numbers)\ndef plotMatrix(ax, mat, title, cm, normalizer):\n  ax.imshow(mat, cmap=cm, norm=normalizer)\n  for (j,i),label in np.ndenumerate(mat):\n    ax.text(i,j,np.round(label,2),ha='center',va='center')\n    ax.set_title(title)\n\n# compare two models\ndef compareHMM(model, model_fit):\n  ## Visualization ###\n  my_cm=plt.cm.Blues        # set color-map\n  normalizer=Normalize(0,1) # set common color code for trans and emission probs \n\n  fig, ax = plt.subplots(3, 2, \n      gridspec_kw={'width_ratios': [1, 3], 'height_ratios': [1, 1, 0.1]},\n      figsize=(10,7))\n\n  plotMatrix(ax[0,0], model.transmat_, 'trans - orig', my_cm, normalizer)\n  plotMatrix(ax[0,1], model.emissionprob_, 'emissions - orig', my_cm, normalizer)\n  plotMatrix(ax[1,0], model_fit.transmat_, 'trans - fit', my_cm, normalizer)\n  plotMatrix(ax[1,1], model_fit.emissionprob_, 'emissions - fit', my_cm, normalizer)\n\n  # add colorbar with common color scale (set by im)\n  im = plt.cm.ScalarMappable( cmap=my_cm, norm=normalizer) \n  fig.colorbar(im, cax=ax[2,0], orientation='horizontal')\n  fig.colorbar(im, cax=ax[2,1], orientation='horizontal')\n  plt.show()\n\n\n\n\nCode\n%%script echo ensure best_model is defined before execution\ncompareHMM(model_gen, best_model)\n\n\n\n\nCode\n%%script echo alternative representation of emission probabilities\nimport pandas as pd\npp=pd.DataFrame(best_model.emissionprob_).plot(kind='bar', stacked=True, title='emissions')\n\n\nMessages:\n\nLearning = Fitting is computationally the most demanding step\nThe Baum-Welch Algorithm only delivers local maxima - run it several times to explore other initial conditions\nBe aware of hyperparameters that may need fine-tuning: number of iterations, initial estimates and prior assumptions\nThe number of components needs to be chosen wisely (limits of unsupervised learning, c.f. k-means)\nMore data helps, but fitting will take longer\nThe state labels may swap"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#confusion-matrix",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino.html#confusion-matrix",
    "title": "003 - An Application (Dishonest Casino)",
    "section": "",
    "text": "Code\nfrom sklearn.metrics import confusion_matrix\n\n\n\n\nCode\n%%script echo ensure that best_model exists\nZ_pred = best_model.predict(X)\nprint('Z - orig: ', *Z[0:50])\nprint('Z - pred: ', *Z_pred[0:50])\nprint('confusion matrix: \\n', confusion_matrix(Z, Z_pred))\n\n\nTask (10 min):\nPredict the path using the original (generative) model, and compare it with the path that was actually generated along with the observations.\n\n\nCode\n_,Z_pred = model_gen.decode(X)\nprint('Z - orig: ', *Z[0:50])\nprint('Z - pred: ', *Z_pred[0:50])\nprint('confusion matrix: \\n', confusion_matrix(Z, Z_pred))\n\n\nMessage:\nAlthough state transitions and emission probabilities may be predicted quite accurately, predicting the correct state path is much harder.\nEven using the true original model, the predicted path \\(Z\\) (the most likely path) is not necessarily the same that really gave rise to the observation \\(X\\)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html",
    "title": "002a - HMMLearn",
    "section": "",
    "text": "Model parameters: \\(\\Theta=(\\pi,P,E)\\)\nObservations: \\(X=(X_1, \\ldots X_T)\\)\nHidden States: \\(Z=(Z_1, \\ldots Z_T)\\)\n\nmost probable path: \\(argmax_Z Pr(Z|X)\\)\n\nQuiz:\n\nHow many state paths are there for \\(N=2\\) states and sequences of length \\(T=100\\) ?\nGiven \\(P(X,Z)\\) how would you calculate \\(Pr(X)\\) ?\nWhy would you need \\(P(X)\\)?"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#goal-avoid-redundant-calculations",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#goal-avoid-redundant-calculations",
    "title": "002a - HMMLearn",
    "section": "Goal: avoid redundant calculations",
    "text": "Goal: avoid redundant calculations\n… use the power of recursion (“dynamic programming”)\n\\[P(X) = \\sum_Z P(X_{1:T},Z_{1:T}) \\to \\sum_i P(X_{1:T},Z_T=i)\\]\n… if \\(P(X_{1:{t+1}},Z_{t+1})\\) can be derived from \\(P(X_{1:{t}},Z_{t})\\)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#idea-track-all-possibilities",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#idea-track-all-possibilities",
    "title": "002a - HMMLearn",
    "section": "Idea: track all possibilities",
    "text": "Idea: track all possibilities\nWe don’t know any \\(Z_t\\) \\(\\longrightarrow\\) Trellis graph.\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_HiddenTrellis.jpg”, width=“1000”&gt;\n\n\nStore \\(\\alpha_{ti} = Pr(X_{1:t}, Z_t=i)\\) for each pvalue of \\(i\\) (as \\(Z_t\\) is unobsorved). This vector will be propagated forward in time.\nIn our previous example we only had 2 states, but the graphic below is an illustration with 4 hidden states"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#graphical-summary-2-steps",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#graphical-summary-2-steps",
    "title": "002a - HMMLearn",
    "section": "Graphical Summary: 2 Steps",
    "text": "Graphical Summary: 2 Steps\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Forward.jpg”, width=“800”&gt;\n\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Forward_summary.jpg”, width=“800”&gt;\n\n\nThe Markov Model pushes the state \\(Z\\) forward in time \\(\\longrightarrow\\) matrix multiplication with \\(P\\)\nEmission probabilities: take into account the state-specifc probabilities for observation \\(X_t\\) \\(\\longrightarrow\\) element-wise multiplication with proper column of \\(E\\)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#notice",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#notice",
    "title": "002a - HMMLearn",
    "section": "Notice",
    "text": "Notice\n\nMarginalization: \\(Pr(X_{1:t}) = \\sum_i Pr(X_{1:t}, Z_t = i) = \\sum_i \\alpha_{ti}\\).\n\nRecursion Efficiency: Calculation of \\(Pr(X)\\) requires \\(T N^2\\) calculations \\(\\ll N^T\\)\n\nExample: \\((N,T) = (2, 100) \\longrightarrow 400 \\ll 2^{100}\\)\n\n\nEmission matrix \\(E_{ik}\\) serves as lookup table for given observation \\(X_t=k\\) at time \\(t\\). (\\(k=f(t)\\))"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#a-single-step-forward",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#a-single-step-forward",
    "title": "002a - HMMLearn",
    "section": "A single step forward",
    "text": "A single step forward\nStarting for a given \\(\\alpha_{ti}\\) th code below illustrates a single update in time. All subsequent times follow the same update, but the observed value of \\(x_t\\) will geenerally change from time to time\n\n\nCode\nimport numpy as np\npi=np.array( [0.75, 0.25] )                          # initial state probability\nP =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\nE =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilties\n\nalpha = np.array([0.75, 0.25])  # P(Z_t-1=i|X_t-1):  initial probability at time t-1\nxt = 1                          # given observation at time t (e.g. 1)\n \nalpha = alpha.dot(P)            # P(Z_t=i | X_t-1):  push Z from t-1 --&gt; t (state transition)\nprint('after state transition: ', alpha) \n\nep=E[:,xt]                      # P(X_t=xt|Z_t=i):  emission probabilities of for each state\nprint('emission vector:        ', LH)\n\nalpha = ep * alpha              # P(Z_t=i | X_1:t): posterior  (take into account new observation X_t)\nprint('new probability         ', alpha) \n\n\nafter state transition:  [0.625 0.375]\nemission vector:         [0.2 0.1]\nnew probability          [0.125  0.0375]"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#extensions",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#extensions",
    "title": "002a - HMMLearn",
    "section": "Extensions",
    "text": "Extensions\n\nAbove we propagated \\(Pr(X_{1:t}, Z_t=i)\\) forward in time, but this works similarly for \\(Pr(Z_t=i| X_{1:t})\\) (plus extra normaliztion)\nForward-backward algortihm for another interesting quantity: \\(Pr(Z_t=i| X_{1:T})\\)\n\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_ForwardBackward.jpg”, width=“1200”&gt;\n\n\\[\nPr(Z_t = i | X_{1:T}) = \\gamma_{ti} = \\frac{\\alpha_{ti} \\beta_{ti}}{\\sum_k \\alpha_{tk} \\beta_{tk}}\n\\]"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#hmmlearn-generating-sequences-and-observations",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_ForwardShort.html#hmmlearn-generating-sequences-and-observations",
    "title": "002a - HMMLearn",
    "section": "HMMlearn: Generating Sequences and Observations",
    "text": "HMMlearn: Generating Sequences and Observations\n\n\nCode\nfrom hmmlearn import hmm\n\npi=np.array( [0.75, 0.25] )                          # initial state probability\nP =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\nE =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilties\n\nnp.random.seed(42)                       # only for reproducibiltiy\n\n# define HMM and set parameters\nmodel = hmm.MultinomialHMM(n_components=2)\nmodel.startprob_ = pi                    # initial state prob\nmodel.transmat_  = P                     # transition prob\nmodel.emissionprob_ = E                  # emission prob\n\n# generate sequence\nX,Z = model.sample(50)                 # c.f. Z, X = generate_HMM(P,pi,E)\nprint('states Z       =',*Z.flatten()) # Z.shape = (T,)\nprint('observations X =',*X.flatten()) # X.shape = (T,1)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_005_project.html",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_005_project.html",
    "title": "005 - Project (Learning an HMM)",
    "section": "",
    "text": "Load Packages\n\n\nCode\n#%%script echo install only once\n!pip install hmmlearn\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom hmmlearn import hmm\n\ndef plotMatrix(mat, title):\n  plt.imshow(mat, cmap=plt.cm.Blues)\n  for (j,i),label in np.ndenumerate(mat):\n    plt.text(i,j,np.round(label,2),ha='center',va='center')\n    plt.title(title)\n\n\n\n\nProject\nThis project is centered around the observations you had generated from your own Markov model, during an earlier part of the course.\nPresentations (5 min):\nEach group should give a brief presentation of their HMM story, if they have one - minimally they should communicate the number of states.\nNo need to communicate any specific probabilities – but listen carefully, sometimes the story may suggest prior information or constraints on emissions and transitions. This could be valuable during the learning.\nExchange files:\nExchange the files between the groups and try to infer the parameters used.\nIf the file has been written properly you should be able to read it like this\n\n\nCode\n%%script echo  ensure that fn points to proper path and exists\nfn=\"obs_group1.txt\"             # include the proper path in filename \n\ntry:\n  with open(fn, \"r\") as f:\n    line  = f.readline().split()  # read first line and split\nexcept IOError:\n    print(fn, ' does not exist. Stop this cell')\n    assert(False)\n\nX = list(map(np.int64, line))   # map line to np.int64\n\nX = np.array(X).reshape(-1,1)   # enforce proper shape: (T, 1)\n\nprint('Read',fn, 'with shape', X.shape)\n\n\n\n\nFirst Inspection\nInspect \\(X\\) and make sure the data is properly formatted\nHow long is \\(X\\)? What is the number of unique observations? It might be a good idea to plot a histogram\n\n\nCode\n%%script echo inspect data X\n...\n\n\n\n\nFit\nNow you are ready to go. Fit the model - and try to optimize the score as much as you (and the Baum-Welch algorithm) can.\n\n\nCode\n%%script echo edit before execution\nnp.random.seed(42)\nbest_model, best_score = None, None\n\nfor i in range(10):\n\n  ... fit here ...\n\n  #keep track of the scores and keep only the best model ...\n\n  score = ...\n  if (best_model is None or score &gt; best_score):\n    best_score = score\n    best_model = model_fit\n    print('new best: ', i, best_score)\n\n# show learned parameters for comparison\nplotMatrix(best_model.transmat_, 'transitions')\nplotMatrix(best_model.emissionprob_, 'emissions')\n\n\n\n\nPresentation\nPresent your efforts and the resulting transition and emission probabilities and compare with the true parameters used by the other group."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "",
    "text": "We have an HMM model and parameters (emissions, transitions and initial probability).\nRemember the travel analogy: we used countries (as hidden states) and food (as observables) - but we could have chosen many other stories.\n\n\nCode\nimport numpy as np\npi=np.array( [0.75, 0.25] )                          # initial state probability\nP =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\nE =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilities\n\n\nWith these parameters we can generate state sequences \\(Z\\) and observations \\(X\\), but in typical applications the state sequences \\(Z\\) are unobserved. We should think about \\(Z\\) probabilistically. Until now we have encountered the following distributions:\n\n“the joint” \\(P(X,Z)\\): nice theoretical form (graphical model), but in general not calculable (\\(Z\\) unknown)\n“scoring” \\(P(X)\\): how to sum over all \\(Z\\) efficiently \\(\\to\\) Forward algorithm\n“monitoring” \\(P(Z_t=i|X_{1:t})\\): similar to \\(P(X)\\) but with extra normalization (Bayesian update)\n\nNaively, these probabilites are very expensive to calculate \\(\\to\\) use efficient recursion (thanks to the Markov property).\nRemember our remaining goals:\n\n\\(P(Z_t=i| X)\\) what are the state probabilities at \\(t\\) given all observations \\(X=X_{1:T}\\) (past, present and future)\n\\(argmax_Z P(Z|X)\\) what is the most likely path \\(Z\\)\n\\(argmax_\\Theta P(\\Theta|X)\\) what are the best parameters"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#the-algorithm",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#the-algorithm",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "The Algorithm",
    "text": "The Algorithm\nNow we want to calculate the state probability at time \\(t\\) given all observations \\(X_{1:T}\\). This is also called smoothing or posterior decoding.\n\\[\n\\gamma_{ti} = P(Z_t=i|X_{1:T}) = Pr(Z_t=i| X_{1:t} X_{t+1:T}) \\propto  Pr(Z_t=i|X_{1:t}) \\cdot Pr(X_{t+1:T}|Z_t=i)\n\\]\nThe first factor is \\(\\alpha_{ti}\\) from the Forward Algorithm. The second factor denotes the likelihood of future observations, given \\(Z_t\\).\nIt can be calculated analoguously by the Backward Algorithm\n\\[\n\\begin{align}\n\\beta_{ti} &= Pr( X_{t+1:T} | Z_t=i) \\\\\n&= \\sum_k Pr(X_{t+2:T}|Z_{t+1}=k) Pr(X_{t+1} | Z_{t+1}=k) Pr(Z_{t+1=}k|Z_t=i) \\\\\n&= \\sum_k \\beta_{t+1 k} Pr(X_{t+1}|Z_{t+1}=k) Pr(Z_{t+1=}k|Z_t=i)\n\\end{align}\n\\]\n\\[\n\\mbox{Initalisation (t=T):}~~~ \\beta_{Ti} = 1~~~~~~~~\n\\mbox{Termination (t=1):}~~~\\sum_i \\beta_{1i} = Pr(X) \\\\\n\\]"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#summary",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#summary",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "Summary",
    "text": "Summary\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_ForwardBackward.jpg”, width=“1200”&gt;\n\nUltimately this leads to Forward-Backward Algorithm (+ Normalization)\n\\[\nPr(Z_t = i | X_{1:T}) = \\gamma_{ti} = \\frac{\\alpha_{ti} \\beta_{ti}}{\\sum_k \\alpha_{tk} \\beta_{tk}}\n\\]"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#uses",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#uses",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "Uses",
    "text": "Uses\n\nassign a posterior state probabilities to each time\n\nsample \\(Z_t|X\\) beyond just the best \\(Z_t\\)\nparameter estimates: couple with EM (Baum-Welch)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#hmmlearn-generating-sequences-and-observations",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#hmmlearn-generating-sequences-and-observations",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "HMMlearn: Generating Sequences and Observations",
    "text": "HMMlearn: Generating Sequences and Observations\n\n\nCode\nimport numpy as np\nfrom hmmlearn import hmm\n\npi=np.array( [0.75, 0.25] )                          # initial state probability\nP =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\nE =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilities\n\nnp.random.seed(42)                       # only for reproducibiltiy\n\n# define HMM and set parameters\nmodel = hmm.MultinomialHMM(n_components=2)\nmodel.startprob_ = pi                    # initial state prob\nmodel.transmat_  = P                     # transition prob\nmodel.emissionprob_ = E                  # emission prob\n\n# generate sequence\nX,Z = model.sample(50)                   # c.f. Z, X = generate_HMM(P,pi,E)\nprint('states Z       =',*Z.flatten())\nprint('observations X =',*X.flatten())"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#hmmlearn-calculating-prx",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#hmmlearn-calculating-prx",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "HMMlearn: Calculating \\(Pr(X)\\)",
    "text": "HMMlearn: Calculating \\(Pr(X)\\)\nRemember that the Forward Algorithm is used to circumvent the difficult calculation\n\\[\nPr(X) = \\sum_Z Pr(X,Z)\n\\]\n\n\nCode\nscore = model.score(X) \nprint('score Pr(X) = ', score)\n\n\nDiscussion: What’s the meaning of this score? Consult the help pages to find out more about this score."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#hmmlearn-calculating-prz_ti-x",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#hmmlearn-calculating-prz_ti-x",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "HMMlearn: Calculating \\(Pr(Z_t=i | X)\\)",
    "text": "HMMlearn: Calculating \\(Pr(Z_t=i | X)\\)\n\n\nCode\nPrZ = model.predict_proba(X) \n\n\n\n\nCode\n# use Pandas for dataframes and powerful plotting\nimport pandas as pd\ndf=pd.DataFrame(data=PrZ,index=Z)  # convert PrZ to dataframe and add true Z as index\npp=df.plot(kind='bar', stacked=True, figsize=(20,5), title='P(Z_t|X)')\n\n\nThis is how the posterior probability of \\(Z_t =i\\) changes over time.\nIndividual task (15 min): Take some time and try to understand the results and data structures above. Does the plot make sense? If this shows the “posterior” state probability; what is the “prior” state probability and how could you obtain it?"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#option-1-maximal-posterior",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#option-1-maximal-posterior",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "Option 1: Maximal Posterior",
    "text": "Option 1: Maximal Posterior\nThe above calculation \\(Pr(Z_t=i|X_{1:T})\\) suggests a simple way to define the “best” state sequence.\nFor example, one might simply collect the state sequence with the largest \\(Pr(Z_t=i|X)\\) for each time \\(t\\):\n\\[\nZ_t = \\underset{i}{argmax} Pr(Z_t=i | X)\n\\]\nDiscussion:\nRefer back to the above figure for \\(Pr(Z_t=i|X)\\) and see whether this is a good prescription (remember that in this case we know the true \\(Z\\)). What could be possible problems with it?"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#option-2-viterbi-decoding",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#option-2-viterbi-decoding",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "Option 2: Viterbi Decoding",
    "text": "Option 2: Viterbi Decoding\nUsing the maximal posterior (as above) may lead to forbidden state transition \\(Z_t \\to Z_{t+1}\\), where the corresponding transition probability is 0.\nFor this reason, we are also looking for a valid path \\(Z_{1:T}\\) that maximizes the conditional probability given all observations \\(X_{1:T}\\).\n\\[\nZ^\\ast = \\underset{Z}{argmax} Pr(Z|X) \\propto \\underset{Z}{argmax} Pr(Z,X) \\\\\n\\]\n\nRecursion: again\nAgain we will utilize the recursion principle - this time assuming that for a given time \\(t\\) we know - the path with maximum probability that ends in \\(Z_t=i\\) - the last link of that path that connects to \\(Z_t=i\\), i.e. it’s origin.\nGoal: maximize over all previous\n\\[\n\\mu_{ti} = \\max_{Z_{1:t-1}} Pr(Z_t=i, Z_{1:t-1}, X_{1:t})\n\\]\nFor time \\(t+1\\) we can use the usual propagation of probabilities.\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Viterbi.jpg”, width=“1500”&gt;\n\nNotice:\n\nThis is the Forward Algorithm with \\(\\sum_k \\to \\max_k\\)\n\\(\\max_i \\mu_{Ti} = \\max_{Z} Pr(Z,X)\\)\nforbidden paths would have some transitions \\(Pr(Z_{t+1}=j | Z_{t}=i) = 0 \\longrightarrow\\) they would never be maximal.\ncomplexity: \\(O(T) \\ll O(N^T)\\)\nalso need to keep track of last links \\(\\lambda_{ti}\\) (the maximizing transitions) \\(\\to\\) back-tracking"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#hmmlearn-decoding",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#hmmlearn-decoding",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "HMMlearn: Decoding",
    "text": "HMMlearn: Decoding\nThe hmmlearn implementation is very easy to use\n\n\nCode\n_, Zv = model.decode(X)\n\n\n\n\nCode\n%%script echo This would be an alternative approach (also with hmmlearn)\n# alternatives\nPrZ = model.predict_proba(X)    # maximum posterior probability \nZm = PrZ.argmax(axis=1)         # get maximum\nZv = model.predict(X)            # Viterbi pathh\n\n\nGroup Task (30 min):\n\nSample the HMM model again with T=50\nConsult the help for model.decode() and identify two different methods to predict a hidden state sequence for a given X.\nTry both algorithms and look for differences in the prediced sequences.\nCompare the predicted state sequence with the “hidden” sequence Z\nIncrease the generated sequence length to 1000 and repeat the comparisons.\n\n\n\nCode\n%%script echo edit before execution\nnp.random.seed(42)\nT=1000\nX,Z = ... sample T timesteps from model ...\n\n# get two different \"best\" paths from two methods \n[ model.decode( ... method 1 ...) ]\n[ model.decode( ... method 2 ...) ]\n\n# Check differences\nprint('differences (Z1-Z):  ', np.sum(Z1 != Z))\nprint('differences (Z2-Z):  ', np.sum(Z2 != Z))"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#if-z-was-observable-skip",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#if-z-was-observable-skip",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "If Z was observable (skip)",
    "text": "If Z was observable (skip)\n\n\nCode\nL=len(Z)    # length of observed sequence\n\ntrans={}\nfor i in range(L-1):\n  pair = '{}-&gt;{}'.format(Z[i],Z[i+1])\n  trans[pair] = trans.get(pair, 0) + 1\n\nemiss={}\nfor i in range(L):\n  pair = '{}-&gt;{}'.format(Z[i],X[i])\n  emiss[pair] = emiss.get(pair, 0) + 1\n\n\nprint('transitions:')\nfor k, v in trans.items():\n  print(k,v)\n\nprint('emissions:')\nfor k, v in emiss.items():\n  print(k,v)\n\n\nDiscussion: Why would this not necessarily be a good estimate? (Longer chains vs multiple chains)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#if-z-is-hidden-baum-welch",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_002_Viterbi.html#if-z-is-hidden-baum-welch",
    "title": "002 - More Algorithms (Viterbi and Baum-Welch)",
    "section": "If Z is hidden (Baum-Welch)",
    "text": "If Z is hidden (Baum-Welch)\n\nInitialize all probabilities randomly\nRe-estimation: change parameters iteratively to maximize \\(Pr(X)\\)\n\nThe re-estimation procedure makes clever use of Forward and Backward algorithm, but one could also use gradient techniques for optimization.\nThe iteration will converge to local maxima and will generally depend on the initial conditions, and the number of iterations.\n\n\nCode\nmodel_fit = hmm.MultinomialHMM(n_components=2)\nmodel_fit.fit(X)\nscore = model_fit.score(X)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_003_Applications.html",
    "href": "lectures/ABS2022/MarkovChains/MC_003_Applications.html",
    "title": "Markov Chain Applications",
    "section": "",
    "text": "Open In Colab\nApplications: - Models for real processes (weather, games, sequences, …) - Hidden Markov Models (unobservable states) - Construct processes that converge to desired distributions \\(\\pi\\) (MCMC)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_003_Applications.html#d-random-walk-with-boundary",
    "href": "lectures/ABS2022/MarkovChains/MC_003_Applications.html#d-random-walk-with-boundary",
    "title": "Markov Chain Applications",
    "section": "1D Random Walk (with boundary)",
    "text": "1D Random Walk (with boundary)\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/MC_RandomWalk.png”, align=left width=“800”&gt;\n\n\\(p+q=1\\)\nDiscussion: What would the transition matrix look like?"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_003_Applications.html#drunkards-walk",
    "href": "lectures/ABS2022/MarkovChains/MC_003_Applications.html#drunkards-walk",
    "title": "Markov Chain Applications",
    "section": "Drunkard’s Walk",
    "text": "Drunkard’s Walk\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/drunkards_walk.png”, align=left width=“1000”&gt;\n\nTask (15 min):\nWrite down the transition matrix and calculate the probabilities for each starting state that the drunkard will reach home"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html",
    "title": "Markov Chain Sampling",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#background",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#background",
    "title": "Markov Chain Sampling",
    "section": "Background",
    "text": "Background\nIn the models discussed previously, all observations were sampled independently of each other, and from the same distribution (iid):\n\\[X \\propto N(\\mu, \\sigma^2)\\]\nIn higher dimensional models \\(X\\) can be a vector with correlations among its components, but different observations of this vector would still be independent of each other.\nLet’s relax this. Assume the next state will depend (only) on the current state. In the travel analogy: don’t sample countries independently, but travel (sample) along some biased paths.\nThe future is only a function of the present (and some parameters):\n\\[x_{t+1} = f(x_t, \\theta)\\]\nThis is similar to deterministic physical laws, but with added stochasticity.\nA Markov Process generates a sequence (a Markov chain) of states: \\(X = (X_0, X_1, \\ldots)\\) where each \\(X_t\\) is a random variable and\n\\[Pr(X_{t+1} | X_t, X_{t-1}, \\ldots X_1) = Pr(X_{t+1} | X_t) \\]\nMarkov Chain is a memory-less stochastic process: the present state encodes all the history.\nIn Statistics parlance: Given the Present, the Future is conditionally independent of the Past.\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/MC_DeterministicStatistic.jpg”, width=“1000”&gt;"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#some-history",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#some-history",
    "title": "Markov Chain Sampling",
    "section": "Some History",
    "text": "Some History\n\n\n\nAndrey Markov (1856-1922) - study processes which are not iid - Law of Large Numbers does not depend on iid assumption - 1st Markov model: words with 2 states (consonant, vowel)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#a-travel-story",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#a-travel-story",
    "title": "Markov Chain Sampling",
    "section": "A travel story",
    "text": "A travel story\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/MC_TravelStory.jpg”, width=“600”&gt;"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#encodings",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#encodings",
    "title": "Markov Chain Sampling",
    "section": "Encodings",
    "text": "Encodings\nIn this course we will assume that possible values for states \\(X_t\\) are discrete. There are different possible encodings: - {0,1,2,3} - {A,B,C,D} or {A,C,G,T} - {00, 01, 10, 11} - …\nFor computational convenience we often use integers, possibly with an added dictionary when necessary."
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#the-markov-chain",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#the-markov-chain",
    "title": "Markov Chain Sampling",
    "section": "The Markov chain",
    "text": "The Markov chain\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/MC_MarkovChain.jpg”, width=“1000”&gt;"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#the-state-graph",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#the-state-graph",
    "title": "Markov Chain Sampling",
    "section": "The State graph",
    "text": "The State graph\nAn image says a thousand words:"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#the-transition-matrix",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#the-transition-matrix",
    "title": "Markov Chain Sampling",
    "section": "The transition matrix",
    "text": "The transition matrix\nMatrices help for actual computation:\n\\[\nP = \\begin{bmatrix}\n0.8   &  0.2 \\\\\n0.1   &  0.9\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#the-trellis-graph",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#the-trellis-graph",
    "title": "Markov Chain Sampling",
    "section": "The Trellis graph",
    "text": "The Trellis graph\nExample for 4 different states:\n\n\n\nAn observed sequence \\(X\\) is a specific path (one of many possible) through the lattice diagram (shown in red)."
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#check",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#check",
    "title": "Markov Chain Sampling",
    "section": "Check:",
    "text": "Check:\nAre you comfortable to swap between the 4 representations? We’ll use them a lot."
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#discussion",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#discussion",
    "title": "Markov Chain Sampling",
    "section": "Discussion",
    "text": "Discussion\nSuggest other examples of Markov chains.\n\nWhat are the observables and what are transitions ?\nHow would you represent this system?\nAre there examples of sequences that cannot be described by simple transitions?\n\nInspirational link: https://en.wikipedia.org/wiki/Examples_of_Markov_chains"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#polls",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#polls",
    "title": "Markov Chain Sampling",
    "section": "Polls:",
    "text": "Polls:\nExample 1: drawing coloured balls from a mixed bag. Colour=random variable=state. 1. without replacement –&gt; Markovian? 2. with replacement –&gt; Markovian?\nExample 2: from physics/mechanics: Consider flying object at time t. What is the state of the object? 1. state=position –&gt; Markovian? 2. state=position+velocity –&gt; Markovian?"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#an-animation",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#an-animation",
    "title": "Markov Chain Sampling",
    "section": "An animation",
    "text": "An animation\nThere is a beautiful animation & visualization, but please be aware that they may not render well on all systems.\n\nVisualization: https://setosa.io/markov/index.html\nExplanation: https://setosa.io/ev/markov-chains"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#coin-throws",
    "href": "lectures/ABS2022/MarkovChains/MC_001_MCSampling.html#coin-throws",
    "title": "Markov Chain Sampling",
    "section": "Coin throws",
    "text": "Coin throws\nThrowing a biased coin is usually modelled as a Bernoulli process with success probability \\(p\\) - i.e. the probability of obtaining heads. \\[\nPr(X=Heads) = p\n\\]\nFor an unbiased coin: \\(p=0.5\\).\nHowever, this can also be formulated as a Markov Process. Let’s encode the two possible states as (heads=0) and (tails=1) for numerical convenience - see figure.\nTask (10 min):\nWrite down the corresponding transition matrix \\(P\\) for a biased coin and simulate the coin throwing experiment - e.g. 50 throws for your favorite \\(p\\).\n\n\nCode\n%%script echo Edit before executing\np = [ ... your choice ...]\nP = np.array(\n   [ ... edit here ...]\n)\nX = generate_sequence(P, T=50)\nprint(*X, sep='')\n\n\n010100101000111110000000110011101011101101111011110"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_006_Sequences.html",
    "href": "lectures/ABS2022/ANN/ANN_006_Sequences.html",
    "title": "Sequence Analysis with Neural Networks",
    "section": "",
    "text": "This lecture is based on a Nature Genetics tutorial:"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_006_Sequences.html#negative-dataset",
    "href": "lectures/ABS2022/ANN/ANN_006_Sequences.html#negative-dataset",
    "title": "Sequence Analysis with Neural Networks",
    "section": "Negative Dataset",
    "text": "Negative Dataset\nIdeally we would like to have a set of non-target sequences. In the absence of such a set, we define our own negative dataset by shuffeling.\n\n\nCode\n# define negative sequences (non targets)\nseq_neg = [ ''.join(random.sample(s, len(s)) ) for s in seq_pos ]\n\n# merge positive and negative sequences\nseq = seq_pos + seq_neg\n\n# define the corresponding labels (1=target, 0=non-target)\nlab = [1]*len(seq_pos) + [0]*len(seq_neg)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_006_Sequences.html#encoding",
    "href": "lectures/ABS2022/ANN/ANN_006_Sequences.html#encoding",
    "title": "Sequence Analysis with Neural Networks",
    "section": "Encoding",
    "text": "Encoding\n\n\nCode\nnuc2int = {\n    \"A\": 0,\n    \"C\": 1,\n    \"T\": 2,\n    \"G\": 3,\n    \"N\": 4,\n}\n\nint2nuc = dict((v, k) for k, v in nuc2int.items())\n\n# integer encoding: A-&gt;0, C-&gt;1, ...\nseq_i = [[nuc2int[letter] for letter in s] for s in seq]\n\n# one-hot encoding: 0 -&gt; [ 1 0 0 0 0], 1-&gt; [0 1 0 0 0], ... \nseq_h = tf.one_hot(seq_i,depth=5, dtype=tf.float32)\n\nlab_i = np.array(lab).reshape(-1,1)                 # integer encoding: list --&gt; numpy\nlab_h = tf.one_hot(lab,depth=2, dtype=tf.float32)   # one-hot encoded: list -&gt; tf.EagerTensor \nprint(lab_i.shape, lab_h.shape)\nprint(type(lab_i), type(lab_h))"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_006_Sequences.html#split-data-train-test",
    "href": "lectures/ABS2022/ANN/ANN_006_Sequences.html#split-data-train-test",
    "title": "Sequence Analysis with Neural Networks",
    "section": "Split Data: Train & Test",
    "text": "Split Data: Train & Test\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nseq_h=np.array(seq_h)  # tf --&gt; numpy\nlab_h=np.array(lab_h)  # tf  --&gt; numpy\n\nX_train, X_test, y_train, y_test = train_test_split(\n  seq_h, lab_h, test_size=0.20, random_state=42)\n\nprint('X_train:    ', X_train.shape)\nprint('y_train:    ', y_train.shape)\nprint('train-test: ', len(y_train), len(y_test))"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_006_Sequences.html#define-model",
    "href": "lectures/ABS2022/ANN/ANN_006_Sequences.html#define-model",
    "title": "Sequence Analysis with Neural Networks",
    "section": "Define Model",
    "text": "Define Model\n\n\nCode\nfrom tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\nfrom tensorflow.keras.models import Sequential\n\n# Define input shape and output dimensions - for each sample\nx_s = X_train.shape[1:] # feature (x) shape (skip first sample axis [0])\nnc = y_train.shape[1]   # number of classes (=1 without one-hot encoding)\n\nl_name='binary_crossentropy' \na_name='binary_accuracy'\n\nmodel = Sequential(name='Nanog_CNN_1')\nmodel.add(Conv1D(filters=32, kernel_size=12, input_shape=x_s))\nmodel.add(MaxPooling1D(pool_size=4))\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(nc, activation='softmax')) \n\nmodel.compile(optimizer='adam', loss=l_name, metrics=a_name)\n\nmodel.summary()"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_006_Sequences.html#fit-model",
    "href": "lectures/ABS2022/ANN/ANN_006_Sequences.html#fit-model",
    "title": "Sequence Analysis with Neural Networks",
    "section": "Fit Model",
    "text": "Fit Model\n\n\nCode\nfh = model.fit(X_train, y_train, epochs=20, verbose=1, validation_split=0.1)\nmodel_fn   = model.name + '.h5'\nhistory_fn = model.name + '_history.npy'\n\nmodel.save(model_fn)      # save model\nnp.save(history_fn, fh)   # save history"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_006_Sequences.html#load-model",
    "href": "lectures/ABS2022/ANN/ANN_006_Sequences.html#load-model",
    "title": "Sequence Analysis with Neural Networks",
    "section": "Load Model",
    "text": "Load Model\n\n\nCode\n%%script echo run if necessary\nmodel_fn   = model.name + '.h5'\nhistory_fn = model.name + '_history.npy'\nmodel = tf.keras.models.load_model(model_fn)        # load model\nfh = np.load(history_fn, allow_pickle=True).item()  # load history\n\n\nrun if necessary"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Goal: Given many images and their labels, learn a neural network to predict the label of a new image (c.f human learning).\nRepetitions: - Define Model & Optimization Strategy - Fit Model - Monitor Fitting - Evaluate Training\nNew items:"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#get-data",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#get-data",
    "title": "Image Classification",
    "section": "Get Data",
    "text": "Get Data\nMany famous datasets can be found here: https://www.tensorflow.org/datasets/catalog/overview\nThey can be accessed easily using keras functionality.\nIn the following we will focus on squared images of handwritten digits. They have also been annotated (labeled).\n\n\nCode\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 0s 0us/step\n\n\n\nTest and Training Sets\nFor models with many parameters there is a real danger of overfitting, i.e. learning the specifics of one set of samples rather than generalizable rules.\nFor performance evaluation it is crucial to retain an independent (but representative) test data set that it is never used for fitting. Using keras functionality, we loaded both training data and test data at the same time. If your data does not come split, you may have to do one of the following:\n\n\nCode\n%%script echo Suggestions for general (X,y). Do not run here\nfract = 0.80\n\nidx = numpy.random.permutation(X.shape[0]) # shuffle indices\ns = round(X.shape[0]*fract)                # split point\ntrain_idx, test_idx = idx[:s], idx[s:]\nX_train, X_test = X[train_idx,:], X[test_idx,:]\n# similar for labels\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = fract)\n\n# keep in mind that the test data set should be representative\n# See here for stratified partitioning\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n\n\nSuggestions for general (X,y). Do not run here"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#task-15-min-data-inspection.",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#task-15-min-data-inspection.",
    "title": "Image Classification",
    "section": "Task (15 min): Data Inspection.",
    "text": "Task (15 min): Data Inspection.\n\nInspect the types and shapes of the newly defined objects: X_train, y_train, …\nHave a look at the data (array) of the 42nd training image and describe what you see\nWhat is the maximal number?\nPlot the image using plt.imshow\nWhat is the label of this image?\n\n\n\nCode\n%%script echo edit here\n...\n\n\nedit here"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#data-normalization",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#data-normalization",
    "title": "Image Classification",
    "section": "Data Normalization",
    "text": "Data Normalization\nAlert: Usually many more steps are involved in preparing data for analysis: reading, reformating, filtering, shuffeling, transformation, normalization. This can take up a significant amount of time.\nHere we rely on a highly standardize data set and will only use normalization for illustration. It is important to do so consistently for both training and test data.\n\n\nCode\n# Run this code cell only once !!!\nX_train = X_train / 255.0\nX_test  = X_test  / 255.0"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#flattening",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#flattening",
    "title": "Image Classification",
    "section": "Flattening",
    "text": "Flattening\nA common procedure to process images is to first “flatten” them to a vector. This step will ultimately be done inside the neural network. The line below just illustrates the behaviour for a specific image:\n\n\nCode\nprint('original:  ', X_train[42,:].shape)\nprint('flattened: ', X_train[42,:].flatten().shape)\n\n\noriginal:   (28, 28)\nflattened:  (784,)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#dimensional-reduction",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#dimensional-reduction",
    "title": "Image Classification",
    "section": "Dimensional Reduction",
    "text": "Dimensional Reduction\n\n\nCode\nfrom sklearn.decomposition import PCA\n\nns = X_train.shape[0] # total number of samples\nnr = 500              # random number of subsamples\nidx=np.random.choice(ns, nr, replace=False)\nX_sub = X_train[idx,:].reshape(nr,-1)  # flatten each of the nr images\nX_pca = PCA(n_components = 2).fit_transform(X_sub)\nprint('Scores: ',X_pca.shape)\n\ncm = plt.get_cmap('tab10')\nplt.scatter( X_pca[:,0], X_pca[:,1] , c=y_train[idx], cmap=cm)\nplt.title('PCA')\nplt.colorbar()\nplt.show()\n\n\nScores:  (500, 2)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#define-and-compile-neural-network-model",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#define-and-compile-neural-network-model",
    "title": "Image Classification",
    "section": "Define and Compile Neural Network Model",
    "text": "Define and Compile Neural Network Model\nNew Elements - Relu Activation: Rectified Linear Unit (search for simple non-linearity) - Adam Optimizer: don’t get stuck in sharp local minima \\(\\to\\) adaptive learning rates (100k citations !)\n\n\nCode\nnc = np.unique(y_train).size     # number of classes / labels in training set\nl_name = 'sparse_categorical_crossentropy'\na_name = 'sparse_categorical_accuracy'\n\ninput_shape = X_train.shape[1:]\n\nprint('X_train.shape:     ', X_train.shape)\nprint('input_shape:       ', input_shape)\nprint('number of classes: ', nc)\n\nmod1 = tf.keras.Sequential( name = 'mnist_model_1')\nmod1.add( tf.keras.layers.Flatten(input_shape=input_shape) )  # flattens input to vector \nmod1.add( tf.keras.layers.Dense(128, activation='relu',name='1st_layer') )  # add layer with 128 nodes + relu\nmod1.add( tf.keras.layers.Dense(nc, activation='softmax', name='softmax_layer') )\n\nmod1.compile(optimizer='adam', loss=l_name, metrics=a_name)\n\nmod1.summary()\n\n\nX_train.shape:      (60000, 28, 28)\ninput_shape:        (28, 28)\nnumber of classes:  10\nModel: \"mnist_model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n 1st_layer (Dense)           (None, 128)               100480    \n                                                                 \n softmax_layer (Dense)       (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 101,770\nTrainable params: 101,770\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#fit-model",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#fit-model",
    "title": "Image Classification",
    "section": "Fit Model",
    "text": "Fit Model\n\n\nCode\nfh1 = mod1.fit(X_train, y_train, epochs=25, verbose=0, validation_split=0.1)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#save-model-and-history",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#save-model-and-history",
    "title": "Image Classification",
    "section": "Save Model and History",
    "text": "Save Model and History\nWe might want to save the trained model and the metrics history for latter use.\n\n\nCode\nmodel_fn   = 'minst_model.h5'   # specifying .h5 generates one h5 file (rather than a directory)\nhistory_fn = 'minst_history.npy'\n\nhist = fh1.history   # dictionary in fh1 object\nmod1.save(model_fn)\nnp.save(history_fn, hist)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#loading-model-and-history",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#loading-model-and-history",
    "title": "Image Classification",
    "section": "Loading Model and History",
    "text": "Loading Model and History\n\n\nCode\n%%script echo Run after new start\nmodel_fn   = 'minst_model.h5'\nhistory_fn = 'minst_history.npy'\n\nmod1 = tf.keras.models.load_model(model_fn)\nhist = np.load(history_fn,allow_pickle=True).item()"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#evaluation",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#evaluation",
    "title": "Image Classification",
    "section": "Evaluation",
    "text": "Evaluation\n\n\nCode\n## Plotting history and test accuracy\ndef plot_fit_history(hist, name='loss', test_score=0):\n  \"\"\"Plots history of metrics 'name'\n  The validation metrics will also be plotted if available as 'val_name' (Keras convention)\n  A test_score can be added to compare the evolution\"\"\"\n\n  if name not in hist:\n      print('plot_fit_history: {} is not a key in history !'.format(name))\n      return\n  val_name = 'val_' + name\n  plt.plot(hist[name],     label='train')\n\n  if val_name in hist:\n    # add validation if in data\n    plt.plot(hist[val_name], label='valid')\n  \n  plt.axhline(y=test_score, color='green', linestyle='-.',label = 'test')\n  plt.xlabel('Epoch')\n  plt.ylabel(name)\n  plt.legend(loc='upper left')\n  plt.show()\n\ntest_loss, test_acc = mod1.evaluate(X_test, y_test)\n\na_name='sparse_categorical_accuracy'\nplot_fit_history(hist, 'loss', test_loss)\nplot_fit_history(hist, a_name, test_acc)\n\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.1202 - sparse_categorical_accuracy: 0.9775\n\n\n\n\n\n\n\n\n\n\nCode\nmod1_pred = mod1.predict(X_test)           # probabilities    \ny_pred    = np.argmax(mod1_pred, axis=1)   # classes with max prob (= labels)\ncm=confusion_matrix(y_pred, y_test)\nplot_cm(cm)\n\n\n313/313 [==============================] - 1s 2ms/step"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#load-own-image",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#load-own-image",
    "title": "Image Classification",
    "section": "Load own image",
    "text": "Load own image\n\n\nCode\nfn='figures/ANN_Digit3.jpg'  # point to proper path\ninput_size=X_train.shape[1:]  \n\n\n\nUsing keras\n\n\nCode\n#%%script echo Make sure to set the proper image size (to match the model)\nfrom tensorflow.keras.preprocessing import image\n\n# load image \n# adjust to desired input_size (x,y) and convert RGB to grayscale (as MINST data)\nimg = image.load_img(fn, target_size=input_size, color_mode=\"grayscale\")\nprint('filename: ', fn, type(img), input_size)\n\n# convert PIL to numpy and reverse 0&lt;-&gt;255 (--&gt; MNIST convention)\nimg = 255 - np.array(img)    # alternative: image.img_to_array(img)\nprint('type, shape: ', type(img), img.shape)\n\nimg = img / 255.  # Normalization (in line with MINST)\n\nplt.imshow(img, cmap='Greys')\nprint('img.shape:', img.shape)\n\n\n\n\nUsing plt and cv2\n\n\nCode\nimport cv2\nimg = plt.imread(fn)                                             # read\nimg = cv2.resize(img, input_size, interpolation=cv2.INTER_AREA)  # resize\n\nimg = np.mean(img,axis=2)   # RGB averaging\nimg = 255 - img             # MINST convention\nimg = img / 255             # Normalization\n\nplt.imshow(img, cmap='Greys')\nprint('img.shape:', img.shape)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#prediction",
    "href": "lectures/ABS2022/ANN/ANN_004_ImageClassification.html#prediction",
    "title": "Image Classification",
    "section": "Prediction",
    "text": "Prediction\n\n\nCode\n# remember to reshape single image to network expectation = set of images\nimg = img[np.newaxis, ...]\npred=mod1.predict(img)\n\nfig, ax = plt.subplots(1,2, figsize=(12,5))\nax[0].imshow(img[0], cmap='Greys')\nax[1].bar(range(10), pred[0])\nax[1].set_xticks(range(10))\nax[1].set_title(np.argmax(pred))\nplt.show()"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_003_Spirals.html",
    "href": "lectures/ABS2022/ANN/ANN_003_Spirals.html",
    "title": "More difficult problems",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_cm(mat):\n  classes = np.arange(cm.shape[0])\n  plt.imshow(mat, cmap=plt.cm.Blues)\n  for (j,i),label in np.ndenumerate(mat):\n    plt.text(i,j,np.round(label,2),ha='center',va='center')\n\n  plt.colorbar()\n  plt.title('Confusion Matrix')\n  plt.xlabel('True label')\n  plt.ylabel('Pred label')\n  plt.xticks(classes)\n  plt.yticks(classes)\n  plt.show()"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_003_Spirals.html#adding-layers-and-non-linearities",
    "href": "lectures/ABS2022/ANN/ANN_003_Spirals.html#adding-layers-and-non-linearities",
    "title": "More difficult problems",
    "section": "Adding Layers and Non-Linearities",
    "text": "Adding Layers and Non-Linearities\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/ANN_ReLU.jpg”, width=“1200”&gt;\n\n\n\nCode\nnc = 3 # number of classes\nloss_name='sparse_categorical_crossentropy'   # for integer labels\nacc='sparse_categorical_accuracy'             # additional metrics to track\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(8, input_dim=2))\n#model.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.Dense(nc, activation='softmax'))\n\nmodel.compile(optimizer='sgd', loss=loss_name, metrics=[acc])\nmodel.summary()\n\n\n\n\nCode\nfh = model.fit(X,y, epochs=300, verbose=0)\nplt.plot(fh.history['loss'][1:])\nplt.plot(fh.history[acc][1:])\nplt.show()\n\n\n\n\nCode\neval = model.evaluate(x=X, y=y, verbose=0)\nprint('[loss, accuracy] = ', eval)\n\nyp = model.predict(X)\nyp1 = np.argmax(yp, axis=1)\n\ncm = confusion_matrix(yp1, y)\nplot_cm(cm)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_003_Spirals.html#saving-models",
    "href": "lectures/ABS2022/ANN/ANN_003_Spirals.html#saving-models",
    "title": "More difficult problems",
    "section": "Saving Models",
    "text": "Saving Models\nFitting is expensive. It’s good practice to save good models. They can be shared or reloaded later - with all parameters in place.\n\n\nCode\n# save: will create a directory of specified name\nmodel.save('model2')\n\n# load\nmodel2 = tf.keras.models.load_model('model2')"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_003_Spirals.html#group-task-30-min-changing-and-comparing-models",
    "href": "lectures/ABS2022/ANN/ANN_003_Spirals.html#group-task-30-min-changing-and-comparing-models",
    "title": "More difficult problems",
    "section": "Group Task (30 min): Changing and comparing models",
    "text": "Group Task (30 min): Changing and comparing models\nYou may have to rerun the fit for a more decent performance than in the lecture.\nMake sure have saved your best model as “model1”\n\nGroup 1: Remove the non-linear activation layer from the model and save the resulting model as model2.\nGroup 2: Add an additional layer with non-linear activation function (e.g. ‘relu’) before the final output layer and save the resulting model as model 2.\n\nReport your results back to all. Summarize the model. What can you say about the performance on the training data?\nThe code cell below should be ready to go without further editing. It may help to visualize differences between the two models, but it requires two distinct models named “model1” and “model2”\n\nDecision Boundaries\n\n\nCode\n# Vizualizing Decision Boundaries\n# This is feasible only for 2D data in higher dim it becomes useless\nfrom mlxtend.plotting import plot_decision_regions\n\n# Trick from http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/\n# see Example 12\n\nclass Onehot2Int(object):\n    def __init__(self, model):\n        self.model = model\n\n    def predict(self, X):\n        y_pred = self.model.predict(X)\n        return np.argmax(y_pred, axis=1)\n\n\nplt.figure(figsize=(12, 5))\n\nmodel1 = tf.keras.models.load_model('model1')\nmodel1_adj = Onehot2Int(model1)\nax = plt.subplot(1, 2, 1)\nplot_decision_regions(X, y, clf=model1_adj, legend=2)\nplt.title('Model 1')\n\nmodel2 = tf.keras.models.load_model('model2')\nmodel2_adj = Onehot2Int(model2)\nax = plt.subplot(1, 2, 2)\nplot_decision_regions(X, y, clf=model2_adj, legend=2)\nplt.title('Model 2')\n\nplt.show()\n\n\nLesson: The problem was more difficult because the data has non-linear patterns.\n\nSimple non-linear activation functions may help to separate more complex data structures.\nWe need to protect against over-fitting"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_outlook.html",
    "href": "lectures/ABS2022/ANN/ANN_outlook.html",
    "title": "007 - Outlook",
    "section": "",
    "text": "Next Stories - not covered\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/Oyens1879_DeStamtafel.jpg”, width=“500”&gt;\n\n\nSpeed-up learning:\n\ngradient improvements (use “Adam” for now)\nearly stopping: monitor validation loss / accuracy\n\nDealing with limited data\n\ntransfer learning: use other peoples network and learn only last layers (with less data)\nData Augmentation: rotate, crop, mirror, recolor, …\n\nAvoid overfitting\n\ndropout: remove random nodes during the learning process\nadd regularizers\n\n\n\n\nKlausur - Expectations\n\nLoad Code: know which packages you’ll need (pip install if necessary)\nLoad Data: Code will be given, but downsampling may be required\nExplore Data: know the shapes, number of samples, number of unique labels (classes)\nDefine Model\n\ndefine proper input shape\ndefine layers (with activation functions) given a verbal prescription\ndefine optimizer & metrics\nsummarize: how many parameters does the model have\n\nFit model:\n\nepochs, validation_split\nsave model\n\nEvaluate Model:\n\nplot metrics, understand difference in accuracy for train, validation and tets\n\nRun Predicitions:\n\nfor individual data set: what’s the true and predicted label\n(same for all test data: confusion matrix)"
  },
  {
    "objectID": "posts/wordcloud/wordCloud.html",
    "href": "posts/wordcloud/wordCloud.html",
    "title": "Word Clouds with R",
    "section": "",
    "text": "Setup\n\n\nCode\n# define libraries\nlibrary(tidyverse)\nlibrary(wordcloud)\nrequire(RColorBrewer)\nrequire(stopwords)\n\n# get data\nurl &lt;- \"http://cran.r-project.org/web/packages/available_packages_by_date.html\" \nhtml &lt;- rvest::read_html(url)  # harvest url\ndf &lt;- rvest::html_table(html)  # turn into dataframe\nhead(df[[1]],3)\n\n\n# A tibble: 3 × 3\n  Date       Package Title                                                      \n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                                                      \n1 2023-06-15 arealDB Harmonise and Integrate Heterogeneous Areal Data           \n2 2023-06-15 arrow   Integration to 'Apache' 'Arrow'                            \n3 2023-06-15 caresid Correspondence Analysis Plot and Associations Visualisation\n\n\nCode\nsw &lt;- stopwords::stopwords(\"english\") # define stopwords\nhead(sw,3)\n\n\n[1] \"i\"  \"me\" \"my\"\n\n\n\n\nWord frequencies\nNext we extract the word frequencies as table\n\n\nCode\nntop &lt;- 100\nword_freq &lt;- df[[1]] %&gt;% \n  pull(Title) %&gt;% \n  stringi::stri_extract_all_words() %&gt;% \n  unlist() %&gt;% \n  tolower() %&gt;%\n  # exclude stopwords in sw. equivalent: words = words[!words %in% sw]\n  .[!. %in% sw] %&gt;%\n  table()\n\nsort(word_freq) %&gt;% tail(5)\n\n\n.\n   using        r   models analysis     data \n     917     1333     1417     2025     3223 \n\n\n\n\nWord Cloud\n\n\nCode\nwordcloud( \n  names(word_freq), \n  word_freq, \n#  scale=c(8,.2), \n  min.freq=3, \n  max.words=100, \n  random.order=FALSE, \n  rot.per=.15, \n  colors=brewer.pal(8,\"Dark2\") \n)\n\n\npp = recordPlot()    # record the plot\n# the size and resolution for the png required some iterative optimization\npng(\"wordcloud.png\", width=300, height=300, res=75) \nreplayPlot(pp)       # replay the plot in png device\ndo &lt;- dev.off()\n\n\n\n\n\nFigure 1: WorldCloud for CRAN packages\n\n\n\n\n\n\nUsing tm\nFor some of the common text manipulation (punctuation, stopword removal etc) we could also use specific packages, e.g. tm."
  },
  {
    "objectID": "posts/python_qmd/python_qmd.html",
    "href": "posts/python_qmd/python_qmd.html",
    "title": "Quarto with Python (qmd)",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nfig.savefig(\"python_qmd.png\")\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\nThe source file for this example is a qmd file with python code blogs."
  },
  {
    "objectID": "posts/python_qmd/python_qmd.html#polar-axis",
    "href": "posts/python_qmd/python_qmd.html#polar-axis",
    "title": "Quarto with Python (qmd)",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nfig.savefig(\"python_qmd.png\")\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\nThe source file for this example is a qmd file with python code blogs."
  },
  {
    "objectID": "posts/python_qmd/python_qmd_revealjs.html#polar-axis",
    "href": "posts/python_qmd/python_qmd_revealjs.html#polar-axis",
    "title": "Quarto with Python (qmd)",
    "section": "Polar Axis",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nfig.savefig(\"python_qmd.png\")\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\nThe source file for this example is a qmd file with python code blogs.\n\n\nThomas Manke"
  },
  {
    "objectID": "posts/python_ipynb/python_ipynb.html",
    "href": "posts/python_ipynb/python_ipynb.html",
    "title": "Quarto with Python (ipynb)",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\n# this figure is save only when running from VSCode\n# but _not_ during `quarto render` - strange.\nfig.savefig(\"python_ipynb.png\") \nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\nThe source file for this example is an ipynb file with python code blogs."
  },
  {
    "objectID": "posts/python_ipynb/python_ipynb.html#polar-axis",
    "href": "posts/python_ipynb/python_ipynb.html#polar-axis",
    "title": "Quarto with Python (ipynb)",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\n# this figure is save only when running from VSCode\n# but _not_ during `quarto render` - strange.\nfig.savefig(\"python_ipynb.png\") \nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\nThe source file for this example is an ipynb file with python code blogs."
  },
  {
    "objectID": "posts/python_ipynb/python_ipynb_revealjs.html#polar-axis",
    "href": "posts/python_ipynb/python_ipynb_revealjs.html#polar-axis",
    "title": "Quarto with Python (ipynb)",
    "section": "Polar Axis",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\n# this figure is save only when running from VSCode\n# but _not_ during `quarto render` - strange.\nfig.savefig(\"python_ipynb.png\") \nplt.show()\n\n\n\nFigure 1: A line plot on a polar axisThe source file for this example is an ipynb file with python code blogs.\n\n\nThomas Manke"
  },
  {
    "objectID": "posts/trees_python/trees_python.html",
    "href": "posts/trees_python/trees_python.html",
    "title": "Trees with Python (qmd)",
    "section": "",
    "text": "Code\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px"
  },
  {
    "objectID": "posts/trees_python/trees_python.html#setup",
    "href": "posts/trees_python/trees_python.html#setup",
    "title": "Trees with Python (qmd)",
    "section": "",
    "text": "Code\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px"
  },
  {
    "objectID": "posts/trees_python/trees_python.html#sunbursts",
    "href": "posts/trees_python/trees_python.html#sunbursts",
    "title": "Trees with Python (qmd)",
    "section": "Sunbursts",
    "text": "Sunbursts\nThe gapminder data has a simple hierachy defined by the map country \\(\\to\\) continent.\nIn px.sunburst we define a path from continent to country. In fact, we may also define a root node (here world).\nNotice how the nodes and their parents are defined as columns in a data frame.\nOther columns can be chosen for color coding of nodes (here: lifeExp).\nTh calculation of proper sizes (continent pop) are done implicitly as part of the px::sunburs() implementation.\n\n\nCode\ndf = px.data.gapminder().query(\"year == 2007\")\ndf.head(4)  # nodes (country) and parents (continent) in pandas df\n\nfig = px.sunburst(df, \n  path=[px.Constant('world'), 'continent', 'country'], values='pop',\n  color='lifeExp', hover_data=['iso_alpha'],\n  color_continuous_scale='RdBu',\n  color_continuous_midpoint=np.average(df['lifeExp'], weights=df['pop']))\n\nfig.write_image(\"sunburst_python.png\")\nfig.show()\n\n\n\n\n                                                \nFigure 1: A Sunburst plot of gapminder data. Sizes (arc lengths) correspond to population sizes of countries aand continents. Colours encode life expectancy (lifeExp) relative to the weighted global average."
  },
  {
    "objectID": "posts/trees_python/trees_python.html#directory-as-treemap",
    "href": "posts/trees_python/trees_python.html#directory-as-treemap",
    "title": "Trees with Python (qmd)",
    "section": "Directory as TreeMap",
    "text": "Directory as TreeMap\nReferences/Links:\n\nhttps://plotly.com/python/sunburst-charts/\nhttps://stackoverflow.com/questions/74895923/create-a-treemap-showing-directory-structure-with-plotly-graph-object\nhttps://towardsdatascience.com/visualize-hierarchical-data-using-plotly-and-datapane-7e5abe2686e1\nhttps://www.geeksforgeeks.org/treemap-using-plotly-in-python/\n\n\nGet data: using du\nCollect data. Make sure to not double-count files. Per default du list files and directories, but here I only collect files.\nNotice 1: there are various ways to achieve the same with python modules (glob.glob, os.walk), but in the end du -a worked best for me.\nNotice 2: The strategy is to look for files and directories only appear as part of the file name (and only the direct parent directory). As a consequence empty directories will be ommitted, also upper level directories will be ommitted if they only contain other directories but no files: /empty/empty/full/files.txt\nIn this case only node (file.txt) and parent (full) would be reported\nempty directories will be omitted. Also\n\n\nCode\nfolder=\"..\"  # chose folder for which to collect data\n\n# run `du` on all files (but not directories)\ncmd='find ' +  folder + ' -type f -exec du -a {} +'\nout=subprocess.check_output( cmd, shell=True, text=True)\nlines = out.split(\"\\n\")\n\ndata = []\nfor line in lines:\n  if line:\n    size, file_name = line.split(\"\\t\")\n    dirs = file_name.split(\"/\")         # extract (sub)dirs from file_name\n    L = len(dirs)\n    if (L&lt;2):\n      continue\n    parent = dirs[L-2]\n    node   = dirs[L-1] \n    #print(line, size, node, parent)\n    data.append({\"size\": int(size), \"level\": L, \"node\": node, \"parent\": parent})\n \ndf = pd.DataFrame(data)\ndf.head(15)\n\n\n\n\n\n\n\n\n\nsize\nlevel\nnode\nparent\n\n\n\n\n0\n8\n5\noverlap_data2_33cab96026ea11c4f90e16721cbdf829...\nhtml\n\n\n1\n8\n5\noverlap_data2_33cab96026ea11c4f90e16721cbdf829...\nhtml\n\n\n2\n8\n5\nggplot_map_c910447fed13122597decb8efbdd1bf4.RData\nhtml\n\n\n3\n24\n5\noverlap_data2_33cab96026ea11c4f90e16721cbdf829...\nhtml\n\n\n4\n8\n5\nsetup_04fa97c80f81582a529607a798d5f49b.RData\nhtml\n\n\n5\n0\n5\nmaps_with_maps_f3b8224ebcb46befcca4d977f625292...\nhtml\n\n\n6\n3416\n5\nrnaturalearth_29e2651519d259454611eeae60c7f30f...\nhtml\n\n\n7\n8\n5\nrnaturalearth_29e2651519d259454611eeae60c7f30f...\nhtml\n\n\n8\n8\n5\nmaps_with_maps_f3b8224ebcb46befcca4d977f625292...\nhtml\n\n\n9\n8\n5\n__packages\nhtml\n\n\n10\n3936\n5\noverlap_data_e7bce49a1d6633888fb13a2d7c4b2b53.rdb\nhtml\n\n\n11\n8\n5\nggplot_map_c910447fed13122597decb8efbdd1bf4.rdx\nhtml\n\n\n12\n104\n5\nsf_intro_752ed2b5b9c5c023012e5ce31db6343c.rdb\nhtml\n\n\n13\n0\n5\nsetup_04fa97c80f81582a529607a798d5f49b.rdb\nhtml\n\n\n14\n8\n5\nsetup_04fa97c80f81582a529607a798d5f49b.rdx\nhtml\n\n\n\n\n\n\n\n\n\nVisualization 1: sunburst\nAgain the sizes of the directories are automatically inferred and visualized by the node-parent relationships. Minor differences with du &lt;dir&gt; may occur.\n\n\nCode\nfig = px.sunburst(df, \n                  path=[px.Constant(folder),'parent', 'node'], \n                  values='size', \n                  color='parent',\n#                  color_continuous_scale='Blues'\n                  )\nfig.update_layout(title_text=\"Content of directory = \" + folder +  \"  Command: \" + cmd, font_size=10)\nfig.show()\n\n\n\n\n                                                \nFigure 2: Sunburst of direcory folder\n\n\n\n\n\nVisualization 2: treemap\n\n\nCode\nfig = px.treemap(df, \n                  path=[px.Constant(folder),'parent', 'node'], \n                  values='size', \n                  color='parent',\n#                  color_continuous_scale='Blues'\n                  )\nfig.update_layout(title_text=\"Content of directory = \" + folder +  \"  Command: \" + cmd, font_size=10)\nfig.show()\n\n\n\n\n                                                \nFigure 3: Treemap of direcory."
  },
  {
    "objectID": "posts/trees_python/trees_python.html#watermark-and-package-versions",
    "href": "posts/trees_python/trees_python.html#watermark-and-package-versions",
    "title": "Trees with Python (qmd)",
    "section": "Watermark and Package Versions",
    "text": "Watermark and Package Versions\n\n\n\n\n\n\nwatermark\n\n\n\n\n\n\n\nCode\nimport watermark\nimport pkg_resources\n\n\n# Display watermark information\nprint(watermark.watermark())\n\n\n# Get installed module versions\ninstalled_packages = pkg_resources.working_set\n\n# Print module versions\nprint(\"Module versions:\")\nfor package in installed_packages:\n    print(package.key, package.version)\n\n\nLast updated: 2023-06-22T23:36:19.870244+02:00\n\nPython implementation: CPython\nPython version       : 3.11.4\nIPython version      : 8.12.0\n\nCompiler    : Clang 15.0.7 \nOS          : Darwin\nRelease     : 21.6.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nModule versions:\njinja2 3.1.2\nmarkupsafe 2.1.2\npillow 9.4.0\npyqt5 5.15.7\npyqt5-sip 12.11.0\npysocks 1.7.1\npyyaml 6.0\npygments 2.14.0\nqtpy 2.3.1\nsqlalchemy 2.0.16\nsend2trash 1.8.0\naltair 5.0.1\nanyio 3.6.2\nappnope 0.1.3\nargon2-cffi 21.3.0\nargon2-cffi-bindings 21.2.0\nasttokens 2.2.1\nattrs 22.2.0\nbackcall 0.2.0\nbackports.functools-lru-cache 1.6.4\nbeautifulsoup4 4.12.0\nbleach 6.0.0\nbrotlipy 0.7.0\ncertifi 2023.5.7\ncffi 1.15.1\ncharset-normalizer 3.1.0\nclick 8.1.3\ncomm 0.1.3\ncontourpy 1.0.7\ncryptography 40.0.1\ncycler 0.11.0\ndebugpy 1.6.6\ndecorator 5.1.1\ndefusedxml 0.7.1\nentrypoints 0.4\nexecuting 1.2.0\nfastjsonschema 2.16.3\nflit-core 3.8.0\nfonttools 4.39.3\ngreenlet 2.0.2\nidna 3.4\nimportlib-metadata 6.1.0\nimportlib-resources 5.12.0\nipykernel 6.22.0\nipython 8.12.0\nipython-genutils 0.2.0\nipywidgets 8.0.6\njedi 0.18.2\njsonschema 4.17.3\njupyter 1.0.0\njupyter-cache 0.6.1\njupyter-client 8.1.0\njupyter-console 6.6.3\njupyter-core 5.3.0\njupyter-events 0.6.3\njupyter-server 2.5.0\njupyter-server-terminals 0.4.4\njupyterlab-pygments 0.2.2\njupyterlab-widgets 3.0.7\nkaleido 0.2.1\nkiwisolver 1.4.4\nmatplotlib 3.7.1\nmatplotlib-inline 0.1.6\nmistune 2.0.5\nmunkres 1.1.4\nnbclassic 0.5.3\nnbclient 0.7.4\nnbconvert 7.2.9\nnbformat 5.9.0\nnest-asyncio 1.5.6\nnotebook 6.5.3\nnotebook-shim 0.2.2\nnumpy 1.23.5\npackaging 23.0\npandas 2.0.2\npandocfilters 1.5.0\nparso 0.8.3\npatsy 0.5.3\npexpect 4.8.0\npickleshare 0.7.5\npip 23.0.1\npkgutil-resolve-name 1.3.10\nplatformdirs 3.2.0\nplotly 5.15.0\nply 3.11\npooch 1.7.0\nprometheus-client 0.16.0\nprompt-toolkit 3.0.38\npsutil 5.9.4\nptyprocess 0.7.0\npure-eval 0.2.2\npyopenssl 23.1.1\npycparser 2.21\npyparsing 3.0.9\npyrsistent 0.19.3\npython-dateutil 2.8.2\npython-json-logger 2.0.7\npytz 2023.3\npyzmq 25.0.2\nqtconsole 5.4.2\nrequests 2.28.2\nrfc3339-validator 0.1.4\nrfc3986-validator 0.1.1\nrpy2 3.5.11\nscipy 1.10.1\nseaborn 0.12.2\nsetuptools 67.6.1\nsimplegeneric 0.8.1\nsip 6.7.7\nsix 1.16.0\nsniffio 1.3.0\nsoupsieve 2.3.2.post1\nstack-data 0.6.2\nstatsmodels 0.14.0\ntabulate 0.9.0\ntenacity 8.2.2\nterminado 0.17.1\ntinycss2 1.2.1\ntoml 0.10.2\ntoolz 0.12.0\ntornado 6.2\ntraitlets 5.9.0\ntyping-extensions 4.5.0\ntzdata 2023.3\ntzlocal 5.0.1\nurllib3 1.26.15\nwatermark 2.3.1\nwcwidth 0.2.6\nwebencodings 0.5.1\nwebsocket-client 1.5.1\nwheel 0.40.0\nwidgetsnbextension 4.0.7\nzipp 3.15.0"
  },
  {
    "objectID": "posts/trees_python/trees_python_revealjs.html#setup",
    "href": "posts/trees_python/trees_python_revealjs.html#setup",
    "title": "Trees with Python (qmd)",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px"
  },
  {
    "objectID": "posts/trees_python/trees_python_revealjs.html#sunbursts",
    "href": "posts/trees_python/trees_python_revealjs.html#sunbursts",
    "title": "Trees with Python (qmd)",
    "section": "Sunbursts",
    "text": "Sunbursts\nThe gapminder data has a simple hierachy defined by the map country \\(\\to\\) continent.\nIn px.sunburst we define a path from continent to country. In fact, we may also define a root node (here world).\nNotice how the nodes and their parents are defined as columns in a data frame.\nOther columns can be chosen for color coding of nodes (here: lifeExp).\nTh calculation of proper sizes (continent pop) are done implicitly as part of the px::sunburs() implementation.\n\n\nCode\ndf = px.data.gapminder().query(\"year == 2007\")\ndf.head(4)  # nodes (country) and parents (continent) in pandas df\n\nfig = px.sunburst(df, \n  path=[px.Constant('world'), 'continent', 'country'], values='pop',\n  color='lifeExp', hover_data=['iso_alpha'],\n  color_continuous_scale='RdBu',\n  color_continuous_midpoint=np.average(df['lifeExp'], weights=df['pop']))\n\nfig.write_image(\"sunburst_python.png\")\nfig.show()\n\n\n\n\n                                                \nFigure 1: A Sunburst plot of gapminder data. Sizes (arc lengths) correspond to population sizes of countries aand continents. Colours encode life expectancy (lifeExp) relative to the weighted global average."
  },
  {
    "objectID": "posts/trees_python/trees_python_revealjs.html#directory-as-treemap",
    "href": "posts/trees_python/trees_python_revealjs.html#directory-as-treemap",
    "title": "Trees with Python (qmd)",
    "section": "Directory as TreeMap",
    "text": "Directory as TreeMap\nReferences/Links:\n\nhttps://plotly.com/python/sunburst-charts/\nhttps://stackoverflow.com/questions/74895923/create-a-treemap-showing-directory-structure-with-plotly-graph-object\nhttps://towardsdatascience.com/visualize-hierarchical-data-using-plotly-and-datapane-7e5abe2686e1\nhttps://www.geeksforgeeks.org/treemap-using-plotly-in-python/\n\nGet data: using du\nCollect data. Make sure to not double-count files. Per default du list files and directories, but here I only collect files.\nNotice 1: there are various ways to achieve the same with python modules (glob.glob, os.walk), but in the end du -a worked best for me.\nNotice 2: The strategy is to look for files and directories only appear as part of the file name (and only the direct parent directory). As a consequence empty directories will be ommitted, also upper level directories will be ommitted if they only contain other directories but no files: /empty/empty/full/files.txt\nIn this case only node (file.txt) and parent (full) would be reported\nempty directories will be omitted. Also\n\n\nCode\nfolder=\"..\"  # chose folder for which to collect data\n\n# run `du` on all files (but not directories)\ncmd='find ' +  folder + ' -type f -exec du -a {} +'\nout=subprocess.check_output( cmd, shell=True, text=True)\nlines = out.split(\"\\n\")\n\ndata = []\nfor line in lines:\n  if line:\n    size, file_name = line.split(\"\\t\")\n    dirs = file_name.split(\"/\")         # extract (sub)dirs from file_name\n    L = len(dirs)\n    if (L&lt;2):\n      continue\n    parent = dirs[L-2]\n    node   = dirs[L-1] \n    #print(line, size, node, parent)\n    data.append({\"size\": int(size), \"level\": L, \"node\": node, \"parent\": parent})\n \ndf = pd.DataFrame(data)\ndf.head(15)\n\n\n\n\n\n\n\n\n\nsize\nlevel\nnode\nparent\n\n\n\n\n0\n8\n5\noverlap_data2_33cab96026ea11c4f90e16721cbdf829...\nhtml\n\n\n1\n8\n5\noverlap_data2_33cab96026ea11c4f90e16721cbdf829...\nhtml\n\n\n2\n8\n5\nggplot_map_c910447fed13122597decb8efbdd1bf4.RData\nhtml\n\n\n3\n24\n5\noverlap_data2_33cab96026ea11c4f90e16721cbdf829...\nhtml\n\n\n4\n8\n5\nsetup_04fa97c80f81582a529607a798d5f49b.RData\nhtml\n\n\n5\n0\n5\nmaps_with_maps_f3b8224ebcb46befcca4d977f625292...\nhtml\n\n\n6\n3416\n5\nrnaturalearth_29e2651519d259454611eeae60c7f30f...\nhtml\n\n\n7\n8\n5\nrnaturalearth_29e2651519d259454611eeae60c7f30f...\nhtml\n\n\n8\n8\n5\nmaps_with_maps_f3b8224ebcb46befcca4d977f625292...\nhtml\n\n\n9\n8\n5\n__packages\nhtml\n\n\n10\n3936\n5\noverlap_data_e7bce49a1d6633888fb13a2d7c4b2b53.rdb\nhtml\n\n\n11\n8\n5\nggplot_map_c910447fed13122597decb8efbdd1bf4.rdx\nhtml\n\n\n12\n104\n5\nsf_intro_752ed2b5b9c5c023012e5ce31db6343c.rdb\nhtml\n\n\n13\n0\n5\nsetup_04fa97c80f81582a529607a798d5f49b.rdb\nhtml\n\n\n14\n8\n5\nsetup_04fa97c80f81582a529607a798d5f49b.rdx\nhtml\n\n\n\n\n\n\n\nVisualization 1: sunburst\nAgain the sizes of the directories are automatically inferred and visualized by the node-parent relationships. Minor differences with du &lt;dir&gt; may occur.\n\n\nCode\nfig = px.sunburst(df, \n                  path=[px.Constant(folder),'parent', 'node'], \n                  values='size', \n                  color='parent',\n#                  color_continuous_scale='Blues'\n                  )\nfig.update_layout(title_text=\"Content of directory = \" + folder +  \"  Command: \" + cmd, font_size=10)\nfig.show()\n\n\n\n\n                                                \nFigure 2: Sunburst of direcory folder\n\n\n\nVisualization 2: treemap\n\n\nCode\nfig = px.treemap(df, \n                  path=[px.Constant(folder),'parent', 'node'], \n                  values='size', \n                  color='parent',\n#                  color_continuous_scale='Blues'\n                  )\nfig.update_layout(title_text=\"Content of directory = \" + folder +  \"  Command: \" + cmd, font_size=10)\nfig.show()\n\n\n\n\n                                                \nFigure 3: Treemap of direcory."
  },
  {
    "objectID": "posts/trees_python/trees_python_revealjs.html#watermark-and-package-versions",
    "href": "posts/trees_python/trees_python_revealjs.html#watermark-and-package-versions",
    "title": "Trees with Python (qmd)",
    "section": "Watermark and Package Versions",
    "text": "Watermark and Package Versions\n\n\n\n\n\n\nwatermark\n\n\n\n\nCode\nimport watermark\nimport pkg_resources\n\n\n# Display watermark information\nprint(watermark.watermark())\n\n\n# Get installed module versions\ninstalled_packages = pkg_resources.working_set\n\n# Print module versions\nprint(\"Module versions:\")\nfor package in installed_packages:\n    print(package.key, package.version)\n\n\nLast updated: 2023-06-22T23:36:19.870244+02:00\n\nPython implementation: CPython\nPython version       : 3.11.4\nIPython version      : 8.12.0\n\nCompiler    : Clang 15.0.7 \nOS          : Darwin\nRelease     : 21.6.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nModule versions:\njinja2 3.1.2\nmarkupsafe 2.1.2\npillow 9.4.0\npyqt5 5.15.7\npyqt5-sip 12.11.0\npysocks 1.7.1\npyyaml 6.0\npygments 2.14.0\nqtpy 2.3.1\nsqlalchemy 2.0.16\nsend2trash 1.8.0\naltair 5.0.1\nanyio 3.6.2\nappnope 0.1.3\nargon2-cffi 21.3.0\nargon2-cffi-bindings 21.2.0\nasttokens 2.2.1\nattrs 22.2.0\nbackcall 0.2.0\nbackports.functools-lru-cache 1.6.4\nbeautifulsoup4 4.12.0\nbleach 6.0.0\nbrotlipy 0.7.0\ncertifi 2023.5.7\ncffi 1.15.1\ncharset-normalizer 3.1.0\nclick 8.1.3\ncomm 0.1.3\ncontourpy 1.0.7\ncryptography 40.0.1\ncycler 0.11.0\ndebugpy 1.6.6\ndecorator 5.1.1\ndefusedxml 0.7.1\nentrypoints 0.4\nexecuting 1.2.0\nfastjsonschema 2.16.3\nflit-core 3.8.0\nfonttools 4.39.3\ngreenlet 2.0.2\nidna 3.4\nimportlib-metadata 6.1.0\nimportlib-resources 5.12.0\nipykernel 6.22.0\nipython 8.12.0\nipython-genutils 0.2.0\nipywidgets 8.0.6\njedi 0.18.2\njsonschema 4.17.3\njupyter 1.0.0\njupyter-cache 0.6.1\njupyter-client 8.1.0\njupyter-console 6.6.3\njupyter-core 5.3.0\njupyter-events 0.6.3\njupyter-server 2.5.0\njupyter-server-terminals 0.4.4\njupyterlab-pygments 0.2.2\njupyterlab-widgets 3.0.7\nkaleido 0.2.1\nkiwisolver 1.4.4\nmatplotlib 3.7.1\nmatplotlib-inline 0.1.6\nmistune 2.0.5\nmunkres 1.1.4\nnbclassic 0.5.3\nnbclient 0.7.4\nnbconvert 7.2.9\nnbformat 5.9.0\nnest-asyncio 1.5.6\nnotebook 6.5.3\nnotebook-shim 0.2.2\nnumpy 1.23.5\npackaging 23.0\npandas 2.0.2\npandocfilters 1.5.0\nparso 0.8.3\npatsy 0.5.3\npexpect 4.8.0\npickleshare 0.7.5\npip 23.0.1\npkgutil-resolve-name 1.3.10\nplatformdirs 3.2.0\nplotly 5.15.0\nply 3.11\npooch 1.7.0\nprometheus-client 0.16.0\nprompt-toolkit 3.0.38\npsutil 5.9.4\nptyprocess 0.7.0\npure-eval 0.2.2\npyopenssl 23.1.1\npycparser 2.21\npyparsing 3.0.9\npyrsistent 0.19.3\npython-dateutil 2.8.2\npython-json-logger 2.0.7\npytz 2023.3\npyzmq 25.0.2\nqtconsole 5.4.2\nrequests 2.28.2\nrfc3339-validator 0.1.4\nrfc3986-validator 0.1.1\nrpy2 3.5.11\nscipy 1.10.1\nseaborn 0.12.2\nsetuptools 67.6.1\nsimplegeneric 0.8.1\nsip 6.7.7\nsix 1.16.0\nsniffio 1.3.0\nsoupsieve 2.3.2.post1\nstack-data 0.6.2\nstatsmodels 0.14.0\ntabulate 0.9.0\ntenacity 8.2.2\nterminado 0.17.1\ntinycss2 1.2.1\ntoml 0.10.2\ntoolz 0.12.0\ntornado 6.2\ntraitlets 5.9.0\ntyping-extensions 4.5.0\ntzdata 2023.3\ntzlocal 5.0.1\nurllib3 1.26.15\nwatermark 2.3.1\nwcwidth 0.2.6\nwebencodings 0.5.1\nwebsocket-client 1.5.1\nwheel 0.40.0\nwidgetsnbextension 4.0.7\nzipp 3.15.0\n\n\n\n\n\n\n\nThomas Manke"
  },
  {
    "objectID": "posts/variance_stabilization/vst.html",
    "href": "posts/variance_stabilization/vst.html",
    "title": "Variance Stabilization",
    "section": "",
    "text": "Let \\(X\\) be a random variable: \\(X \\propto p(\\theta)\\)\nLet \\(f(X)\\) be a transformation \\(X \\to Y = f(X)\\)\nThen we have the following relations between the various expectations. \\[\n\\begin{array}{lll}\nE[X]   &=\\mu      & \\to E[Y] = f(\\mu) \\\\\nVar[X] &=\\sigma^2 & \\to Var[Y] = |f'(X=\\mu)|^2 \\times \\sigma^2\n\\end{array}\n\\]\nLet’s illustrate this with \\(X \\propto Pois(\\lambda)\\) and two different transformations:\n\\(f(X)=X^2\\) (left plot) and \\(f(X)= 2\\sqrt{X}\\) (right plot). In both cases \\(X\\) was sampled from three different distributions: \\(\\lambda=(10,50,100)\\).\n\n\nCode for X ~ Poisson(mu)\nset.seed(42)\nN=100\nmu=c(10, 50, 100)                    # means for Poisson\nx &lt;- rpois(N*length(mu), lambda=mu)  # generate x ~ Pois\ng &lt;- rep(mu,N)                       # group label\n\n# data frame with two different transformation\ndf = data.frame(x, y1=x^2, y2=2*sqrt(x), g)\n\nleg &lt;- labs(color=expression(mu)) \nth  &lt;- theme(legend.position=\"bottom\", \n             legend.text=element_text(size=rel(2)),\n             legend.title = element_text(size=rel(2)),\n             axis.title = element_text(size=rel(2)) )\nyl1 &lt;- ylab(expression(\"Y=\"~X^2))\nyl2 &lt;- ylab(expression(\"Y=\"~2 * sqrt(X)))\n\np1 &lt;- df %&gt;%  ggplot(aes(x,y1,color=as.factor(g))) + geom_point() + leg + yl1 + th\np2 &lt;- df %&gt;%  ggplot(aes(x,y2,color=as.factor(g))) + geom_point() + leg + yl2 + th\n\np1m &lt;- ggMarginal(p1, type=\"histogram\", groupFill=TRUE)\np2m &lt;- ggMarginal(p2, type=\"histogram\", groupFill=TRUE)\n\npatchwork::wrap_elements(p1m) + patchwork::wrap_elements(p2m) + plot_layout(ncol=2)\nggsave('vst.png')\n\n\nSaving 16 x 8 in image\n\n\n\n\n\n\n\nX is sampled from 3 different Poisson distributions having different means (and variances). Two different transformations, Y=f(X), illustrate how mean and variances change under transformation. In the example on the right the variance is stabilised by the transformation.\n\n\n\n\n\n\n\n\nCode\ndf %&gt;% \n  group_by(g) %&gt;% \n  summarize(mean_x=mean(x), var_x=var(x), var_y1=var(y1), var_y2=var(y2)) %&gt;%\n  knitr::kable(digits = 1)\n\n\n\n\nSome transformations “stabilize the variance” \n\n\ng\nmean_x\nvar_x\nvar_y1\nvar_y2\n\n\n\n\n10\n10.3\n13.0\n7080.3\n1.2\n\n\n50\n50.2\n64.5\n657231.9\n1.3\n\n\n100\n99.8\n98.2\n4004616.3\n1.0\n\n\n\n\nNotice that \\(f(X) = 2 \\sqrt{X}\\) “stabilizes” the variance of the transformed variable \\(Y\\).\nThis finding can be generalized."
  },
  {
    "objectID": "posts/variance_stabilization/vst.html#error-propagation",
    "href": "posts/variance_stabilization/vst.html#error-propagation",
    "title": "Variance Stabilization",
    "section": "",
    "text": "Let \\(X\\) be a random variable: \\(X \\propto p(\\theta)\\)\nLet \\(f(X)\\) be a transformation \\(X \\to Y = f(X)\\)\nThen we have the following relations between the various expectations. \\[\n\\begin{array}{lll}\nE[X]   &=\\mu      & \\to E[Y] = f(\\mu) \\\\\nVar[X] &=\\sigma^2 & \\to Var[Y] = |f'(X=\\mu)|^2 \\times \\sigma^2\n\\end{array}\n\\]\nLet’s illustrate this with \\(X \\propto Pois(\\lambda)\\) and two different transformations:\n\\(f(X)=X^2\\) (left plot) and \\(f(X)= 2\\sqrt{X}\\) (right plot). In both cases \\(X\\) was sampled from three different distributions: \\(\\lambda=(10,50,100)\\).\n\n\nCode for X ~ Poisson(mu)\nset.seed(42)\nN=100\nmu=c(10, 50, 100)                    # means for Poisson\nx &lt;- rpois(N*length(mu), lambda=mu)  # generate x ~ Pois\ng &lt;- rep(mu,N)                       # group label\n\n# data frame with two different transformation\ndf = data.frame(x, y1=x^2, y2=2*sqrt(x), g)\n\nleg &lt;- labs(color=expression(mu)) \nth  &lt;- theme(legend.position=\"bottom\", \n             legend.text=element_text(size=rel(2)),\n             legend.title = element_text(size=rel(2)),\n             axis.title = element_text(size=rel(2)) )\nyl1 &lt;- ylab(expression(\"Y=\"~X^2))\nyl2 &lt;- ylab(expression(\"Y=\"~2 * sqrt(X)))\n\np1 &lt;- df %&gt;%  ggplot(aes(x,y1,color=as.factor(g))) + geom_point() + leg + yl1 + th\np2 &lt;- df %&gt;%  ggplot(aes(x,y2,color=as.factor(g))) + geom_point() + leg + yl2 + th\n\np1m &lt;- ggMarginal(p1, type=\"histogram\", groupFill=TRUE)\np2m &lt;- ggMarginal(p2, type=\"histogram\", groupFill=TRUE)\n\npatchwork::wrap_elements(p1m) + patchwork::wrap_elements(p2m) + plot_layout(ncol=2)\nggsave('vst.png')\n\n\nSaving 16 x 8 in image\n\n\n\n\n\n\n\nX is sampled from 3 different Poisson distributions having different means (and variances). Two different transformations, Y=f(X), illustrate how mean and variances change under transformation. In the example on the right the variance is stabilised by the transformation.\n\n\n\n\n\n\n\n\nCode\ndf %&gt;% \n  group_by(g) %&gt;% \n  summarize(mean_x=mean(x), var_x=var(x), var_y1=var(y1), var_y2=var(y2)) %&gt;%\n  knitr::kable(digits = 1)\n\n\n\n\nSome transformations “stabilize the variance” \n\n\ng\nmean_x\nvar_x\nvar_y1\nvar_y2\n\n\n\n\n10\n10.3\n13.0\n7080.3\n1.2\n\n\n50\n50.2\n64.5\n657231.9\n1.3\n\n\n100\n99.8\n98.2\n4004616.3\n1.0\n\n\n\n\nNotice that \\(f(X) = 2 \\sqrt{X}\\) “stabilizes” the variance of the transformed variable \\(Y\\).\nThis finding can be generalized."
  },
  {
    "objectID": "posts/variance_stabilization/vst.html#variance-stabilization",
    "href": "posts/variance_stabilization/vst.html#variance-stabilization",
    "title": "Variance Stabilization",
    "section": "Variance Stabilization",
    "text": "Variance Stabilization\nAssume our observed data is “heteroskedastic”, i.e. \\(VAR[X]\\) is some function of \\(E[X] \\equiv \\mu\\)\nOne specific example could be a Negative Binomial: \\(X \\propto NB(\\mu, \\alpha=1/\\theta)\\) where\n\\[\nVar[X] = g(E[X]) = \\mu + \\alpha \\cdot \\mu^2\n\\]\nThis heteroskedastisity can be removed by choosing a transformation \\(f\\) such that \\(|f'(x=\\mu)|^2\\) is the inverse of \\(g\\).\n\n\nfor \\(\\alpha = 0\\): chose \\(|f'(x=\\mu)|^2 = \\mu^{-1} \\to f(x)=2\\sqrt{x}\\)\nfor \\(\\alpha &gt; 0\\): chose \\(|f'(x=\\mu)|^2 = (\\mu + \\alpha\\mu^2)^{-1} \\to\\) \\[\nf(x) = \\frac{1}{\\sqrt{\\alpha}} acosh(2\\alpha x + 1) \\\\\n\\] …exact solution if \\(\\alpha\\) is known"
  },
  {
    "objectID": "posts/variance_stabilization/vst.html#simpler-transformations",
    "href": "posts/variance_stabilization/vst.html#simpler-transformations",
    "title": "Variance Stabilization",
    "section": "Simpler Transformations",
    "text": "Simpler Transformations\nOnly the derivative \\(f'(x=\\mu)\\) is relevant for variance stabilization, so we can have equivalent transformation for variance stabilization.\n\n\nCode\nx=1:100                         # range of x\na=0.5                           # dispersion\n\ngc=acosh(2*a*x + 1)   # exact: upto division by sqrt(a)\ngl=log(4*a*x+1)       # good for large x\ng2= log(x + 1/(4*a))  # shifted by log(4a) with same derivative\n\ndata.frame(x, acosh=gc, log1=gl, log2=g2) %&gt;%\n  ggplot() +\n  geom_point(aes(x,acosh,color=\"acosh\")) +\n  geom_point(aes(x,log1,color=\"log(4ax+1)\")) + \n  geom_point(aes(x,log2,color=\"log(x + 1/4a)\")) + \n  ylab(\"Y=f(x)\") +\n  theme(legend.text=element_text(size=rel(1.5)))\n\n\n\n\n\n\n\nThere are equivalent or approximate transformations. Notice that only the derivative is important for the purpose of variance stabilzation\n\n\n\n\n\n\nLog-Normalization\n\\(\\log(x + 1/4\\alpha)\\) has same derivative as \\(acosh(2\\alpha x +1)\\) ✓"
  },
  {
    "objectID": "posts/variance_stabilization/revealjs.html#error-propagation",
    "href": "posts/variance_stabilization/revealjs.html#error-propagation",
    "title": "Variance Stabilization",
    "section": "Error Propagation",
    "text": "Error Propagation\nLet \\(X\\) be a random variable: \\(X \\propto p(\\theta)\\)\nLet \\(f(X)\\) be a transformation \\(X \\to Y = f(X)\\)\nThen we have the following relations between the various expectations. \\[\n\\begin{array}{lll}\nE[X]   &=\\mu      & \\to E[Y] = f(\\mu) \\\\\nVar[X] &=\\sigma^2 & \\to Var[Y] = |f'(X=\\mu)|^2 \\times \\sigma^2\n\\end{array}\n\\]\nLet’s illustrate this with \\(X \\propto Pois(\\lambda)\\) and two different transformations:\n\\(f(X)=X^2\\) (left plot) and \\(f(X)= 2\\sqrt{X}\\) (right plot). In both cases \\(X\\) was sampled from three different distributions: \\(\\lambda=(10,50,100)\\).\n\n\nCode for X ~ Poisson(mu)\nset.seed(42)\nN=100\nmu=c(10, 50, 100)                    # means for Poisson\nx &lt;- rpois(N*length(mu), lambda=mu)  # generate x ~ Pois\ng &lt;- rep(mu,N)                       # group label\n\n# data frame with two different transformation\ndf = data.frame(x, y1=x^2, y2=2*sqrt(x), g)\n\nleg &lt;- labs(color=expression(mu)) \nth  &lt;- theme(legend.position=\"bottom\", \n             legend.text=element_text(size=rel(2)),\n             legend.title = element_text(size=rel(2)),\n             axis.title = element_text(size=rel(2)) )\nyl1 &lt;- ylab(expression(\"Y=\"~X^2))\nyl2 &lt;- ylab(expression(\"Y=\"~2 * sqrt(X)))\n\np1 &lt;- df %&gt;%  ggplot(aes(x,y1,color=as.factor(g))) + geom_point() + leg + yl1 + th\np2 &lt;- df %&gt;%  ggplot(aes(x,y2,color=as.factor(g))) + geom_point() + leg + yl2 + th\n\np1m &lt;- ggMarginal(p1, type=\"histogram\", groupFill=TRUE)\np2m &lt;- ggMarginal(p2, type=\"histogram\", groupFill=TRUE)\n\npatchwork::wrap_elements(p1m) + patchwork::wrap_elements(p2m) + plot_layout(ncol=2)\nggsave('vst.png')\n\n\nSaving 16 x 8 in image\n\n\n\n\n\n\n\nX is sampled from 3 different Poisson distributions having different means (and variances). Two different transformations, Y=f(X), illustrate how mean and variances change under transformation. In the example on the right the variance is stabilised by the transformation.\n\n\n\n\n\n\n\n\nCode\ndf %&gt;% \n  group_by(g) %&gt;% \n  summarize(mean_x=mean(x), var_x=var(x), var_y1=var(y1), var_y2=var(y2)) %&gt;%\n  knitr::kable(digits = 1)\n\n\n\nSome transformations “stabilize the variance” \n\n\ng\nmean_x\nvar_x\nvar_y1\nvar_y2\n\n\n\n\n10\n10.3\n13.0\n7080.3\n1.2\n\n\n50\n50.2\n64.5\n657231.9\n1.3\n\n\n100\n99.8\n98.2\n4004616.3\n1.0\n\n\n\n\n\nNotice that \\(f(X) = 2 \\sqrt{X}\\) “stabilizes” the variance of the transformed variable \\(Y\\).\nThis finding can be generalized."
  },
  {
    "objectID": "posts/variance_stabilization/revealjs.html#variance-stabilization",
    "href": "posts/variance_stabilization/revealjs.html#variance-stabilization",
    "title": "Variance Stabilization",
    "section": "Variance Stabilization",
    "text": "Variance Stabilization\nAssume our observed data is “heteroskedastic”, i.e. \\(VAR[X]\\) is some function of \\(E[X] \\equiv \\mu\\)\nOne specific example could be a Negative Binomial: \\(X \\propto NB(\\mu, \\alpha=1/\\theta)\\) where\n\\[\nVar[X] = g(E[X]) = \\mu + \\alpha \\cdot \\mu^2\n\\]\nThis heteroskedastisity can be removed by choosing a transformation \\(f\\) such that \\(|f'(x=\\mu)|^2\\) is the inverse of \\(g\\).\n\n\nfor \\(\\alpha = 0\\): chose \\(|f'(x=\\mu)|^2 = \\mu^{-1} \\to f(x)=2\\sqrt{x}\\)\nfor \\(\\alpha &gt; 0\\): chose \\(|f'(x=\\mu)|^2 = (\\mu + \\alpha\\mu^2)^{-1} \\to\\) \\[\nf(x) = \\frac{1}{\\sqrt{\\alpha}} acosh(2\\alpha x + 1) \\\\\n\\] …exact solution if \\(\\alpha\\) is known"
  },
  {
    "objectID": "posts/variance_stabilization/revealjs.html#simpler-transformations",
    "href": "posts/variance_stabilization/revealjs.html#simpler-transformations",
    "title": "Variance Stabilization",
    "section": "Simpler Transformations",
    "text": "Simpler Transformations\nOnly the derivative \\(f'(x=\\mu)\\) is relevant for variance stabilization, so we can have equivalent transformation for variance stabilization.\n\n\nCode\nx=1:100                         # range of x\na=0.5                           # dispersion\n\ngc=acosh(2*a*x + 1)   # exact: upto division by sqrt(a)\ngl=log(4*a*x+1)       # good for large x\ng2= log(x + 1/(4*a))  # shifted by log(4a) with same derivative\n\ndata.frame(x, acosh=gc, log1=gl, log2=g2) %&gt;%\n  ggplot() +\n  geom_point(aes(x,acosh,color=\"acosh\")) +\n  geom_point(aes(x,log1,color=\"log(4ax+1)\")) + \n  geom_point(aes(x,log2,color=\"log(x + 1/4a)\")) + \n  ylab(\"Y=f(x)\") +\n  theme(legend.text=element_text(size=rel(1.5)))\n\n\n\n\n\n\n\nThere are equivalent or approximate transformations. Notice that only the derivative is important for the purpose of variance stabilzation\n\n\n\n\n\n\nLog-Normalization\n\\(\\log(x + 1/4\\alpha)\\) has same derivative as \\(acosh(2\\alpha x +1)\\) ✓\n\n\nThomas Manke"
  },
  {
    "objectID": "posts/maps/maps.html",
    "href": "posts/maps/maps.html",
    "title": "Geographic Maps with R",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(plotly)\n\n# for map definitions and manipulations\nlibrary(sf)\nlibrary(\"rnaturalearth\")\n#library(\"rnaturalearthdata\")\nlibrary(maps)\n\nlibrary(gapminder)     # for some world data\nlibrary(RColorBrewer)  # for color maps"
  },
  {
    "objectID": "posts/maps/maps.html#more-examples",
    "href": "posts/maps/maps.html#more-examples",
    "title": "Geographic Maps with R",
    "section": "More Examples",
    "text": "More Examples\nNext I repeat the same for other data sets: gapminder and a custom data set with emigration data from an Excel file.\nNotice that some countries are missing because map by name failed (e.g. USA). This should be fixed, but I leave it here to highlight the challenge.\n\n\nCode\n# gapminder data\nggplot(data=gapminder) +\n  geom_map(map=world_map, \n           aes(fill=lifeExp, map_id=country), colour = \"#7f7f7f\") +\n  expand_limits(x = world_map$long, y = world_map$lat) +\n  ggtitle('world_map with gapminder(lifeExp)')\n\n\n\n\n\nCode\n# read emigration data from Excel file\nfn = \"data/20200501_emigrant_remittance_data_visualization_raw.xlsx\"\nemigration_data = readxl::read_excel(path = fn)\n\n#simplify the column names\ncolnames(emigration_data)=c(\"country\", \"Pop2019\", \"Emi2019\", \"Share_t\", \"Share_i\", \"Remit2019_V\", \"Remit2019_GDP\")\n\n# using the orginal world_map (long, lat, region) and data emigration_data\n# where emigration_data$country is used to map to world_map$region\n\nggplot(data=emigration_data) +\n  geom_map(map=world_map, \n           aes(fill=Emi2019, map_id=country), colour = \"#7f7f7f\") +\n  expand_limits(x = world_map$long, y = world_map$lat) +\n  ggtitle('world_map with own data(Emi2019)')"
  },
  {
    "objectID": "posts/maps/maps.html#cooordinates-fine-tuning",
    "href": "posts/maps/maps.html#cooordinates-fine-tuning",
    "title": "Geographic Maps with R",
    "section": "Cooordinates: fine tuning",
    "text": "Cooordinates: fine tuning\nThere are various coordinate systems. We can use coord_sf() to make specific choices for beautification.\nHere I revisit the plot of USA murder statistics from above\n\n\nCode\n# crs = 5070 is a Conus Albers projection for North America, see: https://epsg.io/5070\n# default_crs = 4326 tells coord_sf() that the input map data are in longitude-latitude format\n\ncoords &lt;- ggplot2::coord_sf(\n  crs = 5070, default_crs = 4326,\n  xlim = c(-125, -70), ylim = c(25, 52)\n  )\n\n# p was the defined above\np + coords"
  },
  {
    "objectID": "posts/maps/maps.html#multiple-maps",
    "href": "posts/maps/maps.html#multiple-maps",
    "title": "Geographic Maps with R",
    "section": "Multiple Maps",
    "text": "Multiple Maps\nUsing a long data format and facet_wrap we can also plot multiple maps.\n\n\nCode\nd_long &lt;- d %&gt;% pivot_longer(Murder:Rape)\nd_long %&gt;% head(3)\n\n\n# A tibble: 3 × 3\n  state   name     value\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1 alabama Murder    13.2\n2 alabama Assault  236  \n3 alabama UrbanPop  58  \n\n\nCode\nggplot(d_long) +\n  geom_map(map = states_map, aes(map_id = state, fill = value) ) +\n  coords + \n  facet_wrap(~name)"
  },
  {
    "objectID": "posts/maps/maps.html#sf-format-and-geom_sf",
    "href": "posts/maps/maps.html#sf-format-and-geom_sf",
    "title": "Geographic Maps with R",
    "section": "sf format and geom_sf",
    "text": "sf format and geom_sf\nsf is a common data standard for simple feature objects such as geographical data.\nThe sf package provides many tools for reading and transforming sf data.\nggplot2 provides a convenient interface geom_sf to plot such data.\n\n\nCode\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"), quiet = TRUE)\nglimpse(nc)\n\n\nRows: 100\nColumns: 15\n$ AREA      &lt;dbl&gt; 0.114, 0.061, 0.143, 0.070, 0.153, 0.097, 0.062, 0.091, 0.11…\n$ PERIMETER &lt;dbl&gt; 1.442, 1.231, 1.630, 2.968, 2.206, 1.670, 1.547, 1.284, 1.42…\n$ CNTY_     &lt;dbl&gt; 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 1837, …\n$ CNTY_ID   &lt;dbl&gt; 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 1837, …\n$ NAME      &lt;chr&gt; \"Ashe\", \"Alleghany\", \"Surry\", \"Currituck\", \"Northampton\", \"H…\n$ FIPS      &lt;chr&gt; \"37009\", \"37005\", \"37171\", \"37053\", \"37131\", \"37091\", \"37029…\n$ FIPSNO    &lt;dbl&gt; 37009, 37005, 37171, 37053, 37131, 37091, 37029, 37073, 3718…\n$ CRESS_ID  &lt;int&gt; 5, 3, 86, 27, 66, 46, 15, 37, 93, 85, 17, 79, 39, 73, 91, 42…\n$ BIR74     &lt;dbl&gt; 1091, 487, 3188, 508, 1421, 1452, 286, 420, 968, 1612, 1035,…\n$ SID74     &lt;dbl&gt; 1, 0, 5, 1, 9, 7, 0, 0, 4, 1, 2, 16, 4, 4, 4, 18, 3, 4, 1, 1…\n$ NWBIR74   &lt;dbl&gt; 10, 10, 208, 123, 1066, 954, 115, 254, 748, 160, 550, 1243, …\n$ BIR79     &lt;dbl&gt; 1364, 542, 3616, 830, 1606, 1838, 350, 594, 1190, 2038, 1253…\n$ SID79     &lt;dbl&gt; 0, 3, 6, 2, 3, 5, 2, 2, 2, 5, 2, 5, 4, 4, 6, 17, 4, 7, 1, 0,…\n$ NWBIR79   &lt;dbl&gt; 19, 12, 260, 145, 1197, 1237, 139, 371, 844, 176, 597, 1369,…\n$ geometry  &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-81.47276 3..., MULTIPOLYGON ((…\n\n\nCode\nggplot(nc) + geom_sf(aes(fill = AREA))\n\n\n\n\n\nCode\nnc_3857 &lt;- sf::st_transform(nc, 3857)\nggplot(nc_3857) + geom_sf(aes(fill = PERIMETER))"
  },
  {
    "objectID": "posts/maps/maps.html#adding-new-data",
    "href": "posts/maps/maps.html#adding-new-data",
    "title": "Geographic Maps with R",
    "section": "Adding new data",
    "text": "Adding new data\nWe can add new data to the sf object …\n\n\nCode\n# here we join additional data into the world map\nD=world %&gt;% inner_join(gapminder, by=c(\"name\"=\"country\"))\n\n# D now contains coordinate `geometry` from world and \n# additional data from gapminder - we plot it again with geom_sf()\n\np &lt;- D %&gt;% \n  # keep plot size manageable\n  filter(year&gt;=1992) %&gt;%                                         \n  ggplot(data = .) +\n  # add labels to be shown in plotly hover\n  geom_sf(aes(fill=log10(pop),label=name, label1=year)) +\n  # custom colour scheme\n  scale_fill_gradient2(low=\"white\", high=\"red\", midpoint=6) +\n  # default map\n  coord_sf(crs = st_crs(\"+proj=longlat +datum=WGS84\")) +         \n  # Mercator map\n  #  coord_sf(crs = st_crs(3857)) +    \n  # separate plot for each year\n  facet_wrap(~year) + \n  ggtitle('Map of world with gapminder data')\n\n#p\n\n# use plotly for more interactivity - just because we can\nggplotly(p)"
  },
  {
    "objectID": "posts/maps/maps.html#maps-with-maps",
    "href": "posts/maps/maps.html#maps-with-maps",
    "title": "Geographic Maps with R",
    "section": "Maps with maps",
    "text": "Maps with maps\n\n\nCode\nmaps::map()         # map of workd\n\n\n\n\n\nCode\nmaps::map(\"state\")  # map of US states\n\n\n\n\n\nCode\n# pacific-centered map of the world\nmaps::map(wrap = c(0,360), fill = TRUE, col = \"lightblue\")"
  },
  {
    "objectID": "posts/maps/maps.html#map-manipulation-with-maps-and-sf",
    "href": "posts/maps/maps.html#map-manipulation-with-maps-and-sf",
    "title": "Geographic Maps with R",
    "section": "Map Manipulation with maps and sf",
    "text": "Map Manipulation with maps and sf\n\n\nCode\n# convert map object (list) to sf object (data frame)\ncounties &lt;- st_as_sf(map(\"county\", plot = FALSE, fill = TRUE))\n#states &lt;- st_as_sf(map(\"state\", plot = FALSE, fill = TRUE)) # unused here\n\n# subset US counties in Florida and check if polygon is valid (not for all!)\ncounties &lt;- counties %&gt;% \n  subset(grepl(\"florida\", counties$ID)) %&gt;% \n  filter(st_is_valid(.))\n\n# Calculate the county area\ncounties$area &lt;- as.numeric(st_area(counties))\n\n# ggplot with geom_sf() - a more modern version of geom_map\nggplot(data = world) +\n    # first show map and borders beyond Florida counties\n    geom_sf() + \n    # now overlay the filled counties of Florida\n    geom_sf(data = counties, aes(fill = area)) +\n    # chose colour scheme\n    scale_fill_viridis_c(trans = \"sqrt\", alpha = .4) +\n    # define coordinates\n    coord_sf(xlim = c(-88, -78), ylim = c(24.5, 33), expand = FALSE)"
  },
  {
    "objectID": "posts/maps/maps.html#and-there-is-more",
    "href": "posts/maps/maps.html#and-there-is-more",
    "title": "Geographic Maps with R",
    "section": "And there is more …",
    "text": "And there is more …"
  },
  {
    "objectID": "posts/maps/revealjs.html#more-examples",
    "href": "posts/maps/revealjs.html#more-examples",
    "title": "Geographic Maps with R",
    "section": "More Examples",
    "text": "More Examples\nNext I repeat the same for other data sets: gapminder and a custom data set with emigration data from an Excel file.\nNotice that some countries are missing because map by name failed (e.g. USA). This should be fixed, but I leave it here to highlight the challenge.\n\n\nCode\n# gapminder data\nggplot(data=gapminder) +\n  geom_map(map=world_map, \n           aes(fill=lifeExp, map_id=country), colour = \"#7f7f7f\") +\n  expand_limits(x = world_map$long, y = world_map$lat) +\n  ggtitle('world_map with gapminder(lifeExp)')\n\n\n\n\n\nCode\n# read emigration data from Excel file\nfn = \"data/20200501_emigrant_remittance_data_visualization_raw.xlsx\"\nemigration_data = readxl::read_excel(path = fn)\n\n#simplify the column names\ncolnames(emigration_data)=c(\"country\", \"Pop2019\", \"Emi2019\", \"Share_t\", \"Share_i\", \"Remit2019_V\", \"Remit2019_GDP\")\n\n# using the orginal world_map (long, lat, region) and data emigration_data\n# where emigration_data$country is used to map to world_map$region\n\nggplot(data=emigration_data) +\n  geom_map(map=world_map, \n           aes(fill=Emi2019, map_id=country), colour = \"#7f7f7f\") +\n  expand_limits(x = world_map$long, y = world_map$lat) +\n  ggtitle('world_map with own data(Emi2019)')"
  },
  {
    "objectID": "posts/maps/revealjs.html#cooordinates-fine-tuning",
    "href": "posts/maps/revealjs.html#cooordinates-fine-tuning",
    "title": "Geographic Maps with R",
    "section": "Cooordinates: fine tuning",
    "text": "Cooordinates: fine tuning\nThere are various coordinate systems. We can use coord_sf() to make specific choices for beautification.\nHere I revisit the plot of USA murder statistics from above\n\n\nCode\n# crs = 5070 is a Conus Albers projection for North America, see: https://epsg.io/5070\n# default_crs = 4326 tells coord_sf() that the input map data are in longitude-latitude format\n\ncoords &lt;- ggplot2::coord_sf(\n  crs = 5070, default_crs = 4326,\n  xlim = c(-125, -70), ylim = c(25, 52)\n  )\n\n# p was the defined above\np + coords"
  },
  {
    "objectID": "posts/maps/revealjs.html#multiple-maps",
    "href": "posts/maps/revealjs.html#multiple-maps",
    "title": "Geographic Maps with R",
    "section": "Multiple Maps",
    "text": "Multiple Maps\nUsing a long data format and facet_wrap we can also plot multiple maps.\n\n\nCode\nd_long &lt;- d %&gt;% pivot_longer(Murder:Rape)\nd_long %&gt;% head(3)\n\n\n# A tibble: 3 × 3\n  state   name     value\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1 alabama Murder    13.2\n2 alabama Assault  236  \n3 alabama UrbanPop  58  \n\n\nCode\nggplot(d_long) +\n  geom_map(map = states_map, aes(map_id = state, fill = value) ) +\n  coords + \n  facet_wrap(~name)"
  },
  {
    "objectID": "posts/maps/revealjs.html#sf-format-and-geom_sf",
    "href": "posts/maps/revealjs.html#sf-format-and-geom_sf",
    "title": "Geographic Maps with R",
    "section": "sf format and geom_sf",
    "text": "sf format and geom_sf\nsf is a common data standard for simple feature objects such as geographical data.\nThe sf package provides many tools for reading and transforming sf data.\nggplot2 provides a convenient interface geom_sf to plot such data.\n\n\nCode\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"), quiet = TRUE)\nglimpse(nc)\n\n\nRows: 100\nColumns: 15\n$ AREA      &lt;dbl&gt; 0.114, 0.061, 0.143, 0.070, 0.153, 0.097, 0.062, 0.091, 0.11…\n$ PERIMETER &lt;dbl&gt; 1.442, 1.231, 1.630, 2.968, 2.206, 1.670, 1.547, 1.284, 1.42…\n$ CNTY_     &lt;dbl&gt; 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 1837, …\n$ CNTY_ID   &lt;dbl&gt; 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 1837, …\n$ NAME      &lt;chr&gt; \"Ashe\", \"Alleghany\", \"Surry\", \"Currituck\", \"Northampton\", \"H…\n$ FIPS      &lt;chr&gt; \"37009\", \"37005\", \"37171\", \"37053\", \"37131\", \"37091\", \"37029…\n$ FIPSNO    &lt;dbl&gt; 37009, 37005, 37171, 37053, 37131, 37091, 37029, 37073, 3718…\n$ CRESS_ID  &lt;int&gt; 5, 3, 86, 27, 66, 46, 15, 37, 93, 85, 17, 79, 39, 73, 91, 42…\n$ BIR74     &lt;dbl&gt; 1091, 487, 3188, 508, 1421, 1452, 286, 420, 968, 1612, 1035,…\n$ SID74     &lt;dbl&gt; 1, 0, 5, 1, 9, 7, 0, 0, 4, 1, 2, 16, 4, 4, 4, 18, 3, 4, 1, 1…\n$ NWBIR74   &lt;dbl&gt; 10, 10, 208, 123, 1066, 954, 115, 254, 748, 160, 550, 1243, …\n$ BIR79     &lt;dbl&gt; 1364, 542, 3616, 830, 1606, 1838, 350, 594, 1190, 2038, 1253…\n$ SID79     &lt;dbl&gt; 0, 3, 6, 2, 3, 5, 2, 2, 2, 5, 2, 5, 4, 4, 6, 17, 4, 7, 1, 0,…\n$ NWBIR79   &lt;dbl&gt; 19, 12, 260, 145, 1197, 1237, 139, 371, 844, 176, 597, 1369,…\n$ geometry  &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-81.47276 3..., MULTIPOLYGON ((…\n\n\nCode\nggplot(nc) + geom_sf(aes(fill = AREA))\n\n\n\n\n\nCode\nnc_3857 &lt;- sf::st_transform(nc, 3857)\nggplot(nc_3857) + geom_sf(aes(fill = PERIMETER))"
  },
  {
    "objectID": "posts/maps/revealjs.html#adding-new-data",
    "href": "posts/maps/revealjs.html#adding-new-data",
    "title": "Geographic Maps with R",
    "section": "Adding new data",
    "text": "Adding new data\nWe can add new data to the sf object …\n\n\nCode\n# here we join additional data into the world map\nD=world %&gt;% inner_join(gapminder, by=c(\"name\"=\"country\"))\n\n# D now contains coordinate `geometry` from world and \n# additional data from gapminder - we plot it again with geom_sf()\n\np &lt;- D %&gt;% \n  # keep plot size manageable\n  filter(year&gt;=1992) %&gt;%                                         \n  ggplot(data = .) +\n  # add labels to be shown in plotly hover\n  geom_sf(aes(fill=log10(pop),label=name, label1=year)) +\n  # custom colour scheme\n  scale_fill_gradient2(low=\"white\", high=\"red\", midpoint=6) +\n  # default map\n  coord_sf(crs = st_crs(\"+proj=longlat +datum=WGS84\")) +         \n  # Mercator map\n  #  coord_sf(crs = st_crs(3857)) +    \n  # separate plot for each year\n  facet_wrap(~year) + \n  ggtitle('Map of world with gapminder data')\n\n#p\n\n# use plotly for more interactivity - just because we can\nggplotly(p)"
  },
  {
    "objectID": "posts/maps/revealjs.html#maps-with-maps",
    "href": "posts/maps/revealjs.html#maps-with-maps",
    "title": "Geographic Maps with R",
    "section": "Maps with maps",
    "text": "Maps with maps\n\n\nCode\nmaps::map()         # map of workd\n\n\n\n\n\nCode\nmaps::map(\"state\")  # map of US states\n\n\n\n\n\nCode\n# pacific-centered map of the world\nmaps::map(wrap = c(0,360), fill = TRUE, col = \"lightblue\")"
  },
  {
    "objectID": "posts/maps/revealjs.html#map-manipulation-with-maps-and-sf",
    "href": "posts/maps/revealjs.html#map-manipulation-with-maps-and-sf",
    "title": "Geographic Maps with R",
    "section": "Map Manipulation with maps and sf",
    "text": "Map Manipulation with maps and sf\n\n\nCode\n# convert map object (list) to sf object (data frame)\ncounties &lt;- st_as_sf(map(\"county\", plot = FALSE, fill = TRUE))\n#states &lt;- st_as_sf(map(\"state\", plot = FALSE, fill = TRUE)) # unused here\n\n# subset US counties in Florida and check if polygon is valid (not for all!)\ncounties &lt;- counties %&gt;% \n  subset(grepl(\"florida\", counties$ID)) %&gt;% \n  filter(st_is_valid(.))\n\n# Calculate the county area\ncounties$area &lt;- as.numeric(st_area(counties))\n\n# ggplot with geom_sf() - a more modern version of geom_map\nggplot(data = world) +\n    # first show map and borders beyond Florida counties\n    geom_sf() + \n    # now overlay the filled counties of Florida\n    geom_sf(data = counties, aes(fill = area)) +\n    # chose colour scheme\n    scale_fill_viridis_c(trans = \"sqrt\", alpha = .4) +\n    # define coordinates\n    coord_sf(xlim = c(-88, -78), ylim = c(24.5, 33), expand = FALSE)"
  },
  {
    "objectID": "posts/maps/revealjs.html#and-there-is-more",
    "href": "posts/maps/revealjs.html#and-there-is-more",
    "title": "Geographic Maps with R",
    "section": "And there is more …",
    "text": "And there is more …\n\n\nThomas Manke"
  },
  {
    "objectID": "posts/quarto_demo/revealjs.html#load-packages",
    "href": "posts/quarto_demo/revealjs.html#load-packages",
    "title": "Quarto Intro",
    "section": "Load packages",
    "text": "Load packages\nEvery analysis comes wityh certain software requirements (and dependencies). Here we load the frequenly used tidyverse which is a collection of R package with many useful tools\n\n\nCode\nlibrary(tidyverse)\n\n\nIt’s best practice to record all packages\n\n\n\n\n\n\nsessionInfo\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.3 (2023-03-15)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/manke/miniconda3/envs/web/lib/libopenblasp-r0.3.21.dylib\n\nlocale:\n[1] C/UTF-8/C/C/C/C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0   dplyr_1.1.1    \n [5] purrr_1.0.1     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1   \n [9] ggplot2_3.4.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] pillar_1.9.0      compiler_4.2.3    tools_4.2.3       digest_0.6.31    \n [5] timechange_0.2.0  jsonlite_1.8.4    evaluate_0.20     lifecycle_1.0.3  \n [9] gtable_0.3.3      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] yaml_2.3.7        xfun_0.38         fastmap_1.1.1     withr_2.5.0      \n[17] knitr_1.42        generics_0.1.3    vctrs_0.6.1       htmlwidgets_1.6.2\n[21] hms_1.1.3         grid_4.2.3        tidyselect_1.2.0  glue_1.6.2       \n[25] R6_2.5.1          fansi_1.0.4       rmarkdown_2.14    tzdb_0.3.0       \n[29] magrittr_2.0.3    codetools_0.2-19  scales_1.2.1      htmltools_0.5.5  \n[33] colorspace_2.1-0  utf8_1.2.3        stringi_1.7.12    munsell_0.5.0"
  },
  {
    "objectID": "posts/quarto_demo/revealjs.html#load-data",
    "href": "posts/quarto_demo/revealjs.html#load-data",
    "title": "Quarto Intro",
    "section": "Load data",
    "text": "Load data\nLoading data can also be challenging. In the following we will use the pre-compiled iris data set for simplicity.\n\n\n\n\n\n\nNote\n\n\nR has many predefined data sets to illustrate data science concepts and software functionality. You can inspect more of those using data().\n\n\n\n\n\nCode\nd &lt;- iris\n\n\nAn aside: I assign iris to a new data frame d to keep the workflow generic. But this is optional and I may also have used iris instead of d below.\n\n\n\n\n\n\nUnfold this callout if you want to know more about this data set.\n\n\nYou may refer to the original paper in (Fischer, 1936) or by Anderson (1935). Those references are defined in the file iris.bib.\nBut if you are using R/Rstudio you can also use\n?iris\nclass(iris)\nstr(iris)\nNotice that this code chunk is not executed during rendering, but you can copy it conveniently from html to your clipboard.\n\n\n\n\n\n\n\n\n\nThe iris data can also be read from the following URL.\n\n\nThe following code block will not be evaluated because of the parameter eval: false\n\n\nCode\nurl = 'https://iris.csv'  # choose valid link!\n\nd &lt;- read_csv(url) %&gt;% \n  drop_na()  # some filtering maybe necessary\n\n\n\n\n\nMore information on additional callouts (warning, caution, important) is available here"
  },
  {
    "objectID": "posts/quarto_demo/revealjs.html#data-inspection",
    "href": "posts/quarto_demo/revealjs.html#data-inspection",
    "title": "Quarto Intro",
    "section": "Data Inspection",
    "text": "Data Inspection\n\n\nCode\nknitr::kable(head(d))\n\n\n\n\nTable 1: A formatted table with knitr::kable\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\nTable 1 is nicely formatted to show the first few observations of the iris data set. It has 150 rows and 5 columns1.\nNote that we may have filtered the data as part of the pre-processing steps."
  },
  {
    "objectID": "posts/quarto_demo/revealjs.html#columns",
    "href": "posts/quarto_demo/revealjs.html#columns",
    "title": "Quarto Intro",
    "section": "Columns",
    "text": "Columns\nSometimes we may want to define columns\n\n\nThe average petal length is 3.76 cm and the average petal width is 1.2 cm.\n\n\n\nThe data dimensions is 150, 5."
  },
  {
    "objectID": "posts/quarto_demo/revealjs.html#coloured-by-species",
    "href": "posts/quarto_demo/revealjs.html#coloured-by-species",
    "title": "Quarto Intro",
    "section": "Coloured by species",
    "text": "Coloured by species\n\n\nCode\np &lt;- d %&gt;%\n  ggplot(aes(x = Petal.Width, y = Petal.Length)) + \n  geom_point(aes(color=Species), size = 3, alpha = 0.8) +\n  geom_smooth(method=\"lm\", color=\"grey\") +\n  theme_minimal() +\n  labs(title = \"Iris Data\",\n       subtitle = \"Petal Length vs Petal Width\",\n       x = \"Petal Width (cm)\",\n       y = \"Petal Length (cm)\",\n       color = \"Species\",\n       shape = \"Species\") +\n  theme_minimal() \n#  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) \n\np\nggsave('quarto_demo.png', plot=p)\n\n\n\nFigure 3: Iris data (colored by Species)"
  },
  {
    "objectID": "posts/quarto_demo/revealjs.html#facet-wrap",
    "href": "posts/quarto_demo/revealjs.html#facet-wrap",
    "title": "Quarto Intro",
    "section": "Facet Wrap",
    "text": "Facet Wrap\n\n\nCode\np +   \n  geom_smooth(aes(color=Species), method=\"lm\") +\n  facet_wrap(~Species)\n\n\n\nFigure 4: Iris data with caption in marginFigure 3 colors species but runs regression over all samples. Figure 4 facets the data by species and shows also the species-specific regression lines. Notice that in this case the caption is put in the margin. And this comment too.\nHere is just another reference to a figure way back up: Figure 1 (But notice that it works only when the corresponding tabset panel is open).\n\n\nThomas Manke\n\n\n\nAnderson, E. (1935). The irises of the gaspe peninsula. In Bulletin of the American Iris Society (pp. 2–5).\n\n\nFischer, R. A. (1936). The use of multiple measurements in taxonomic problems. In Annals of Eugenics: Vol. II (pp. 179–188)."
  },
  {
    "objectID": "posts/quarto_demo/quarto_demo.html",
    "href": "posts/quarto_demo/quarto_demo.html",
    "title": "Quarto Intro",
    "section": "",
    "text": "The generic steps in data analysis is to\n\nLoad software\nLoad data\n\nfilter\ntransformations\n\nInspect data\nPlot data\n\nIn this summary I follow this paradigm with very simplistic examples - primarily to illustrate quatro functionality, rather than data science.\n\n\nEvery analysis comes wityh certain software requirements (and dependencies). Here we load the frequenly used tidyverse which is a collection of R package with many useful tools\n\n\nCode\nlibrary(tidyverse)\n\n\nIt’s best practice to record all packages\n\n\n\n\n\n\nsessionInfo\n\n\n\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.3 (2023-03-15)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/manke/miniconda3/envs/web/lib/libopenblasp-r0.3.21.dylib\n\nlocale:\n[1] C/UTF-8/C/C/C/C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0   dplyr_1.1.1    \n [5] purrr_1.0.1     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1   \n [9] ggplot2_3.4.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] pillar_1.9.0      compiler_4.2.3    tools_4.2.3       digest_0.6.31    \n [5] timechange_0.2.0  jsonlite_1.8.4    evaluate_0.20     lifecycle_1.0.3  \n [9] gtable_0.3.3      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] yaml_2.3.7        xfun_0.38         fastmap_1.1.1     withr_2.5.0      \n[17] knitr_1.42        generics_0.1.3    vctrs_0.6.1       htmlwidgets_1.6.2\n[21] hms_1.1.3         grid_4.2.3        tidyselect_1.2.0  glue_1.6.2       \n[25] R6_2.5.1          fansi_1.0.4       rmarkdown_2.14    tzdb_0.3.0       \n[29] magrittr_2.0.3    codetools_0.2-19  scales_1.2.1      htmltools_0.5.5  \n[33] colorspace_2.1-0  utf8_1.2.3        stringi_1.7.12    munsell_0.5.0    \n\n\n\n\n\n\n\n\nLoading data can also be challenging. In the following we will use the pre-compiled iris data set for simplicity.\n\n\n\n\n\n\nNote\n\n\n\nR has many predefined data sets to illustrate data science concepts and software functionality. You can inspect more of those using data().\n\n\n\n\nCode\nd &lt;- iris\n\n\nAn aside: I assign iris to a new data frame d to keep the workflow generic. But this is optional and I may also have used iris instead of d below.\n\n\n\n\n\n\nUnfold this callout if you want to know more about this data set.\n\n\n\n\n\nYou may refer to the original paper in (Fischer, 1936) or by Anderson (1935). Those references are defined in the file iris.bib.\nBut if you are using R/Rstudio you can also use\n?iris\nclass(iris)\nstr(iris)\nNotice that this code chunk is not executed during rendering, but you can copy it conveniently from html to your clipboard.\n\n\n\n\n\n\n\n\n\nThe iris data can also be read from the following URL.\n\n\n\n\n\nThe following code block will not be evaluated because of the parameter eval: false\n\n\nCode\nurl = 'https://iris.csv'  # choose valid link!\n\nd &lt;- read_csv(url) %&gt;% \n  drop_na()  # some filtering maybe necessary\n\n\n\n\n\nMore information on additional callouts (warning, caution, important) is available here\n\n\n\n\n\nCode\nknitr::kable(head(d))\n\n\n\n\nTable 1: A formatted table with knitr::kable\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\nTable 1 is nicely formatted to show the first few observations of the iris data set. It has 150 rows and 5 columns1."
  },
  {
    "objectID": "posts/quarto_demo/quarto_demo.html#load-packages",
    "href": "posts/quarto_demo/quarto_demo.html#load-packages",
    "title": "Quarto Intro",
    "section": "",
    "text": "Every analysis comes wityh certain software requirements (and dependencies). Here we load the frequenly used tidyverse which is a collection of R package with many useful tools\n\n\nCode\nlibrary(tidyverse)\n\n\nIt’s best practice to record all packages\n\n\n\n\n\n\nsessionInfo\n\n\n\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.3 (2023-03-15)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/manke/miniconda3/envs/web/lib/libopenblasp-r0.3.21.dylib\n\nlocale:\n[1] C/UTF-8/C/C/C/C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0   dplyr_1.1.1    \n [5] purrr_1.0.1     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1   \n [9] ggplot2_3.4.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] pillar_1.9.0      compiler_4.2.3    tools_4.2.3       digest_0.6.31    \n [5] timechange_0.2.0  jsonlite_1.8.4    evaluate_0.20     lifecycle_1.0.3  \n [9] gtable_0.3.3      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] yaml_2.3.7        xfun_0.38         fastmap_1.1.1     withr_2.5.0      \n[17] knitr_1.42        generics_0.1.3    vctrs_0.6.1       htmlwidgets_1.6.2\n[21] hms_1.1.3         grid_4.2.3        tidyselect_1.2.0  glue_1.6.2       \n[25] R6_2.5.1          fansi_1.0.4       rmarkdown_2.14    tzdb_0.3.0       \n[29] magrittr_2.0.3    codetools_0.2-19  scales_1.2.1      htmltools_0.5.5  \n[33] colorspace_2.1-0  utf8_1.2.3        stringi_1.7.12    munsell_0.5.0"
  },
  {
    "objectID": "posts/quarto_demo/quarto_demo.html#load-data",
    "href": "posts/quarto_demo/quarto_demo.html#load-data",
    "title": "Quarto Intro",
    "section": "",
    "text": "Loading data can also be challenging. In the following we will use the pre-compiled iris data set for simplicity.\n\n\n\n\n\n\nNote\n\n\n\nR has many predefined data sets to illustrate data science concepts and software functionality. You can inspect more of those using data().\n\n\n\n\nCode\nd &lt;- iris\n\n\nAn aside: I assign iris to a new data frame d to keep the workflow generic. But this is optional and I may also have used iris instead of d below.\n\n\n\n\n\n\nUnfold this callout if you want to know more about this data set.\n\n\n\n\n\nYou may refer to the original paper in (Fischer, 1936) or by Anderson (1935). Those references are defined in the file iris.bib.\nBut if you are using R/Rstudio you can also use\n?iris\nclass(iris)\nstr(iris)\nNotice that this code chunk is not executed during rendering, but you can copy it conveniently from html to your clipboard.\n\n\n\n\n\n\n\n\n\nThe iris data can also be read from the following URL.\n\n\n\n\n\nThe following code block will not be evaluated because of the parameter eval: false\n\n\nCode\nurl = 'https://iris.csv'  # choose valid link!\n\nd &lt;- read_csv(url) %&gt;% \n  drop_na()  # some filtering maybe necessary\n\n\n\n\n\nMore information on additional callouts (warning, caution, important) is available here"
  },
  {
    "objectID": "posts/quarto_demo/quarto_demo.html#data-inspection",
    "href": "posts/quarto_demo/quarto_demo.html#data-inspection",
    "title": "Quarto Intro",
    "section": "",
    "text": "Code\nknitr::kable(head(d))\n\n\n\n\nTable 1: A formatted table with knitr::kable\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\nTable 1 is nicely formatted to show the first few observations of the iris data set. It has 150 rows and 5 columns1."
  },
  {
    "objectID": "posts/quarto_demo/quarto_demo.html#columns",
    "href": "posts/quarto_demo/quarto_demo.html#columns",
    "title": "Quarto Intro",
    "section": "Columns",
    "text": "Columns\nSometimes we may want to define columns\n\n\nThe average petal length is 3.76 cm and the average petal width is 1.2 cm.\n\n\n\nThe data dimensions is 150, 5."
  },
  {
    "objectID": "posts/quarto_demo/quarto_demo.html#coloured-by-species",
    "href": "posts/quarto_demo/quarto_demo.html#coloured-by-species",
    "title": "Quarto Intro",
    "section": "Coloured by species",
    "text": "Coloured by species\n\n\nCode\np &lt;- d %&gt;%\n  ggplot(aes(x = Petal.Width, y = Petal.Length)) + \n  geom_point(aes(color=Species), size = 3, alpha = 0.8) +\n  geom_smooth(method=\"lm\", color=\"grey\") +\n  theme_minimal() +\n  labs(title = \"Iris Data\",\n       subtitle = \"Petal Length vs Petal Width\",\n       x = \"Petal Width (cm)\",\n       y = \"Petal Length (cm)\",\n       color = \"Species\",\n       shape = \"Species\") +\n  theme_minimal() \n#  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) \n\np\nggsave('quarto_demo.png', plot=p)\n\n\n\n\n\nFigure 3: Iris data (colored by Species)"
  },
  {
    "objectID": "posts/quarto_demo/quarto_demo.html#facet-wrap",
    "href": "posts/quarto_demo/quarto_demo.html#facet-wrap",
    "title": "Quarto Intro",
    "section": "Facet Wrap",
    "text": "Facet Wrap\n\n\nCode\np +   \n  geom_smooth(aes(color=Species), method=\"lm\") +\n  facet_wrap(~Species)\n\n\n\n\n\nFigure 4: Iris data with caption in margin\n\n\n\n\nFigure 3 colors species but runs regression over all samples. Figure 4 facets the data by species and shows also the species-specific regression lines. Notice that in this case the caption is put in the margin. And this comment too.\nHere is just another reference to a figure way back up: Figure 1 (But notice that it works only when the corresponding tabset panel is open)."
  },
  {
    "objectID": "posts/quarto_demo/quarto_demo.html#footnotes",
    "href": "posts/quarto_demo/quarto_demo.html#footnotes",
    "title": "Quarto Intro",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that we may have filtered the data as part of the pre-processing steps.↩︎"
  },
  {
    "objectID": "posts/BuffonsNeedle/revealjs.html#idea",
    "href": "posts/BuffonsNeedle/revealjs.html#idea",
    "title": "Buffon’s Needle",
    "section": "Idea",
    "text": "Idea"
  },
  {
    "objectID": "posts/BuffonsNeedle/revealjs.html#simulation",
    "href": "posts/BuffonsNeedle/revealjs.html#simulation",
    "title": "Buffon’s Needle",
    "section": "Simulation",
    "text": "Simulation\nHere I simulate Buffon’s Needle Problem (Comte de Buffon: 1707 - 1788) using shinylive and its quarto extension. This package allows to generate html that can run shiny code on the client side (and without the need for a dedicated shiny server).\n#| standalone: true\n#| components: [viewer, editor]\n#| code-folding: true\n#| layout: vertical\n#| viewerHeight: 420\n\n## file: app.py\nfrom shiny import App, reactive, render, ui\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom my_plots import plotHist, plotNeedles\nfrom my_calcs import simulateNeedles\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_numeric(\"N\", \"Needles\", 10, min = 2, max = 1e7),\n            ui.input_numeric(\"NR\",\"Replicates=\", 1, min = 1, max = 1000),\n            ui.input_action_button(\"update\", \"Update\", class_=\"btn-success\"),\n        ),\n        ui.panel_main(\n            ui.output_plot(\"Buffon\"),\n        ),\n    ),\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"Buffon\")\n    @reactive.event(input.update, ignore_none=False)\n    def Buffon():\n        N=input.N()         # number of needles\n        NR=input.NR()       # number of replicates\n     \n        x0,x1,y0,y1 = simulateNeedles(N*NR)   # simulate  N needles thrown NR times\n        cross = np.floor(x0) != np.floor(x1)  # cross=TRUE if line crossing\n        \n        # interpret N*NR needles as N needles thrown NR times (simulate)\n        cross_r = np.reshape(cross,(N, NR))    # reshape: rows=needles columns=replicates\n        Nc = np.sum(cross_r, axis=0)           # number of crossings (per replicate) (R: colSum(cross))\n        ratio = N / Nc                         # ratios for all replicate\n\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plotNeedles(x0,x1,y0,y1,cross, N)\n        \n        plt.subplot(1, 2, 2)\n        plotHist(ratio)\n\n        plt.tight_layout()\n        plt.show()\n\napp = App(app_ui, server)\n\n## file: my_calcs.py\nimport numpy as np\n\ndef simulateNeedles(n):\n  # simulate n random start coordinates (x0,y0) and N random angles\n  # notice that for the problem y0,y1 are irrelevant, but distribution across y helps visualization\n  x0 = np.random.uniform(low=0.0, high=3.0, size=n)\n  y0 = np.random.uniform(low=1.0, high=3.0, size=n)\n  alpha = np.random.uniform(low=0.0, high=np.pi, size=n)    \n\n  # calculate end coordinates (x1,y1)\n  x1=x0 + 0.5*np.cos(alpha)\n  y1=y0 + 0.5*np.sin(alpha)\n\n  return x0,x1,y0,y1\n  \n## file: my_plots.py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plotHist(ratio):\n  # Plot the histogram over all replicate trials, ratio is a vector of length NR\n\n  ratio_finite = ratio[np.isfinite(ratio)]\n  plt.hist(ratio_finite, color='green', edgecolor='black')\n  plt.axvline(np.pi, color='red', linewidth=2)\n  plt.xlabel(\"N/Nc\")\n  plt.ylabel(\"Frequency\")\n  plt.title(\"Histogram of N/Nc\")\n\n  \ndef plotNeedles(x0,x1,y0,y1,cross,N):\n  Nc = np.sum(cross[:N])          # _calculate_ number of crossing for N needles (first replicate) \n  title = \"N={} Nc={} --&gt; N/Nc={}\".format(N, Nc, round(N/Nc, 4))\n  plt.plot()\n  plt.xlim(-1, 4)\n  plt.ylim(0, 4)\n  plt.xlabel(\"x\")\n  plt.ylabel(\"y\")\n  plt.vlines(np.arange(0, 11), ymin=0, ymax=4, colors='gray')\n  plt.title(title)\n  \n  Nmax = min(N, 1000)                                      # _plot_ at most 1000 needles\n  colors = ['red' if c else 'blue' for c in cross[:Nmax]]  # plot crossing lines 'red'\n  for i in range(Nmax):\n    plt.plot([x0[i], x1[i]], [y0[i], y1[i]], color=colors[i])"
  },
  {
    "objectID": "posts/BuffonsNeedle/revealjs.html#references",
    "href": "posts/BuffonsNeedle/revealjs.html#references",
    "title": "Buffon’s Needle",
    "section": "References",
    "text": "References\n\nWikipedia\nshinylive github\nshinylive.io\n\n\n\n\n\n\n\nwatermark\n\n\n\n\nCode\nimport watermark\nimport pkg_resources\n\n#import os\n#print(\"Local variables:\", locals())\n#print(\"Global variables:\", globals())\n\n# Display watermark information\nprint(watermark.watermark())\n\n# Get installed module versions\ninstalled_packages = pkg_resources.working_set\n\n# Print module versions\nprint(\"Installed Packages:\")\nfor package in installed_packages:\n    print(package.key, package.version)\n\n\nLast updated: 2023-07-01T19:09:02.155829+02:00\n\nPython implementation: CPython\nPython version       : 3.11.4\nIPython version      : 8.12.0\n\nCompiler    : Clang 15.0.7 \nOS          : Darwin\nRelease     : 21.6.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nInstalled Packages:\njinja2 3.1.2\nmarkupsafe 2.1.2\npillow 9.4.0\npyqt5 5.15.7\npyqt5-sip 12.11.0\npysocks 1.7.1\npyyaml 6.0\npygments 2.14.0\nqtpy 2.3.1\nsqlalchemy 2.0.16\nsend2trash 1.8.0\naltair 5.0.1\nanyio 3.6.2\nappdirs 1.4.4\nappnope 0.1.3\nargon2-cffi 21.3.0\nargon2-cffi-bindings 21.2.0\nasgiref 3.7.2\nasttokens 2.2.1\nattrs 22.2.0\nbackcall 0.2.0\nbackports.functools-lru-cache 1.6.4\nbeautifulsoup4 4.12.0\nbleach 6.0.0\nbrotlipy 0.7.0\ncertifi 2023.5.7\ncffi 1.15.1\ncharset-normalizer 3.1.0\nclick 8.1.3\ncomm 0.1.3\ncontextvars 2.4\ncontourpy 1.0.7\ncryptography 40.0.1\ncycler 0.11.0\ndebugpy 1.6.6\ndecorator 5.1.1\ndefusedxml 0.7.1\nentrypoints 0.4\nexecuting 1.2.0\nfastjsonschema 2.16.3\nflit-core 3.8.0\nfonttools 4.39.3\ngreenlet 2.0.2\nh11 0.14.0\nhtmltools 0.2.1\nidna 3.4\nimmutables 0.19\nimportlib-metadata 6.1.0\nimportlib-resources 5.12.0\nipykernel 6.22.0\nipython 8.12.0\nipython-genutils 0.2.0\nipywidgets 8.0.6\njedi 0.18.2\njsonschema 4.17.3\njupyter 1.0.0\njupyter-cache 0.6.1\njupyter-client 8.1.0\njupyter-console 6.6.3\njupyter-core 5.3.0\njupyter-events 0.6.3\njupyter-server 2.5.0\njupyter-server-terminals 0.4.4\njupyterlab-pygments 0.2.2\njupyterlab-widgets 3.0.7\nkaleido 0.2.1\nkiwisolver 1.4.4\nlinkify-it-py 2.0.2\nmarkdown-it-py 3.0.0\nmatplotlib 3.7.1\nmatplotlib-inline 0.1.6\nmdit-py-plugins 0.4.0\nmdurl 0.1.2\nmistune 2.0.5\nmunkres 1.1.4\nnbclassic 0.5.3\nnbclient 0.7.4\nnbconvert 7.2.9\nnbformat 5.9.0\nnest-asyncio 1.5.6\nnotebook 6.5.3\nnotebook-shim 0.2.2\nnumpy 1.23.5\npackaging 23.0\npandas 2.0.3\npandocfilters 1.5.0\nparso 0.8.3\npatsy 0.5.3\npexpect 4.8.0\npickleshare 0.7.5\npip 23.1.2\npkgutil-resolve-name 1.3.10\nplatformdirs 3.2.0\nplotly 5.15.0\nply 3.11\npooch 1.7.0\nprometheus-client 0.16.0\nprompt-toolkit 3.0.38\npsutil 5.9.4\nptyprocess 0.7.0\npure-eval 0.2.2\npyopenssl 23.1.1\npycparser 2.21\npyparsing 3.0.9\npyrsistent 0.19.3\npython-dateutil 2.8.2\npython-json-logger 2.0.7\npython-multipart 0.0.6\npytz 2023.3\npyzmq 25.0.2\nqtconsole 5.4.2\nrequests 2.28.2\nrfc3339-validator 0.1.4\nrfc3986-validator 0.1.1\nrpy2 3.5.11\nscipy 1.10.1\nseaborn 0.12.2\nsetuptools 67.6.1\nshiny 0.4.0\nshinylive 0.0.14\nsimplegeneric 0.8.1\nsip 6.7.7\nsix 1.16.0\nsniffio 1.3.0\nsoupsieve 2.3.2.post1\nstack-data 0.6.2\nstarlette 0.28.0\nstatsmodels 0.14.0\ntabulate 0.9.0\ntenacity 8.2.2\nterminado 0.17.1\ntinycss2 1.2.1\ntoml 0.10.2\ntoolz 0.12.0\ntornado 6.2\ntraitlets 5.9.0\ntyping-extensions 4.5.0\ntzdata 2023.3\ntzlocal 5.0.1\nuc-micro-py 1.0.2\nurllib3 1.26.15\nuvicorn 0.22.0\nwatchfiles 0.19.0\nwatermark 2.3.1\nwcwidth 0.2.6\nwebencodings 0.5.1\nwebsocket-client 1.5.1\nwebsockets 11.0.3\nwheel 0.40.0\nwidgetsnbextension 4.0.7\nzipp 3.15.0\n\n\n\n\n\n\n\nThomas Manke"
  },
  {
    "objectID": "posts/BuffonsNeedle/BuffonsNeedle.html#simulation",
    "href": "posts/BuffonsNeedle/BuffonsNeedle.html#simulation",
    "title": "Buffon’s Needle",
    "section": "Simulation",
    "text": "Simulation\nHere I simulate Buffon’s Needle Problem (Comte de Buffon: 1707 - 1788) using shinylive and its quarto extension. This package allows to generate html that can run shiny code on the client side (and without the need for a dedicated shiny server).\n#| standalone: true\n#| components: [viewer, editor]\n#| code-folding: true\n#| layout: vertical\n#| viewerHeight: 420\n\n## file: app.py\nfrom shiny import App, reactive, render, ui\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom my_plots import plotHist, plotNeedles\nfrom my_calcs import simulateNeedles\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_numeric(\"N\", \"Needles\", 10, min = 2, max = 1e7),\n            ui.input_numeric(\"NR\",\"Replicates=\", 1, min = 1, max = 1000),\n            ui.input_action_button(\"update\", \"Update\", class_=\"btn-success\"),\n        ),\n        ui.panel_main(\n            ui.output_plot(\"Buffon\"),\n        ),\n    ),\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"Buffon\")\n    @reactive.event(input.update, ignore_none=False)\n    def Buffon():\n        N=input.N()         # number of needles\n        NR=input.NR()       # number of replicates\n     \n        x0,x1,y0,y1 = simulateNeedles(N*NR)   # simulate  N needles thrown NR times\n        cross = np.floor(x0) != np.floor(x1)  # cross=TRUE if line crossing\n        \n        # interpret N*NR needles as N needles thrown NR times (simulate)\n        cross_r = np.reshape(cross,(N, NR))    # reshape: rows=needles columns=replicates\n        Nc = np.sum(cross_r, axis=0)           # number of crossings (per replicate) (R: colSum(cross))\n        ratio = N / Nc                         # ratios for all replicate\n\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plotNeedles(x0,x1,y0,y1,cross, N)\n        \n        plt.subplot(1, 2, 2)\n        plotHist(ratio)\n\n        plt.tight_layout()\n        plt.show()\n\napp = App(app_ui, server)\n\n## file: my_calcs.py\nimport numpy as np\n\ndef simulateNeedles(n):\n  # simulate n random start coordinates (x0,y0) and N random angles\n  # notice that for the problem y0,y1 are irrelevant, but distribution across y helps visualization\n  x0 = np.random.uniform(low=0.0, high=3.0, size=n)\n  y0 = np.random.uniform(low=1.0, high=3.0, size=n)\n  alpha = np.random.uniform(low=0.0, high=np.pi, size=n)    \n\n  # calculate end coordinates (x1,y1)\n  x1=x0 + 0.5*np.cos(alpha)\n  y1=y0 + 0.5*np.sin(alpha)\n\n  return x0,x1,y0,y1\n  \n## file: my_plots.py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plotHist(ratio):\n  # Plot the histogram over all replicate trials, ratio is a vector of length NR\n\n  ratio_finite = ratio[np.isfinite(ratio)]\n  plt.hist(ratio_finite, color='green', edgecolor='black')\n  plt.axvline(np.pi, color='red', linewidth=2)\n  plt.xlabel(\"N/Nc\")\n  plt.ylabel(\"Frequency\")\n  plt.title(\"Histogram of N/Nc\")\n\n  \ndef plotNeedles(x0,x1,y0,y1,cross,N):\n  Nc = np.sum(cross[:N])          # _calculate_ number of crossing for N needles (first replicate) \n  title = \"N={} Nc={} --&gt; N/Nc={}\".format(N, Nc, round(N/Nc, 4))\n  plt.plot()\n  plt.xlim(-1, 4)\n  plt.ylim(0, 4)\n  plt.xlabel(\"x\")\n  plt.ylabel(\"y\")\n  plt.vlines(np.arange(0, 11), ymin=0, ymax=4, colors='gray')\n  plt.title(title)\n  \n  Nmax = min(N, 1000)                                      # _plot_ at most 1000 needles\n  colors = ['red' if c else 'blue' for c in cross[:Nmax]]  # plot crossing lines 'red'\n  for i in range(Nmax):\n    plt.plot([x0[i], x1[i]], [y0[i], y1[i]], color=colors[i])"
  },
  {
    "objectID": "posts/BuffonsNeedle/BuffonsNeedle.html#references",
    "href": "posts/BuffonsNeedle/BuffonsNeedle.html#references",
    "title": "Buffon’s Needle",
    "section": "References",
    "text": "References\n\nWikipedia\nshinylive github\nshinylive.io\n\n\n\n\n\n\n\nwatermark\n\n\n\n\n\n\n\nCode\nimport watermark\nimport pkg_resources\n\n#import os\n#print(\"Local variables:\", locals())\n#print(\"Global variables:\", globals())\n\n# Display watermark information\nprint(watermark.watermark())\n\n# Get installed module versions\ninstalled_packages = pkg_resources.working_set\n\n# Print module versions\nprint(\"Installed Packages:\")\nfor package in installed_packages:\n    print(package.key, package.version)\n\n\nLast updated: 2023-07-01T19:09:02.155829+02:00\n\nPython implementation: CPython\nPython version       : 3.11.4\nIPython version      : 8.12.0\n\nCompiler    : Clang 15.0.7 \nOS          : Darwin\nRelease     : 21.6.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nInstalled Packages:\njinja2 3.1.2\nmarkupsafe 2.1.2\npillow 9.4.0\npyqt5 5.15.7\npyqt5-sip 12.11.0\npysocks 1.7.1\npyyaml 6.0\npygments 2.14.0\nqtpy 2.3.1\nsqlalchemy 2.0.16\nsend2trash 1.8.0\naltair 5.0.1\nanyio 3.6.2\nappdirs 1.4.4\nappnope 0.1.3\nargon2-cffi 21.3.0\nargon2-cffi-bindings 21.2.0\nasgiref 3.7.2\nasttokens 2.2.1\nattrs 22.2.0\nbackcall 0.2.0\nbackports.functools-lru-cache 1.6.4\nbeautifulsoup4 4.12.0\nbleach 6.0.0\nbrotlipy 0.7.0\ncertifi 2023.5.7\ncffi 1.15.1\ncharset-normalizer 3.1.0\nclick 8.1.3\ncomm 0.1.3\ncontextvars 2.4\ncontourpy 1.0.7\ncryptography 40.0.1\ncycler 0.11.0\ndebugpy 1.6.6\ndecorator 5.1.1\ndefusedxml 0.7.1\nentrypoints 0.4\nexecuting 1.2.0\nfastjsonschema 2.16.3\nflit-core 3.8.0\nfonttools 4.39.3\ngreenlet 2.0.2\nh11 0.14.0\nhtmltools 0.2.1\nidna 3.4\nimmutables 0.19\nimportlib-metadata 6.1.0\nimportlib-resources 5.12.0\nipykernel 6.22.0\nipython 8.12.0\nipython-genutils 0.2.0\nipywidgets 8.0.6\njedi 0.18.2\njsonschema 4.17.3\njupyter 1.0.0\njupyter-cache 0.6.1\njupyter-client 8.1.0\njupyter-console 6.6.3\njupyter-core 5.3.0\njupyter-events 0.6.3\njupyter-server 2.5.0\njupyter-server-terminals 0.4.4\njupyterlab-pygments 0.2.2\njupyterlab-widgets 3.0.7\nkaleido 0.2.1\nkiwisolver 1.4.4\nlinkify-it-py 2.0.2\nmarkdown-it-py 3.0.0\nmatplotlib 3.7.1\nmatplotlib-inline 0.1.6\nmdit-py-plugins 0.4.0\nmdurl 0.1.2\nmistune 2.0.5\nmunkres 1.1.4\nnbclassic 0.5.3\nnbclient 0.7.4\nnbconvert 7.2.9\nnbformat 5.9.0\nnest-asyncio 1.5.6\nnotebook 6.5.3\nnotebook-shim 0.2.2\nnumpy 1.23.5\npackaging 23.0\npandas 2.0.3\npandocfilters 1.5.0\nparso 0.8.3\npatsy 0.5.3\npexpect 4.8.0\npickleshare 0.7.5\npip 23.1.2\npkgutil-resolve-name 1.3.10\nplatformdirs 3.2.0\nplotly 5.15.0\nply 3.11\npooch 1.7.0\nprometheus-client 0.16.0\nprompt-toolkit 3.0.38\npsutil 5.9.4\nptyprocess 0.7.0\npure-eval 0.2.2\npyopenssl 23.1.1\npycparser 2.21\npyparsing 3.0.9\npyrsistent 0.19.3\npython-dateutil 2.8.2\npython-json-logger 2.0.7\npython-multipart 0.0.6\npytz 2023.3\npyzmq 25.0.2\nqtconsole 5.4.2\nrequests 2.28.2\nrfc3339-validator 0.1.4\nrfc3986-validator 0.1.1\nrpy2 3.5.11\nscipy 1.10.1\nseaborn 0.12.2\nsetuptools 67.6.1\nshiny 0.4.0\nshinylive 0.0.14\nsimplegeneric 0.8.1\nsip 6.7.7\nsix 1.16.0\nsniffio 1.3.0\nsoupsieve 2.3.2.post1\nstack-data 0.6.2\nstarlette 0.28.0\nstatsmodels 0.14.0\ntabulate 0.9.0\ntenacity 8.2.2\nterminado 0.17.1\ntinycss2 1.2.1\ntoml 0.10.2\ntoolz 0.12.0\ntornado 6.2\ntraitlets 5.9.0\ntyping-extensions 4.5.0\ntzdata 2023.3\ntzlocal 5.0.1\nuc-micro-py 1.0.2\nurllib3 1.26.15\nuvicorn 0.22.0\nwatchfiles 0.19.0\nwatermark 2.3.1\nwcwidth 0.2.6\nwebencodings 0.5.1\nwebsocket-client 1.5.1\nwebsockets 11.0.3\nwheel 0.40.0\nwidgetsnbextension 4.0.7\nzipp 3.15.0"
  },
  {
    "objectID": "posts/trees_R/revealjs.html#setup",
    "href": "posts/trees_R/revealjs.html#setup",
    "title": "Trees with R",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(gapminder)"
  },
  {
    "objectID": "posts/trees_R/revealjs.html#simple-sunburst-example",
    "href": "posts/trees_R/revealjs.html#simple-sunburst-example",
    "title": "Trees with R",
    "section": "Simple Sunburst Example",
    "text": "Simple Sunburst Example\nNotice that the simplicity of this example hides a few important issues\n\nlabels: are arbitrary (and can be redundant)\nids: should be unique\n\nif not used explicitly they will be implicitly set from labels\nmanual says they could not be numbers\n\nparents: must refer to the unique IDs\n\nRef: https://plotly.com/r/reference/sunburst/#sunburst\n\n\nCode\nfig &lt;- plot_ly(\n  labels = c(\"Eve\", \"Cain\", \"Seth\", \"Enos\", \"Noam\", \"Abel\", \"Awan\", \"Enoch\", \"Azura\"),   # = ids !!!\n  parents = c(\"\", \"Eve\", \"Eve\", \"Seth\", \"Seth\", \"Eve\", \"Eve\", \"Awan\", \"Eve\"),\n  values = c(10, 14, 12, 10, 2, 6, 6, 4, 4),\n  type = 'sunburst'\n)\n\nfig\n\n\n\n\n\nFigure 1: A simple sunburst exaample from plotly\n\n\n\nNotice: If all parents are set to root=\"\" then this reduces to a simple pie chart\n\n\nCode\nfig &lt;- plot_ly(\n  labels = c(\"Eve\", \"Cain\", \"Seth\", \"Enos\", \"Noam\", \"Abel\", \"Awan\", \"Enoch\", \"Azura\"),   # = ids !!!\n  parents = c(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"),\n  values = c(10, 14, 12, 10, 2, 6, 6, 4, 4),\n  type = 'sunburst'\n)\n\nfig\n\n\n\n\n\nFigure 2: A pie chart is a special case of a sunburst"
  },
  {
    "objectID": "posts/trees_R/revealjs.html#gapminder",
    "href": "posts/trees_R/revealjs.html#gapminder",
    "title": "Trees with R",
    "section": "Gapminder",
    "text": "Gapminder\nFilter and Subset Data\nAs usual we need some pre-processing and filtering of the data before we get started. In this case we only want to filter for year and reduce it to the relevant columns.\n\n\nCode\n# preprocess gapminder data\ngpm = gapminder %&gt;% \n  filter(year==2007) %&gt;%                 # filter specific year (the last)\n#  mutate_if(is.factor, as.character) %&gt;% # convert factors to character for uniformity (optional)\n  select(continent, country, pop)        # select only specific columns\n\n\nDefine hierarchy\nGoal: create a new data frame that contains node with labels, parent labels and values - with one line per node.\nPossible parent=“” to denote root node(s). In the gapminder example the continents are parents (and roots) of countries.\n\n\nCode\nroot=\"world\"\n# define nodes\ndf1 = gpm %&gt;% \n  group_by(continent) %&gt;%          # organize gapminder by continent\n  summarise(val=sum(pop)) %&gt;%      # continents value = sum(pop)  = sum over countries\n  mutate(parent=root) %&gt;%            # continents are root (no label)\n  select(label=continent, parent, val)\n\n\n# for each country (=leaf)-level define val(=pop), parent (continent) and label (country)\ndf2 = gpm %&gt;% \n  select(label=country, parent=continent, val=pop) \n\n# merge nodes (continents and countries)\ndf = df1 %&gt;% bind_rows(df2)\ndf %&gt;% head(10)\n\n\n# A tibble: 10 × 3\n   label       parent          val\n   &lt;fct&gt;       &lt;chr&gt;         &lt;dbl&gt;\n 1 Africa      world     929539692\n 2 Americas    world     898871184\n 3 Asia        world    3811953827\n 4 Europe      world     586098529\n 5 Oceania     world      24549947\n 6 Afghanistan Asia       31889923\n 7 Albania     Europe      3600523\n 8 Algeria     Africa     33333216\n 9 Angola      Africa     12420476\n10 Argentina   Americas   40301927\n\n\nPlotting\nThere are two popular plots to visualize hierarchical relationships with quantitative values: sunburst and treemaps.\nGiven a properly prepared dataframe they can be easily plotted with plot_ly.\nNotice: In the code below branchvalues = 'total' ensures that the relative arc length (sunburst) and areas (treemaps) correspond to the total values implied by the hierarchy. (Otherwise the arc length of the continents would be twice the length over all countries)\nSunbursts\n\n\nCode\nfig = plot_ly(df, type = 'sunburst', \n              branchvalues = 'total',\n              labels  = ~label,\n              parents = ~parent,\n              values  = ~val\n)\nfig\n\n\n\n\n\nFigure 3: Gapminder sunburst.\n\n\n\nTreemap\nIn fact, treemaps are preferred to visualize data of different proportions. For the same reasons that pie charts are discouraged.\n\n\nCode\nfig = plot_ly(df, type = 'treemap', \n        branchvalues = \"total\",\n        labels  = ~label,\n        parents = ~parent,\n        values  = ~val\n)\n\nfig\n\n\n\n\n\nFigure 4: Treemap of World Population.\n\n\n\nFurther Developments\nHere are my efforts to color-code the population size and also scale the countries separately for their respective continent.\nAnd I also tested how to combine figures - an additional challenge for plotly sunbursts.\nBeware: the setting for branchvalues has to be handled carefully.\n\n\nCode\n# simple scale function\nscale01 &lt;- function(x){(x-min(x))/(max(x)-min(x))}\n\n# calculate scaled population for each parent (continent)\ndf &lt;- df %&gt;% group_by(parent) %&gt;% mutate(val_s = scale01(val))\n\n# illustrate the parent-specific scaling for Europe\ndf %&gt;% filter(parent==\"Europe\") \n\n\n# A tibble: 30 × 4\n# Groups:   parent [1]\n   label                  parent      val  val_s\n   &lt;fct&gt;                  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 Albania                Europe  3600523 0.0402\n 2 Austria                Europe  8199783 0.0962\n 3 Belgium                Europe 10392226 0.123 \n 4 Bosnia and Herzegovina Europe  4552198 0.0518\n 5 Bulgaria               Europe  7322858 0.0855\n 6 Croatia                Europe  4493312 0.0511\n 7 Czech Republic         Europe 10228744 0.121 \n 8 Denmark                Europe  5468120 0.0629\n 9 Finland                Europe  5238460 0.0601\n10 France                 Europe 61083916 0.740 \n# ℹ 20 more rows\n\n\nCode\np1 &lt;- plot_ly() %&gt;% \n  add_trace(data = df,\n            type = \"sunburst\",    \n            branchvalues = 'total',\n            labels  = ~label,\n            parents = ~parent,\n            values  = ~val,\n            marker = list( color = ~val, \n                           colorscale='Blues', \n                           colorbar = list(title = \"Population\"),\n                           reversescale=TRUE,\n                           showscale=TRUE\n                           )\n            )\n\n\np2 &lt;- plot_ly() %&gt;% \n  add_trace(data = df,\n            type = \"sunburst\",    \n            # for the scaled values branchvalue='total' will cause problems\n            #branchvalues = 'total',\n            labels  = ~label,\n            parents = ~parent,\n            values  = ~val_s,\n            marker = list( color = ~val_s, \n                           colorscale='Blues', \n                           colorbar = list(title = \"Scaled Population\"),\n                           reversescale=TRUE,\n                           showscale=TRUE\n                           )\n            )\n\n#plotly::subplot(p1,p2)  #does not work: https://github.com/plotly/plotly.R/issues/1867\n\ncrosstalk::bscols(p1, p2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Two Sunburst of world population (2007). a) unscaled and b) scaled"
  },
  {
    "objectID": "posts/trees_R/revealjs.html#directory-content",
    "href": "posts/trees_R/revealjs.html#directory-content",
    "title": "Trees with R",
    "section": "Directory Content",
    "text": "Directory Content\nHere I want to visualize the file structure and size of a given directory.\nLoad Data\nGet all files and directories (under a given root folder) and determine their sizes using ‘du -a’\n\n\nCode\nroot=\"..\"\n#cmd=paste('find', root,' -type f -exec du -a {} +')\ncmd=paste('du -a', root)\ndf = read_tsv(pipe(cmd), col_names = c(\"size\", \"file_name\"))\n\n\nData Munging\nThe directory structure is a tree. The key steps below are to define appropriate node-IDs, node-labels and parents for each node.\nA sidetrack …\nIn retrospective the above goals sounds simple, but much work went into this lengthy sidetrack. which was caued by a misunderstanding on how node and parent labels work. The code cell below introduced some convenience function to extract only the partial directory name as parent, but this is overly complicated since parent should refer to a unique ID. I keep this code fragment only for reference because it took some time to develop\n\n\nCode\nget_2ndlast &lt;- function(L){\n  # get 2nd last element from a list\n  if (length(L) &gt;= 2) {\n    L[length(L) - 1]\n  } else {\n    \"root\"\n  }\n}\n\nget_parent &lt;- function(filename){\n  # get the direct parent directory of a file (before the last \"/\", possibly empty)\n  # split filename by directories %&gt;% convert to simple vector %&gt;% extract 2nd last element\n  \n  #this is not vectorize and doesn't work properly if filename is a vector, e.g df$file_name\n  strsplit(filename, \"/\") %&gt;% unlist %&gt;% .[length(.)-1]     \n}\n\n# to vectorize a function there are different strategies \n# this solution does not work: https://deanattali.com/blog/mutate-non-vectorized/\nget_parent_v &lt;- Vectorize(get_parent)\n\n# this version worked, but I stopped using this anyhow\nget_parent_v &lt;- function(filename){\n  strsplit(filename, \"/\") %&gt;% sapply(get_2ndlast) %&gt;% unlist\n}\n\n\nBelow we add a label and a parent-ID to the data frame. Notice the following choices in the code block below\n\nfile_name will serve as a unique ID of each file\nbasenanme(file_name) will serve as a convenient label for files (and directories)\ndirname(file_name) will serve as a unique ID for parent directories\n\n… and a simple solution\n\n\nCode\ndf = df %&gt;% \n  mutate(label=basename(file_name), parent=dirname(file_name)) \n  # this one does not help because the reduced directory name is not a unique ID\n  #mutate(node=basename(file_name), parent=get_parent_v(file_name)) \n\n#df %&gt;% summary()\ndf %&gt;% head()\n\n\n# A tibble: 6 × 4\n   size file_name                                                   label parent\n  &lt;dbl&gt; &lt;chr&gt;                                                       &lt;chr&gt; &lt;chr&gt; \n1     8 ../maps/maps_cache/html/overlap_data2_33cab96026ea11c4f90e… over… ../ma…\n2     8 ../maps/maps_cache/html/overlap_data2_33cab96026ea11c4f90e… over… ../ma…\n3     8 ../maps/maps_cache/html/ggplot_map_c910447fed13122597decb8… ggpl… ../ma…\n4    24 ../maps/maps_cache/html/overlap_data2_33cab96026ea11c4f90e… over… ../ma…\n5     8 ../maps/maps_cache/html/setup_04fa97c80f81582a529607a798d5… setu… ../ma…\n6     0 ../maps/maps_cache/html/maps_with_maps_f3b8224ebcb46befcca… maps… ../ma…\n\n\nSunburst\n\n\nCode\nfig = df %&gt;% \n  plot_ly(\n    type = 'sunburst', \n    branchvalues = \"total\",  # choice on how to represent \n    ids     = ~file_name,    # unique ID\n    labels  = ~label,        # arbitrary label\n    parents = ~parent,       # unique ID\n    values  = ~size\n) %&gt;%\n  layout(\n    title = paste(\"File System Sunburst for direcory \", root)\n  )\n\nfig\n\n\n\n\n\n\nFigure 6: A Sunburst directory\n\n\n\nTreemap\n\n\nCode\nfig = df %&gt;% \n  plot_ly(\n    type = 'treemap', \n    branchvalues = \"total\",\n    ids     = ~file_name,    # unique ID\n    labels  = ~label,        # arbitrary label\n    parents = ~parent,\n    values  = ~size\n) %&gt;%\n  layout(\n    title = paste(\"File System Treemap for direcory \", root)\n  )\n\nfig\n\n\n\n\n\n\nFigure 7: Treemap of directory"
  },
  {
    "objectID": "posts/trees_R/revealjs.html#exercise-titanic",
    "href": "posts/trees_R/revealjs.html#exercise-titanic",
    "title": "Trees with R",
    "section": "Exercise: Titanic",
    "text": "Exercise: Titanic\nUse the Titanic data set to visualize the number of survivors in the various categories. Use the following hierarchy: Class -&gt; Sex -&gt; Age –&gt; Survival Status.\nDefine IDs and parent-IDs for each cell in this hierarchy: Class/Sex/Age/Survived.\n\n\nCode\ndf = as.data.frame(Titanic)\ndf %&gt;% head(10)\n\n# Prepare df\n\n# Plot\nfig = plot_ly(df, \n              type = 'sunburst', \n              branchvalues = 'total',\n              ids =~ id,\n              labels  = ~label,\n              parents = ~parent,\n              values  = ~n\n)\nfig"
  },
  {
    "objectID": "posts/trees_R/revealjs.html#sessioninfo",
    "href": "posts/trees_R/revealjs.html#sessioninfo",
    "title": "Trees with R",
    "section": "sessionInfo",
    "text": "sessionInfo\n\n\n\n\n\n\nsessionInfo\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.3 (2023-03-15)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/manke/miniconda3/envs/web/lib/libopenblasp-r0.3.21.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] gapminder_1.0.0 plotly_4.10.2   lubridate_1.9.2 forcats_1.0.0  \n [5] stringr_1.5.0   dplyr_1.1.1     purrr_1.0.1     readr_2.1.4    \n [9] tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0  xfun_0.38         colorspace_2.1-0  vctrs_0.6.1      \n [5] generics_0.1.3    htmltools_0.5.5   viridisLite_0.4.1 yaml_2.3.7       \n [9] utf8_1.2.3        rlang_1.1.0       pillar_1.9.0      glue_1.6.2       \n[13] withr_2.5.0       bit64_4.0.5       lifecycle_1.0.3   munsell_0.5.0    \n[17] gtable_0.3.3      htmlwidgets_1.6.2 codetools_0.2-19  evaluate_0.20    \n[21] knitr_1.42        tzdb_0.3.0        fastmap_1.1.1     crosstalk_1.2.0  \n[25] parallel_4.2.3    fansi_1.0.4       scales_1.2.1      vroom_1.6.1      \n[29] jsonlite_1.8.4    bit_4.0.5         hms_1.1.3         digest_0.6.31    \n[33] stringi_1.7.12    grid_4.2.3        cli_3.6.1         tools_4.2.3      \n[37] magrittr_2.0.3    lazyeval_0.2.2    crayon_1.5.2      pkgconfig_2.0.3  \n[41] ellipsis_0.3.2    data.table_1.14.8 timechange_0.2.0  rmarkdown_2.14   \n[45] httr_1.4.5        rstudioapi_0.14   R6_2.5.1          compiler_4.2.3   \n\n\n\n\n\n\n\nThomas Manke"
  },
  {
    "objectID": "posts/trees_R/trees_R.html",
    "href": "posts/trees_R/trees_R.html",
    "title": "Trees with R",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(gapminder)"
  },
  {
    "objectID": "posts/trees_R/trees_R.html#setup",
    "href": "posts/trees_R/trees_R.html#setup",
    "title": "Trees with R",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(gapminder)"
  },
  {
    "objectID": "posts/trees_R/trees_R.html#simple-sunburst-example",
    "href": "posts/trees_R/trees_R.html#simple-sunburst-example",
    "title": "Trees with R",
    "section": "Simple Sunburst Example",
    "text": "Simple Sunburst Example\nNotice that the simplicity of this example hides a few important issues\n\nlabels: are arbitrary (and can be redundant)\nids: should be unique\n\nif not used explicitly they will be implicitly set from labels\nmanual says they could not be numbers\n\nparents: must refer to the unique IDs\n\nRef: https://plotly.com/r/reference/sunburst/#sunburst\n\n\nCode\nfig &lt;- plot_ly(\n  labels = c(\"Eve\", \"Cain\", \"Seth\", \"Enos\", \"Noam\", \"Abel\", \"Awan\", \"Enoch\", \"Azura\"),   # = ids !!!\n  parents = c(\"\", \"Eve\", \"Eve\", \"Seth\", \"Seth\", \"Eve\", \"Eve\", \"Awan\", \"Eve\"),\n  values = c(10, 14, 12, 10, 2, 6, 6, 4, 4),\n  type = 'sunburst'\n)\n\nfig\n\n\n\n\n\nFigure 1: A simple sunburst exaample from plotly\n\n\n\nNotice: If all parents are set to root=\"\" then this reduces to a simple pie chart\n\n\nCode\nfig &lt;- plot_ly(\n  labels = c(\"Eve\", \"Cain\", \"Seth\", \"Enos\", \"Noam\", \"Abel\", \"Awan\", \"Enoch\", \"Azura\"),   # = ids !!!\n  parents = c(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"),\n  values = c(10, 14, 12, 10, 2, 6, 6, 4, 4),\n  type = 'sunburst'\n)\n\nfig\n\n\n\n\n\nFigure 2: A pie chart is a special case of a sunburst"
  },
  {
    "objectID": "posts/trees_R/trees_R.html#gapminder",
    "href": "posts/trees_R/trees_R.html#gapminder",
    "title": "Trees with R",
    "section": "Gapminder",
    "text": "Gapminder\n\nFilter and Subset Data\nAs usual we need some pre-processing and filtering of the data before we get started. In this case we only want to filter for year and reduce it to the relevant columns.\n\n\nCode\n# preprocess gapminder data\ngpm = gapminder %&gt;% \n  filter(year==2007) %&gt;%                 # filter specific year (the last)\n#  mutate_if(is.factor, as.character) %&gt;% # convert factors to character for uniformity (optional)\n  select(continent, country, pop)        # select only specific columns\n\n\n\n\nDefine hierarchy\nGoal: create a new data frame that contains node with labels, parent labels and values - with one line per node.\nPossible parent=“” to denote root node(s). In the gapminder example the continents are parents (and roots) of countries.\n\n\nCode\nroot=\"world\"\n# define nodes\ndf1 = gpm %&gt;% \n  group_by(continent) %&gt;%          # organize gapminder by continent\n  summarise(val=sum(pop)) %&gt;%      # continents value = sum(pop)  = sum over countries\n  mutate(parent=root) %&gt;%            # continents are root (no label)\n  select(label=continent, parent, val)\n\n\n# for each country (=leaf)-level define val(=pop), parent (continent) and label (country)\ndf2 = gpm %&gt;% \n  select(label=country, parent=continent, val=pop) \n\n# merge nodes (continents and countries)\ndf = df1 %&gt;% bind_rows(df2)\ndf %&gt;% head(10)\n\n\n# A tibble: 10 × 3\n   label       parent          val\n   &lt;fct&gt;       &lt;chr&gt;         &lt;dbl&gt;\n 1 Africa      world     929539692\n 2 Americas    world     898871184\n 3 Asia        world    3811953827\n 4 Europe      world     586098529\n 5 Oceania     world      24549947\n 6 Afghanistan Asia       31889923\n 7 Albania     Europe      3600523\n 8 Algeria     Africa     33333216\n 9 Angola      Africa     12420476\n10 Argentina   Americas   40301927\n\n\n\n\nPlotting\nThere are two popular plots to visualize hierarchical relationships with quantitative values: sunburst and treemaps.\nGiven a properly prepared dataframe they can be easily plotted with plot_ly.\nNotice: In the code below branchvalues = 'total' ensures that the relative arc length (sunburst) and areas (treemaps) correspond to the total values implied by the hierarchy. (Otherwise the arc length of the continents would be twice the length over all countries)\n\nSunbursts\n\n\nCode\nfig = plot_ly(df, type = 'sunburst', \n              branchvalues = 'total',\n              labels  = ~label,\n              parents = ~parent,\n              values  = ~val\n)\nfig\n\n\n\n\n\nFigure 3: Gapminder sunburst.\n\n\n\n\n\nTreemap\nIn fact, treemaps are preferred to visualize data of different proportions. For the same reasons that pie charts are discouraged.\n\n\nCode\nfig = plot_ly(df, type = 'treemap', \n        branchvalues = \"total\",\n        labels  = ~label,\n        parents = ~parent,\n        values  = ~val\n)\n\nfig\n\n\n\n\n\nFigure 4: Treemap of World Population.\n\n\n\n\n\n\nFurther Developments\nHere are my efforts to color-code the population size and also scale the countries separately for their respective continent.\nAnd I also tested how to combine figures - an additional challenge for plotly sunbursts.\nBeware: the setting for branchvalues has to be handled carefully.\n\n\nCode\n# simple scale function\nscale01 &lt;- function(x){(x-min(x))/(max(x)-min(x))}\n\n# calculate scaled population for each parent (continent)\ndf &lt;- df %&gt;% group_by(parent) %&gt;% mutate(val_s = scale01(val))\n\n# illustrate the parent-specific scaling for Europe\ndf %&gt;% filter(parent==\"Europe\") \n\n\n# A tibble: 30 × 4\n# Groups:   parent [1]\n   label                  parent      val  val_s\n   &lt;fct&gt;                  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 Albania                Europe  3600523 0.0402\n 2 Austria                Europe  8199783 0.0962\n 3 Belgium                Europe 10392226 0.123 \n 4 Bosnia and Herzegovina Europe  4552198 0.0518\n 5 Bulgaria               Europe  7322858 0.0855\n 6 Croatia                Europe  4493312 0.0511\n 7 Czech Republic         Europe 10228744 0.121 \n 8 Denmark                Europe  5468120 0.0629\n 9 Finland                Europe  5238460 0.0601\n10 France                 Europe 61083916 0.740 \n# ℹ 20 more rows\n\n\nCode\np1 &lt;- plot_ly() %&gt;% \n  add_trace(data = df,\n            type = \"sunburst\",    \n            branchvalues = 'total',\n            labels  = ~label,\n            parents = ~parent,\n            values  = ~val,\n            marker = list( color = ~val, \n                           colorscale='Blues', \n                           colorbar = list(title = \"Population\"),\n                           reversescale=TRUE,\n                           showscale=TRUE\n                           )\n            )\n\n\np2 &lt;- plot_ly() %&gt;% \n  add_trace(data = df,\n            type = \"sunburst\",    \n            # for the scaled values branchvalue='total' will cause problems\n            #branchvalues = 'total',\n            labels  = ~label,\n            parents = ~parent,\n            values  = ~val_s,\n            marker = list( color = ~val_s, \n                           colorscale='Blues', \n                           colorbar = list(title = \"Scaled Population\"),\n                           reversescale=TRUE,\n                           showscale=TRUE\n                           )\n            )\n\n#plotly::subplot(p1,p2)  #does not work: https://github.com/plotly/plotly.R/issues/1867\n\ncrosstalk::bscols(p1, p2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Two Sunburst of world population (2007). a) unscaled and b) scaled"
  },
  {
    "objectID": "posts/trees_R/trees_R.html#directory-content",
    "href": "posts/trees_R/trees_R.html#directory-content",
    "title": "Trees with R",
    "section": "Directory Content",
    "text": "Directory Content\nHere I want to visualize the file structure and size of a given directory.\n\nLoad Data\nGet all files and directories (under a given root folder) and determine their sizes using ‘du -a’\n\n\nCode\nroot=\"..\"\n#cmd=paste('find', root,' -type f -exec du -a {} +')\ncmd=paste('du -a', root)\ndf = read_tsv(pipe(cmd), col_names = c(\"size\", \"file_name\"))\n\n\n\n\nData Munging\nThe directory structure is a tree. The key steps below are to define appropriate node-IDs, node-labels and parents for each node.\n\nA sidetrack …\nIn retrospective the above goals sounds simple, but much work went into this lengthy sidetrack. which was caued by a misunderstanding on how node and parent labels work. The code cell below introduced some convenience function to extract only the partial directory name as parent, but this is overly complicated since parent should refer to a unique ID. I keep this code fragment only for reference because it took some time to develop\n\n\nCode\nget_2ndlast &lt;- function(L){\n  # get 2nd last element from a list\n  if (length(L) &gt;= 2) {\n    L[length(L) - 1]\n  } else {\n    \"root\"\n  }\n}\n\nget_parent &lt;- function(filename){\n  # get the direct parent directory of a file (before the last \"/\", possibly empty)\n  # split filename by directories %&gt;% convert to simple vector %&gt;% extract 2nd last element\n  \n  #this is not vectorize and doesn't work properly if filename is a vector, e.g df$file_name\n  strsplit(filename, \"/\") %&gt;% unlist %&gt;% .[length(.)-1]     \n}\n\n# to vectorize a function there are different strategies \n# this solution does not work: https://deanattali.com/blog/mutate-non-vectorized/\nget_parent_v &lt;- Vectorize(get_parent)\n\n# this version worked, but I stopped using this anyhow\nget_parent_v &lt;- function(filename){\n  strsplit(filename, \"/\") %&gt;% sapply(get_2ndlast) %&gt;% unlist\n}\n\n\nBelow we add a label and a parent-ID to the data frame. Notice the following choices in the code block below\n\nfile_name will serve as a unique ID of each file\nbasenanme(file_name) will serve as a convenient label for files (and directories)\ndirname(file_name) will serve as a unique ID for parent directories\n\n\n\n… and a simple solution\n\n\nCode\ndf = df %&gt;% \n  mutate(label=basename(file_name), parent=dirname(file_name)) \n  # this one does not help because the reduced directory name is not a unique ID\n  #mutate(node=basename(file_name), parent=get_parent_v(file_name)) \n\n#df %&gt;% summary()\ndf %&gt;% head()\n\n\n# A tibble: 6 × 4\n   size file_name                                                   label parent\n  &lt;dbl&gt; &lt;chr&gt;                                                       &lt;chr&gt; &lt;chr&gt; \n1     8 ../maps/maps_cache/html/overlap_data2_33cab96026ea11c4f90e… over… ../ma…\n2     8 ../maps/maps_cache/html/overlap_data2_33cab96026ea11c4f90e… over… ../ma…\n3     8 ../maps/maps_cache/html/ggplot_map_c910447fed13122597decb8… ggpl… ../ma…\n4    24 ../maps/maps_cache/html/overlap_data2_33cab96026ea11c4f90e… over… ../ma…\n5     8 ../maps/maps_cache/html/setup_04fa97c80f81582a529607a798d5… setu… ../ma…\n6     0 ../maps/maps_cache/html/maps_with_maps_f3b8224ebcb46befcca… maps… ../ma…\n\n\n\n\n\nSunburst\n\n\nCode\nfig = df %&gt;% \n  plot_ly(\n    type = 'sunburst', \n    branchvalues = \"total\",  # choice on how to represent \n    ids     = ~file_name,    # unique ID\n    labels  = ~label,        # arbitrary label\n    parents = ~parent,       # unique ID\n    values  = ~size\n) %&gt;%\n  layout(\n    title = paste(\"File System Sunburst for direcory \", root)\n  )\n\nfig\n\n\n\n\n\n\nFigure 6: A Sunburst directory\n\n\n\n\n\nTreemap\n\n\nCode\nfig = df %&gt;% \n  plot_ly(\n    type = 'treemap', \n    branchvalues = \"total\",\n    ids     = ~file_name,    # unique ID\n    labels  = ~label,        # arbitrary label\n    parents = ~parent,\n    values  = ~size\n) %&gt;%\n  layout(\n    title = paste(\"File System Treemap for direcory \", root)\n  )\n\nfig\n\n\n\n\n\n\nFigure 7: Treemap of directory"
  },
  {
    "objectID": "posts/trees_R/trees_R.html#exercise-titanic",
    "href": "posts/trees_R/trees_R.html#exercise-titanic",
    "title": "Trees with R",
    "section": "Exercise: Titanic",
    "text": "Exercise: Titanic\nUse the Titanic data set to visualize the number of survivors in the various categories. Use the following hierarchy: Class -&gt; Sex -&gt; Age –&gt; Survival Status.\nDefine IDs and parent-IDs for each cell in this hierarchy: Class/Sex/Age/Survived.\n\n\nCode\ndf = as.data.frame(Titanic)\ndf %&gt;% head(10)\n\n# Prepare df\n\n# Plot\nfig = plot_ly(df, \n              type = 'sunburst', \n              branchvalues = 'total',\n              ids =~ id,\n              labels  = ~label,\n              parents = ~parent,\n              values  = ~n\n)\nfig"
  },
  {
    "objectID": "posts/trees_R/trees_R.html#sessioninfo",
    "href": "posts/trees_R/trees_R.html#sessioninfo",
    "title": "Trees with R",
    "section": "sessionInfo",
    "text": "sessionInfo\n\n\n\n\n\n\nsessionInfo\n\n\n\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.3 (2023-03-15)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/manke/miniconda3/envs/web/lib/libopenblasp-r0.3.21.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] gapminder_1.0.0 plotly_4.10.2   lubridate_1.9.2 forcats_1.0.0  \n [5] stringr_1.5.0   dplyr_1.1.1     purrr_1.0.1     readr_2.1.4    \n [9] tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0  xfun_0.38         colorspace_2.1-0  vctrs_0.6.1      \n [5] generics_0.1.3    htmltools_0.5.5   viridisLite_0.4.1 yaml_2.3.7       \n [9] utf8_1.2.3        rlang_1.1.0       pillar_1.9.0      glue_1.6.2       \n[13] withr_2.5.0       bit64_4.0.5       lifecycle_1.0.3   munsell_0.5.0    \n[17] gtable_0.3.3      htmlwidgets_1.6.2 codetools_0.2-19  evaluate_0.20    \n[21] knitr_1.42        tzdb_0.3.0        fastmap_1.1.1     crosstalk_1.2.0  \n[25] parallel_4.2.3    fansi_1.0.4       scales_1.2.1      vroom_1.6.1      \n[29] jsonlite_1.8.4    bit_4.0.5         hms_1.1.3         digest_0.6.31    \n[33] stringi_1.7.12    grid_4.2.3        cli_3.6.1         tools_4.2.3      \n[37] magrittr_2.0.3    lazyeval_0.2.2    crayon_1.5.2      pkgconfig_2.0.3  \n[41] ellipsis_0.3.2    data.table_1.14.8 timechange_0.2.0  rmarkdown_2.14   \n[45] httr_1.4.5        rstudioapi_0.14   R6_2.5.1          compiler_4.2.3"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts-To-Myself",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nBuffon’s Needle\n\n\n\n\n\n\n\nPI\n\n\nshinylive\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nTrees with R\n\n\n\n\n\n\n\nplotly\n\n\ntreemap\n\n\nsunburst\n\n\ngapminder\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nGapminder (Animated)\n\n\n\n\n\n\n\nplotly\n\n\ngapminder\n\n\nanimation\n\n\ngeographic maps\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nWord Clouds with R\n\n\n\n\n\n\n\nword cloud\n\n\ntext analysis\n\n\nweb scraping\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nGapminder\n\n\n\n\n\n\n\nggplotly\n\n\ngapminder\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nGeographic Maps with R\n\n\n\n\n\n\n\ngeographic maps\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nTrees with Python (qmd)\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nQuarto with Python (ipynb)\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nQuarto with Python (qmd)\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nQuarto Intro\n\n\n\n\n\n\n\nQuarto\n\n\n\n\nSummary of useful features\n\n\n\n\n\n\nJun 1, 2023\n\n\nThomas Manke\n\n\n\n\n\n\n  \n\n\n\n\nVariance Stabilization\n\n\n\n\n\n\n\nvariance stabilization\n\n\ntransformations\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nThomas Manke\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gapminder_R/revealjs.html#summary-statistics",
    "href": "posts/gapminder_R/revealjs.html#summary-statistics",
    "title": "Gapminder",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nCode\nD=gapminder\nsummary(D)\n\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1"
  },
  {
    "objectID": "posts/gapminder_R/revealjs.html#number-of-countries-per-continent",
    "href": "posts/gapminder_R/revealjs.html#number-of-countries-per-continent",
    "title": "Gapminder",
    "section": "Number of Countries per Continent",
    "text": "Number of Countries per Continent\nTwo fancy bar plots:\n\n\nCode\np &lt;- gapminder %&gt;% \n  group_by(continent) %&gt;% \n  summarize(nc=n_distinct(country)) %&gt;%\n  ggplot(aes(x=continent, y=nc, fill=continent)) +\n  geom_bar(stat='identity') +\n  labs(y=\"Number of countries\")\n\np\n\n\n\n\n\nCode\np + coord_polar()"
  },
  {
    "objectID": "posts/gapminder_R/revealjs.html#distribution-of-life-expectancy",
    "href": "posts/gapminder_R/revealjs.html#distribution-of-life-expectancy",
    "title": "Gapminder",
    "section": "Distribution of Life Expectancy",
    "text": "Distribution of Life Expectancy\n\n\nCode\nggplot(data=gapminder, aes(x=lifeExp, fill=continent)) +\n    geom_density(alpha=0.3)"
  },
  {
    "objectID": "posts/gapminder_R/revealjs.html#life-expectancy-vs.-gdp-2002",
    "href": "posts/gapminder_R/revealjs.html#life-expectancy-vs.-gdp-2002",
    "title": "Gapminder",
    "section": "Life Expectancy vs. GDP (2002)",
    "text": "Life Expectancy vs. GDP (2002)\n\n\nCode\nD=gapminder %&gt;% filter(year==2002)\np = ggplot(D, aes(x = gdpPercap, y = lifeExp, color=continent)) +\n  geom_point(aes(size = pop, ids = country), alpha=0.7) + \n  #geom_bin2d(bins=10) +\n  scale_x_log10() +\n  geom_smooth(formula = y~x, se = F, method = \"lm\") + \n  facet_wrap(~continent)\n\n\n\nggplotly(p) %&gt;% highlight(\"plotly_hover\")\n\n\n\n\nLife Expectancy shown vs GDP (2002)\n\n\nCode\nggsave('gapminder.png', plot=p) # save for blog image"
  },
  {
    "objectID": "posts/gapminder_R/revealjs.html#life-expectancy-vs-gdp-vs-year",
    "href": "posts/gapminder_R/revealjs.html#life-expectancy-vs-gdp-vs-year",
    "title": "Gapminder",
    "section": "Life Expectancy vs GDP (vs year)",
    "text": "Life Expectancy vs GDP (vs year)\nHere I visualize the variable year dynamically.\n\n\nCode\n#g &lt;- crosstalk::SharedData$new(D, ~continent)\ngg = gapminder %&gt;% \n  group_by(continent) %&gt;%\n  ggplot(aes(gdpPercap, lifeExp, color = continent, frame = year)) +\n  geom_point(aes(size = pop), alpha=0.5) +\n  geom_smooth(formula = y ~x, se = F, method = \"lm\", show.legend = FALSE) +\n  scale_x_log10()\n\n\n\n\nCode\n# Notice that legends behave differently for ggplot and ggplotly: \n# + guides(size = FALSE) works as intended for ggplot but it would \n# include multiple redundant labels for color (=continent)\ngp = gg  # + guides(size = FALSE)\n#gp\nggplotly(gp) %&gt;% highlight(\"plotly_hover\")\n\n\n\n\n\n\nSame can be done with facet_wrap:\n\n\nCode\ngp = gg +  facet_wrap(~continent)\nggplotly(gp) %&gt;% highlight(\"plotly_hover\")"
  },
  {
    "objectID": "posts/gapminder_R/gapminder.html",
    "href": "posts/gapminder_R/gapminder.html",
    "title": "Gapminder",
    "section": "",
    "text": "Code\nlibrary(gapminder)\nlibrary(tidyverse)\nlibrary(plotly)"
  },
  {
    "objectID": "posts/gapminder_R/gapminder.html#summary-statistics",
    "href": "posts/gapminder_R/gapminder.html#summary-statistics",
    "title": "Gapminder",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nCode\nD=gapminder\nsummary(D)\n\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1"
  },
  {
    "objectID": "posts/gapminder_R/gapminder.html#number-of-countries-per-continent",
    "href": "posts/gapminder_R/gapminder.html#number-of-countries-per-continent",
    "title": "Gapminder",
    "section": "Number of Countries per Continent",
    "text": "Number of Countries per Continent\nTwo fancy bar plots:\n\n\nCode\np &lt;- gapminder %&gt;% \n  group_by(continent) %&gt;% \n  summarize(nc=n_distinct(country)) %&gt;%\n  ggplot(aes(x=continent, y=nc, fill=continent)) +\n  geom_bar(stat='identity') +\n  labs(y=\"Number of countries\")\n\np\n\n\n\n\n\nCode\np + coord_polar()"
  },
  {
    "objectID": "posts/gapminder_R/gapminder.html#distribution-of-life-expectancy",
    "href": "posts/gapminder_R/gapminder.html#distribution-of-life-expectancy",
    "title": "Gapminder",
    "section": "Distribution of Life Expectancy",
    "text": "Distribution of Life Expectancy\n\n\nCode\nggplot(data=gapminder, aes(x=lifeExp, fill=continent)) +\n    geom_density(alpha=0.3)"
  },
  {
    "objectID": "posts/gapminder_R/gapminder.html#life-expectancy-vs.-gdp-2002",
    "href": "posts/gapminder_R/gapminder.html#life-expectancy-vs.-gdp-2002",
    "title": "Gapminder",
    "section": "Life Expectancy vs. GDP (2002)",
    "text": "Life Expectancy vs. GDP (2002)\n\n\nCode\nD=gapminder %&gt;% filter(year==2002)\np = ggplot(D, aes(x = gdpPercap, y = lifeExp, color=continent)) +\n  geom_point(aes(size = pop, ids = country), alpha=0.7) + \n  #geom_bin2d(bins=10) +\n  scale_x_log10() +\n  geom_smooth(formula = y~x, se = F, method = \"lm\") + \n  facet_wrap(~continent)\n\n\n\nggplotly(p) %&gt;% highlight(\"plotly_hover\")\n\n\n\n\nLife Expectancy shown vs GDP (2002)\n\n\nCode\nggsave('gapminder.png', plot=p) # save for blog image"
  },
  {
    "objectID": "posts/gapminder_R/gapminder.html#life-expectancy-vs-gdp-vs-year",
    "href": "posts/gapminder_R/gapminder.html#life-expectancy-vs-gdp-vs-year",
    "title": "Gapminder",
    "section": "Life Expectancy vs GDP (vs year)",
    "text": "Life Expectancy vs GDP (vs year)\nHere I visualize the variable year dynamically.\n\n\nCode\n#g &lt;- crosstalk::SharedData$new(D, ~continent)\ngg = gapminder %&gt;% \n  group_by(continent) %&gt;%\n  ggplot(aes(gdpPercap, lifeExp, color = continent, frame = year)) +\n  geom_point(aes(size = pop), alpha=0.5) +\n  geom_smooth(formula = y ~x, se = F, method = \"lm\", show.legend = FALSE) +\n  scale_x_log10()\n\n\n\n\nCode\n# Notice that legends behave differently for ggplot and ggplotly: \n# + guides(size = FALSE) works as intended for ggplot but it would \n# include multiple redundant labels for color (=continent)\ngp = gg  # + guides(size = FALSE)\n#gp\nggplotly(gp) %&gt;% highlight(\"plotly_hover\")\n\n\n\n\n\n\nSame can be done with facet_wrap:\n\n\nCode\ngp = gg +  facet_wrap(~continent)\nggplotly(gp) %&gt;% highlight(\"plotly_hover\")"
  },
  {
    "objectID": "posts/gapminder_python/gapminder_revealjs.html#cell-magic",
    "href": "posts/gapminder_python/gapminder_revealjs.html#cell-magic",
    "title": "Gapminder (Animated)",
    "section": "Cell magic",
    "text": "Cell magic\nIt would be nice to get the cell magic working as with Jupyter. But the ipython code block seems to be ignored.\nSee: https://stackoverflow.com/questions/75024926/is-quarto-able-to-render-documents-with-ipython-magic\n%load_ext rpy2.ipython\n%watermark\n%watermark --iversions\n\n\nThomas Manke"
  },
  {
    "objectID": "posts/gapminder_python/gapminder.html",
    "href": "posts/gapminder_python/gapminder.html",
    "title": "Gapminder (Animated)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt                      # simple plots\nimport plotly.express as px                          # visualizations\nfrom statsmodels.tsa.vector_ar.var_model import VAR  # predictions\n\ndf=px.data.gapminder()\ndf.head()\n\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\niso_alpha\niso_num\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\nAFG\n4\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\nAFG\n4\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\nAFG\n4\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\nAFG\n4\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\nAFG\n4"
  },
  {
    "objectID": "posts/gapminder_python/gapminder.html#cell-magic",
    "href": "posts/gapminder_python/gapminder.html#cell-magic",
    "title": "Gapminder (Animated)",
    "section": "Cell magic",
    "text": "Cell magic\nIt would be nice to get the cell magic working as with Jupyter. But the ipython code block seems to be ignored.\nSee: https://stackoverflow.com/questions/75024926/is-quarto-able-to-render-documents-with-ipython-magic\n%load_ext rpy2.ipython\n%watermark\n%watermark --iversions"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html",
    "title": "Get Libraries",
    "section": "",
    "text": "Code\nfrom httpimport import github_repo\nwith github_repo( 'thomasmanke', 'ABS','ABS_tools') :\n  import ABS_tools\nCode\n%%script echo \"Replaced by module import from github\"\n# my plot function for confusion matrix\ndef plot_cm(mat):\n  classes = np.arange(cm.shape[0])\n  plt.imshow(mat, cmap=plt.cm.Blues)\n  for (j,i),label in np.ndenumerate(mat):\n    plt.text(i,j,np.round(label,2),ha='center',va='center')\n\n  plt.colorbar()\n  plt.title('Confusion Matrix')\n  plt.xlabel('True label')\n  plt.ylabel('Pred label')\n  plt.xticks(classes)\n  plt.yticks(classes)\n  plt.show()\nCode\n%matplotlib inline\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nprint('tf-version: ', tf.__version__)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#explore",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#explore",
    "title": "Get Libraries",
    "section": "Explore",
    "text": "Explore\n\n\nCode\n%%script echo edit before running\n... Data Exploration ...\n... shapes etc . ..."
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#repeat-try-a-simple-neural-network",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#repeat-try-a-simple-neural-network",
    "title": "Get Libraries",
    "section": "Repeat: Try a Simple Neural Network",
    "text": "Repeat: Try a Simple Neural Network\n… you might copy the model with one dense layer from the handwritten digits example.\n\n\nCode\n%%script echo define model here\n\nmodel.summary()"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#fit",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#fit",
    "title": "Get Libraries",
    "section": "Fit",
    "text": "Fit\nFit model for 20 epochs. Afterwards save the model and the metrics history.\nRAM limitations?\nThis has been tested on mybinder.org (with 2GB RAM) but if RAM should still be limited you may have to downsample the data further (see above) and/or use smaller batch sizes (16) here.\n\n\nCode\nfh = mod1.fit(X_train, y_train, epochs=20, validation_split=0.1, batch_size= 32, verbose=0)\n\nmodel_fn   = 'cifar_model_1.h5'  \nhistory_fn = 'cifar_history_1.npy'\nmod1.save(model_fn)\nnp.save(history_fn, fh)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#evaluation-and-predictions",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#evaluation-and-predictions",
    "title": "Get Libraries",
    "section": "Evaluation and Predictions",
    "text": "Evaluation and Predictions\n\n\nCode\ntest_loss, test_acc = mod1.evaluate(X_test, y_test)\n\na_name='sparse_categorical_accuracy'\nval_a_name = 'val_' + a_name\nplt.plot(fh.history['loss'])\nplt.plot(fh.history['val_loss'])\nplt.axhline(y=test_loss, color='green', linestyle='-.',label = 'test')\nplt.show()\n\nplt.plot(fh.history[a_name])\nplt.plot(fh.history[val_a_name])\nplt.axhline(y=test_acc, color='green', linestyle='-.',label = 'test')\nplt.show()\n\nmod1_pred = mod1.predict(X_test)           # probabilities    \ny_pred    = np.argmax(mod1_pred, axis=1)   # classes with max prob (= labels)\ncm=confusion_matrix(y_pred, y_test)\nplot_cm(cm)\n\n\nMessage:\nThere are several ways to improve this (more data, hyperparameters etc.)\nHowever, achieving higher (test and valdiation) accuracy on flattened images will generally be difficult \\(\\to\\) change network architecture"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#new-layers-filters-and-pools",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#new-layers-filters-and-pools",
    "title": "Get Libraries",
    "section": "New layers: Filters and Pools",
    "text": "New layers: Filters and Pools\n (from wiki.tum.de)\n\ninput layer: image shape [width, height, 3 (RGB)]\nconvolutional layer (Conv): n filters e.g (n,3,3,3)\n\ndetect pattern (e.g. horizontal, vertical, diagonal lines)\nhave the same depth as input\nseveral filter per layer: different filters applied to same spatial location in image\n\npooling filter (Pool)\n\nspatially downsampling, depth stay the same (e.g. max or average)\n\nfully conncted layer (Dense)\n\nconnect all previous nodes\n\n\nTypical structures: Input - Conv/Relu - Conv/Relu - Pool - Conv/Relu - … - Dense\nLower layers: Primitive “concepts”, Higher layers: Higher order “concepts”"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#cnn-fitting-gpus-and-batch-size",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#cnn-fitting-gpus-and-batch-size",
    "title": "Get Libraries",
    "section": "CNN: fitting, GPUs and batch size",
    "text": "CNN: fitting, GPUs and batch size\nThe following cell will fit the model. This will take some time - especially without dedicated hardware (e.g. GPU) or further optimization (improved algorithm).\nAlternatively you can obtain the model from the github directory and load it without learning (see instructions below)\n\n\nCode\nfh = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.1, verbose=0)\n\nmodel_fn   = model.name + '.h5'  \nhistory_fn = model.name + '_history.npy'\nmodel.save(model_fn)\nnp.save(history_fn, fh)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#loading-model-and-history",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#loading-model-and-history",
    "title": "Get Libraries",
    "section": "Loading Model and History",
    "text": "Loading Model and History\nIf the learning above takes too much time you may also obtain the trained model and metric history from the github data/ directory and load it here.\n\n\nCode\n#%%script echo Run after new start\nmodel_fn   = 'cifar_CNN_1.h5'\nhistory_fn = 'cifar_CNN_1_history.npy'\n\nmodel = tf.keras.models.load_model(model_fn)\nfh = np.load(history_fn, allow_pickle=True).item()\n\n# For the following to work you also need to load+normalize the test_data !!!!"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#evaluations",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#evaluations",
    "title": "Get Libraries",
    "section": "Evaluations",
    "text": "Evaluations\n\n\nCode\n%%script echo evaluate the model\n...\n\n\nMessage: An improvement over the previous network.\nDiscussion: What could be further improvements?"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#group-task-20-min-predictions",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#group-task-20-min-predictions",
    "title": "Get Libraries",
    "section": "Group Task (20 min): Predictions",
    "text": "Group Task (20 min): Predictions\nExplore individual predictions for test data\n\n\nCode\n%%script echo chose an index of your choice\nidx=...\nX = X_test[idx]\nX = np.expand_dims(X, axis=0)\n\npred = model.predict(X)\ni_max=np.argmax(pred)\ntrue_lab=class_names[y_test[idx][0]]\npred_lab=class_names[i_max]\n\nplt.figure(figsize=(12,6))\nax = plt.subplot(2,2,1)\nplt.imshow(X_test[idx]) \nplt.title(true_lab)\n\nax = plt.subplot(2,2,2)\nplt.bar(range(10), pred[0])\nplt.title(pred_lab)\nplt.show()\n\n\nCompare all predictions with ground truth for test data\n\n\nCode\nmodel_pred = model.predict(X_test)          # probabilities    \ny_pred    = np.argmax(model_pred, axis=1)   # classes with max prob (= labels)\ncm=confusion_matrix(y_pred, y_test)\nplot_cm(cm)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#filters",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#filters",
    "title": "Get Libraries",
    "section": "Filters",
    "text": "Filters\n\n\nCode\nlayer_name='first_conv'\nW,b = model.get_layer(layer_name).get_weights()  # by name\n# W,b = model.layers[0].get_weights()            # by index\n\nfig = plt.figure(figsize=(16, 8))\nnr, nc = (2,5)                    # define number of rows and columns\nfor i in range(0, nr*nc):\n    img = W[:,:,:,i]                                      # each filter has 3D\n    img = (img - img.min()) / (img.max() - img.min())     # scale -&gt; [0,1]\n    fig.add_subplot(nr, nc, i+1)\n    plt.imshow(img)\n    plt.title('filter: {}'.format(i))\n\nplt.show()\n\n\nNotice: Maximal activation for all RGB channels=1 (black) and minimal activation for all RGB channels = 0 (white)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_005_CNN.html#the-last-layer",
    "href": "lectures/ABS2022/ANN/ANN_005_CNN.html#the-last-layer",
    "title": "Get Libraries",
    "section": "The last layer",
    "text": "The last layer\nThe goal of the added convolutional layers is to obtain a better representation of the image data by representing spatial features (edges etc.). If this is successful then we would expect better separation properties in the penultimate layer (last before output layer).\n\n\nCode\nmodel.summary()\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# define new model: X -&gt; last layer\nlayer_name = 'last'\nlayer_model = tf.keras.Model(inputs=model.input,\n                             outputs=model.get_layer(layer_name).output)\n\nnr = 1000\ny=y_train[:nr]\nX=X_train[:nr]\nX_lay = layer_model(X).numpy()   # last layer representation of X\n\nX = X.reshape(nr,-1)                # flatten each image\nX_lay = X_lay.reshape(nr,-1)        # flatten each image (redundant)\n\nprint('X shapes: ',X.shape, X_lay.shape)\n\n#X_pca = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n#X_lay_pca = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X_lay)\n\nX_pca = PCA(n_components = 2).fit_transform(X)\nX_lay_pca = PCA(n_components = 2).fit_transform(X_lay)\nprint('PCA Shapes: ',X_pca.shape, X_lay_pca.shape)\n\nplt.figure(figsize=(12, 5))\ncm = plt.get_cmap('tab10')\n\nax = plt.subplot(1, 2, 1)\nplt.scatter( X_pca[:,0], X_pca[:,1] , c=y, cmap=cm)\nplt.title('PCA: X orig.')\n\nax = plt.subplot(1, 2, 2)\nplt.scatter( X_lay_pca[:,0], X_lay_pca[:,1] , c=y, cmap=cm)\nplt.title('PCA - X transformed')\n\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_002_LogRegression.html",
    "href": "lectures/ABS2022/ANN/ANN_002_LogRegression.html",
    "title": "002 - Logistic Regression",
    "section": "",
    "text": "Figure 1: Logistic regression\n\n\n\n\n\nNow aim to predict a specific flower species (label y) from length measurements (data X)\nWe can use the previous workflow for linear regression with only minor changes – to turn it into a classification tool.\n\nallow for multidimensional features as input (e.g. all 4 length measurements)\nmake probabilistic predictions y in [0,1]\nadjust the cost function\n\n\n\n\n\nCode\n# define data\nX = iris.data         # What is the shape of X? How many samples?\ny = (iris.target==2)  # What is iris.target? What is this equation ?\ny = y.astype(int)     # binarize y for class of choice\nnc = 1                # number of classes\n\ninput_shape = (1,)    # shape of X - without samples\nif (X.ndim &gt; 1):\n  input_shape = X.shape[1:]\n\n\n\n\n\n\n\n\n\n\nCode\n# check shapes frequently!\nprint('input_shape: ', input_shape)\nprint('X.shape:     ', X.shape)\nprint('y.shape:     ', y.shape)\nprint('classes:     ', nc)\n\nmodel = tf.keras.Sequential(name='my_log_model')\nmodel.add( tf.keras.layers.Dense(nc, input_shape = input_shape,\n                                 activation = 'sigmoid',\n                                 name='1st_layer'))\n\n# define optimizer and loss\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['binary_accuracy'])\n\n# see and understand the number of parameters\nmodel.summary()  \n\n\ninput_shape:  (4,)\nX.shape:      (150, 4)\ny.shape:      (150,)\nclasses:      1\nModel: \"my_log_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n 1st_layer (Dense)           (None, 1)                 5         \n                                                                 \n=================================================================\nTotal params: 5\nTrainable params: 5\nNon-trainable params: 0\n_________________________________________________________________\n\n\nSome definitions - known labels: \\(y_i \\in \\{0,1\\}\\) - predicted labels: \\(\\hat y_i \\in [0,1]\\) - accuracy: fraction of true assignments - cross-entropy (per sample \\(i\\)) \\[y_i \\log(\\hat y_i) + (1 - y_i) \\log(1-\\hat y_i) \\longrightarrow \\] Average the above over all samples \\(i\\)\n.\n\n\n\n\n\nCode\nfh = model.fit(X, y, epochs=200, verbose=0)\nplt.plot(fh.history['loss'][1:])\nplt.plot(fh.history['binary_accuracy'][1:])\nplt.show()\n\n\n\n\n\nNotice: If you run the above code cell repeatedly (without redefining the model), the fit the loss will continue to decrease.\n\n\nUnderstand the values and the shape of the predictions. Compare them to the observed label (use confusion matrix)\n\n\nCode\n%%script echo edit before execution\neval = model.evaluate(...)\nprint('[loss, accuracy] = ', eval)\n\nyp = model.predict( ... ) \n\n# Inspect and understand the values in yp and compare them to y\nprint('y.shape:  ', y.shape)\nprint('yp.shape: ', yp.shape)\nprint('y[45:55]: ',  y[45:55])\nprint('yp[45:55]: ', yp[45:55])\n\n\nRun the code below and inspect the confusion matrix\n\n\nCode\n%%script echo Ensure yp is defined before execution\nfrom sklearn.metrics import confusion_matrix\n# my plot function for cm\ndef plot_cm(mat):\n  classes = np.arange(cm.shape[0])\n  plt.imshow(mat, cmap=plt.cm.Blues)\n  for (j,i),label in np.ndenumerate(mat):\n    plt.text(i,j,np.round(label,2),ha='center',va='center')\n\n  plt.colorbar()\n  plt.title('Confusion Matrix')\n  plt.xlabel('True label')\n  plt.ylabel('Pred label')\n  plt.xticks(classes)\n  plt.yticks(classes)\n  plt.show()\n\n\ncm = confusion_matrix(yp&gt;=0.5, y)\nplot_cm(cm)\n\n\n\n\n\n\n\n\n\nChange the code above to fit a model and predict another species. Track the fit performance (model loss) and report the final confusion analysis.\nWhat could you do to improve performance?"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#define-the-data",
    "href": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#define-the-data",
    "title": "002 - Logistic Regression",
    "section": "",
    "text": "Code\n# define data\nX = iris.data         # What is the shape of X? How many samples?\ny = (iris.target==2)  # What is iris.target? What is this equation ?\ny = y.astype(int)     # binarize y for class of choice\nnc = 1                # number of classes\n\ninput_shape = (1,)    # shape of X - without samples\nif (X.ndim &gt; 1):\n  input_shape = X.shape[1:]"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#define-the-model",
    "href": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#define-the-model",
    "title": "002 - Logistic Regression",
    "section": "",
    "text": "Code\n# check shapes frequently!\nprint('input_shape: ', input_shape)\nprint('X.shape:     ', X.shape)\nprint('y.shape:     ', y.shape)\nprint('classes:     ', nc)\n\nmodel = tf.keras.Sequential(name='my_log_model')\nmodel.add( tf.keras.layers.Dense(nc, input_shape = input_shape,\n                                 activation = 'sigmoid',\n                                 name='1st_layer'))\n\n# define optimizer and loss\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['binary_accuracy'])\n\n# see and understand the number of parameters\nmodel.summary()  \n\n\ninput_shape:  (4,)\nX.shape:      (150, 4)\ny.shape:      (150,)\nclasses:      1\nModel: \"my_log_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n 1st_layer (Dense)           (None, 1)                 5         \n                                                                 \n=================================================================\nTotal params: 5\nTrainable params: 5\nNon-trainable params: 0\n_________________________________________________________________\n\n\nSome definitions - known labels: \\(y_i \\in \\{0,1\\}\\) - predicted labels: \\(\\hat y_i \\in [0,1]\\) - accuracy: fraction of true assignments - cross-entropy (per sample \\(i\\)) \\[y_i \\log(\\hat y_i) + (1 - y_i) \\log(1-\\hat y_i) \\longrightarrow \\] Average the above over all samples \\(i\\)\n."
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#inspecting-the-fit",
    "href": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#inspecting-the-fit",
    "title": "002 - Logistic Regression",
    "section": "",
    "text": "Code\nfh = model.fit(X, y, epochs=200, verbose=0)\nplt.plot(fh.history['loss'][1:])\nplt.plot(fh.history['binary_accuracy'][1:])\nplt.show()\n\n\n\n\n\nNotice: If you run the above code cell repeatedly (without redefining the model), the fit the loss will continue to decrease.\n\n\nUnderstand the values and the shape of the predictions. Compare them to the observed label (use confusion matrix)\n\n\nCode\n%%script echo edit before execution\neval = model.evaluate(...)\nprint('[loss, accuracy] = ', eval)\n\nyp = model.predict( ... ) \n\n# Inspect and understand the values in yp and compare them to y\nprint('y.shape:  ', y.shape)\nprint('yp.shape: ', yp.shape)\nprint('y[45:55]: ',  y[45:55])\nprint('yp[45:55]: ', yp[45:55])\n\n\nRun the code below and inspect the confusion matrix\n\n\nCode\n%%script echo Ensure yp is defined before execution\nfrom sklearn.metrics import confusion_matrix\n# my plot function for cm\ndef plot_cm(mat):\n  classes = np.arange(cm.shape[0])\n  plt.imshow(mat, cmap=plt.cm.Blues)\n  for (j,i),label in np.ndenumerate(mat):\n    plt.text(i,j,np.round(label,2),ha='center',va='center')\n\n  plt.colorbar()\n  plt.title('Confusion Matrix')\n  plt.xlabel('True label')\n  plt.ylabel('Pred label')\n  plt.xticks(classes)\n  plt.yticks(classes)\n  plt.show()\n\n\ncm = confusion_matrix(yp&gt;=0.5, y)\nplot_cm(cm)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#group-task-30-min-repeat",
    "href": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#group-task-30-min-repeat",
    "title": "002 - Logistic Regression",
    "section": "",
    "text": "Change the code above to fit a model and predict another species. Track the fit performance (model loss) and report the final confusion analysis.\nWhat could you do to improve performance?"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#preliminaries",
    "href": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#preliminaries",
    "title": "002 - Logistic Regression",
    "section": "Preliminaries",
    "text": "Preliminaries\nThe following will be almost identical to logistic regression (predicting one class label) with only small changes\n\nSoftmax: \\(z \\to \\hat y\\)\nThe softmax operation transforms a score vector (z) into a vector of probabilities (\\(\\hat y\\)):\n\\[\\hat y_l = \\frac{\\exp(z_l)}{\\sum_k \\exp(z_k)}\\]\nElements of \\(z\\) are real number (with any sign), but this definition ensures that \\(\\hat y_l \\ge 0\\) and \\(\\sum_l \\hat y_l = 1\\)\n\n\nCross-Entropy Loss\nFor a given sample, we have the known annotations \\(y\\) and the probabilistic predictions (\\(\\hat y)\\) from the model.\nFor \\(k\\) categories both vectors a \\(k\\)-dimensional. You may think of the true label \\(y\\) as “one-hot encoded” For example:\n\\[y = (0,0,1,0,0,0)\\]\nWhere the \\(1\\) denotes the true class label: the third out of six in this example.\nThe loss for a given sample is defined as\n\\[l(y, \\hat y)= - \\sum_l^k y_l \\log \\hat y_l\\]\nNotice that this sum is non-zero only for a single term from the correct class (where \\(y_l=1\\)). Moreover, if the model prediction \\(\\hat y_l\\) was certain for that class and that specific sample, then \\(l((y, \\hat y) = -\\log(\\hat y_l) = 0\\). In general the loss will be larger than 0.\nThe total loss will be the average over all samples losses."
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#model",
    "href": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#model",
    "title": "002 - Logistic Regression",
    "section": "Model",
    "text": "Model\n\n\nCode\nprint('input_shape: ', input_shape)\nprint('X.shape:     ', X.shape)\nprint('y.shape:     ', y.shape)\nprint('classes:     ', nc)\n\n# keras has many predefined loss functions and metrics\nloss_name='sparse_categorical_crossentropy'   # for integer labels\nacc='sparse_categorical_accuracy'             # additional metrics to track\n\nmodel = tf.keras.Sequential(name='my_softmax_model')\nmodel.add( tf.keras.layers.Dense(nc, input_shape = input_shape,\n                                 activation = 'softmax',\n                                 name='1st_layer'))\n\n\n# define optimizer and loss\nmodel.compile(optimizer='sgd', loss=loss_name, metrics=[acc])\n\n \n\n\ninput_shape:  (4,)\nX.shape:      (150, 4)\ny.shape:      (150,)\nclasses:      3\nModel: \"my_softmax_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n 1st_layer (Dense)           (None, 3)                 15        \n                                                                 \n=================================================================\nTotal params: 15\nTrainable params: 15\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#group-task-30-min-fit-and-evaluate-the-model",
    "href": "lectures/ABS2022/ANN/ANN_002_LogRegression.html#group-task-30-min-fit-and-evaluate-the-model",
    "title": "002 - Logistic Regression",
    "section": "Group task (30 min): Fit and Evaluate the model",
    "text": "Group task (30 min): Fit and Evaluate the model\n\nSummarize the model and understand the number of parameters\nInspect the fit history and evaluate the model: has it converged?\nrun a prediction: yp = model.predict(X). Understand the output.\nHow would you predict the label based on yp\nLook at the corresponding confusion matrix\n\n\n\nCode\n%%script echo Edit before execution\n# see and understand the number of parameters\n... show model summary ...\n\nfh= model.fit(....)\n\n# plot fit history: loss and accuracy\n...\n\neval = model.evaluate(...)\nprint('[loss, accuracy] = ', eval)\n\nyp = model.predict( ... )\n\n... inspect yp ...\n\nyp1 = ... predict label ...\n\n\ncm = confusion_matrix(yp1, y)\nplot_cm(cm)"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_001_Intro.html",
    "href": "lectures/ABS2022/ANN/ANN_001_Intro.html",
    "title": "001 - Introduction to Artificial Neural Networks",
    "section": "",
    "text": "A problem: How do we recognize (label) images? Can we write a program to do the same?\n\n\n“The principal difficulty … lay in the fact of there being too much evidence. What was vital was overlaid and hidden by what was irrelevant. Of all the facts which were presented to us we had to pick just those which we deemed to be essential, and then piece them together in their order, so as to reconstruct this very remarkable chain of events.” Sherlock Holmes (The Naval treaty, 1893)\n\n\nClassical Programming: Rules (f) + Data (X) –&gt; Answers (Y)\n\nMachine Learning: Answers (Y) + Data (X) –&gt; Rules (f)\n\nQuiz (5 min): A simpler challenge: Find the rule \\(y = f(x, \\beta)\\) for x and y below\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([-1,  0, 1, 2, 3, 4])     # Data\ny = np.array([-3, -1, 1, 3, 5, 7])     # Answer\n\n\nNotice: a more realistic scenario would allow for errors: \\(y = f(x, \\beta) + \\epsilon\\)\n\n\n\\[f(X, \\beta) = \\beta_0  + \\beta_1 x\\]\nOrdinary Least-squares fit: analytical\n\\[\\underset{\\beta}{\\mbox{argmin}} || y - f(X,\\beta)||^2 \\longrightarrow (\\beta_0, \\beta_1)=(-1,2)\\]\nPredictions: for two new values \\((x=10, -40)\\)\n\n\nCode\nx_new = np.array([10, -40])\ny_new = -1 + 2.0*np.array(x_new)\nprint('predictions: ', x_new, \"-&gt;\", y_new)\n\n\npredictions:  [ 10 -40] -&gt; [ 19. -81.]\n\n\n\n\n\nThe modeling steps: - define the data - define the model - fit - evaluate - predict\n\n\nCode\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n\nlm = linear_model.LinearRegression()  # define model\nxr = x.reshape(-1,1)                  # define data (+restructuring for specific tool) \n\nlm.fit(xr, y)                         # fit c.f. R: lm(Y ~ X)\n\n# report fit\nprint('Fitted Parameters       ', lm.intercept_, lm.coef_)\n\nyp = lm.predict(xr)                   # prediction\nMSE= mean_squared_error(y, yp)        # evaluate fit. other scores: R2=lm.score(xr, y)\nprint('Mean Squared Error:     ', MSE)\n\n# predict y for some new x\nx_new=np.array([10, -40])\ny_new = lm.predict(x_new.reshape(-1,1))\n\nprint('predictions:   ', y_new)\n\n\nFitted Parameters        -0.9999999999999991 [2.]\nMean Squared Error:      7.231224964525941e-31\npredictions:    [ 19. -81.]\n\n\n\n\n\n\n\nCode\nimport tensorflow as tf\nprint('tf version:',tf.__version__)\n\n# define model - the \"black box\"\nmodel = tf.keras.Sequential()\nmodel.add( tf.keras.layers.Dense(units=1, input_shape=[1]) )\n\n# define optimization and loss\nmodel.compile(optimizer='sgd', loss='mean_squared_error') \n\n# fit model ###\nfit_history = model.fit(x,y, epochs=100, verbose=0)       \n\n # report fit ####\nprint('Fitted Parameters             ', model.trainable_variables)\nprint('Mean Squared Error (loss):    ', fit_history.history['loss'][-1])\n\n# make predictions ####\nx_new = [ 10.0 , -40.0 ]\ny_new = model.predict(x_new)\ny_ana = -1 + 2.0*np.array(x_new)\n\nprint('analytical: ', y_ana)\nprint('numerical:  ', y_new)\n\n\ntf version: 2.8.2\nFitted Parameters              [&lt;tf.Variable 'dense_55/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.8585451]], dtype=float32)&gt;, &lt;tf.Variable 'dense_55/bias:0' shape=(1,) dtype=float32, numpy=array([-0.5614597], dtype=float32)&gt;]\nMean Squared Error (loss):     0.11189737915992737\nWARNING:tensorflow:5 out of the last 19 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f1a699cc8c0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nanalytical:  [ 19. -81.]\nnumerical:   [[ 18.02399]\n [-74.90327]]\n\n\n\n\n\nNotice: - tensorflow supports generic modeling steps: define model, define loss function, fit model, predict. - the most cryptic (and the most flexible!) part is the definition of the “black box”. We will spend much more time with this - so don’t panic. - There are many alternative frameworks: pyTorch, Caffe2, … In this course we use the high-level API Keras rather than Tensorflow directly. - tensorflow has new data structures that need to get used to: e.g. fitted_parameters - tensorflow model predictions appear less accurate (and slower) for this task of linear regression. This is because they have been obtained from an iterative approach (epcohs). In contrast, sklearn:LinearRegression() uses fast analytical tools (specific for linear regression) under the hood. - Tthe tensorflow approach is more generic and extends to much more complex models - The iteration can be monitored by the loss function (MSE) to assess convergence\n\n\nCode\nplt.figure()\nplt.plot(fit_history.history['loss'][1:])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n\n\n\n\n\nTask (10 min): Play with the new toy !\n\nThe model is fitted iteratively (iterations = epochs) to reduce some loss function (here: mean_squared error MSE).\nChange the number of epochs and/or the new data points \\(x\\_new\\). Observe the different results.\nYou may also activate the verbose function to see some progress reporting"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_001_Intro.html#the-linear-regression-way-gauss-1809",
    "href": "lectures/ABS2022/ANN/ANN_001_Intro.html#the-linear-regression-way-gauss-1809",
    "title": "001 - Introduction to Artificial Neural Networks",
    "section": "",
    "text": "\\[f(X, \\beta) = \\beta_0  + \\beta_1 x\\]\nOrdinary Least-squares fit: analytical\n\\[\\underset{\\beta}{\\mbox{argmin}} || y - f(X,\\beta)||^2 \\longrightarrow (\\beta_0, \\beta_1)=(-1,2)\\]\nPredictions: for two new values \\((x=10, -40)\\)\n\n\nCode\nx_new = np.array([10, -40])\ny_new = -1 + 2.0*np.array(x_new)\nprint('predictions: ', x_new, \"-&gt;\", y_new)\n\n\npredictions:  [ 10 -40] -&gt; [ 19. -81.]"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_001_Intro.html#the-python-way-sklearn-2013",
    "href": "lectures/ABS2022/ANN/ANN_001_Intro.html#the-python-way-sklearn-2013",
    "title": "001 - Introduction to Artificial Neural Networks",
    "section": "",
    "text": "The modeling steps: - define the data - define the model - fit - evaluate - predict\n\n\nCode\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n\nlm = linear_model.LinearRegression()  # define model\nxr = x.reshape(-1,1)                  # define data (+restructuring for specific tool) \n\nlm.fit(xr, y)                         # fit c.f. R: lm(Y ~ X)\n\n# report fit\nprint('Fitted Parameters       ', lm.intercept_, lm.coef_)\n\nyp = lm.predict(xr)                   # prediction\nMSE= mean_squared_error(y, yp)        # evaluate fit. other scores: R2=lm.score(xr, y)\nprint('Mean Squared Error:     ', MSE)\n\n# predict y for some new x\nx_new=np.array([10, -40])\ny_new = lm.predict(x_new.reshape(-1,1))\n\nprint('predictions:   ', y_new)\n\n\nFitted Parameters        -0.9999999999999991 [2.]\nMean Squared Error:      7.231224964525941e-31\npredictions:    [ 19. -81.]"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_001_Intro.html#the-tensorflowkeras-way-2022",
    "href": "lectures/ABS2022/ANN/ANN_001_Intro.html#the-tensorflowkeras-way-2022",
    "title": "001 - Introduction to Artificial Neural Networks",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nprint('tf version:',tf.__version__)\n\n# define model - the \"black box\"\nmodel = tf.keras.Sequential()\nmodel.add( tf.keras.layers.Dense(units=1, input_shape=[1]) )\n\n# define optimization and loss\nmodel.compile(optimizer='sgd', loss='mean_squared_error') \n\n# fit model ###\nfit_history = model.fit(x,y, epochs=100, verbose=0)       \n\n # report fit ####\nprint('Fitted Parameters             ', model.trainable_variables)\nprint('Mean Squared Error (loss):    ', fit_history.history['loss'][-1])\n\n# make predictions ####\nx_new = [ 10.0 , -40.0 ]\ny_new = model.predict(x_new)\ny_ana = -1 + 2.0*np.array(x_new)\n\nprint('analytical: ', y_ana)\nprint('numerical:  ', y_new)\n\n\ntf version: 2.8.2\nFitted Parameters              [&lt;tf.Variable 'dense_55/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.8585451]], dtype=float32)&gt;, &lt;tf.Variable 'dense_55/bias:0' shape=(1,) dtype=float32, numpy=array([-0.5614597], dtype=float32)&gt;]\nMean Squared Error (loss):     0.11189737915992737\nWARNING:tensorflow:5 out of the last 19 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f1a699cc8c0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nanalytical:  [ 19. -81.]\nnumerical:   [[ 18.02399]\n [-74.90327]]"
  },
  {
    "objectID": "lectures/ABS2022/ANN/ANN_001_Intro.html#dont-panic",
    "href": "lectures/ABS2022/ANN/ANN_001_Intro.html#dont-panic",
    "title": "001 - Introduction to Artificial Neural Networks",
    "section": "",
    "text": "Notice: - tensorflow supports generic modeling steps: define model, define loss function, fit model, predict. - the most cryptic (and the most flexible!) part is the definition of the “black box”. We will spend much more time with this - so don’t panic. - There are many alternative frameworks: pyTorch, Caffe2, … In this course we use the high-level API Keras rather than Tensorflow directly. - tensorflow has new data structures that need to get used to: e.g. fitted_parameters - tensorflow model predictions appear less accurate (and slower) for this task of linear regression. This is because they have been obtained from an iterative approach (epcohs). In contrast, sklearn:LinearRegression() uses fast analytical tools (specific for linear regression) under the hood. - Tthe tensorflow approach is more generic and extends to much more complex models - The iteration can be monitored by the loss function (MSE) to assess convergence\n\n\nCode\nplt.figure()\nplt.plot(fit_history.history['loss'][1:])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n\n\n\n\n\nTask (10 min): Play with the new toy !\n\nThe model is fitted iteratively (iterations = epochs) to reduce some loss function (here: mean_squared error MSE).\nChange the number of epochs and/or the new data points \\(x\\_new\\). Observe the different results.\nYou may also activate the verbose function to see some progress reporting"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html",
    "href": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html",
    "title": "00: Markov Chain Introduction",
    "section": "",
    "text": "Mechanistic models: describe process in terms of parameters: \\[ \\frac{d x}{d t} = f(x, \\Theta) ~~~~~\\mbox{e.g. Newton:}~ \\frac{d p}{d t} = F(x,m,g)\\]\n\nsimplifying assumption\ncausal input-output relations\n\nStatistical models: Describe data (distributions) in terms of parameters: \\[ X \\propto f(\\Theta) ~~~~~\\mbox{e.g. Gauss:}~ X \\propto N(\\mu, \\sigma^2)\\]\n\nsimplifying assumptions (or CLT)\nmodel \\(\\ne\\) mechanism\n\n\n\n\nStatistical models are machines that can generate numbers (according to some rules and given certain parameters).\n\n\n\nhuman_height\n\n\n(Source: ourworldindata.org)\nAssumption: measurements can be characterised by a (parametric) random process.\nDistinguish: discrete and continuous distributions and give examples\n\n\n\n\n…\n…\n…"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#statistical-models-as-number-generators",
    "href": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#statistical-models-as-number-generators",
    "title": "00: Markov Chain Introduction",
    "section": "",
    "text": "Statistical models are machines that can generate numbers (according to some rules and given certain parameters).\n\n\n\nhuman_height\n\n\n(Source: ourworldindata.org)\nAssumption: measurements can be characterised by a (parametric) random process.\nDistinguish: discrete and continuous distributions and give examples"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#discussion-what-can-you-do-with-models-why-are-some-useful",
    "href": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#discussion-what-can-you-do-with-models-why-are-some-useful",
    "title": "00: Markov Chain Introduction",
    "section": "",
    "text": "…\n…\n…"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#install-libraries",
    "href": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#install-libraries",
    "title": "00: Markov Chain Introduction",
    "section": "Install Libraries",
    "text": "Install Libraries\n… this needs to be run only once (if the libraries below were not yet installed).\nYou can remove this line or comment it out if you need this installation step.\n\n\nCode\n%%script echo skip installation\n!pip install matplotlib\n\n\nskip installation"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#load-libraries",
    "href": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#load-libraries",
    "title": "00: Markov Chain Introduction",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# line magic: enable plotting below code cell\n%matplotlib inline"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#sampling-from-poisson",
    "href": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#sampling-from-poisson",
    "title": "00: Markov Chain Introduction",
    "section": "Sampling from Poisson",
    "text": "Sampling from Poisson\n\ngenerates integer numbers \\(X\\ge 0\\)\nuseful for count data (horse accidents per year, number of birth per year, sequence reads per genome region)\nsimple: only one parameter, \\(\\lambda\\) (=expected counts)\n\n\n\nCode\nnp.random.seed(42)\nX = np.random.poisson(lam=1.0, size=1000)\nH = plt.hist(X)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#exploration",
    "href": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#exploration",
    "title": "00: Markov Chain Introduction",
    "section": "Exploration",
    "text": "Exploration\nGroup Task (15 min):\nWhich other (famous) distributions are possible ? Explore and modify the cell below.\nReport back to the class: - which distribution was chosen? - how many parameters ? - what is it useful for ?\n\n\nCode\n%%script echo edit before execution\nX = np.random[ ... your choice ... ]\nH = plt.hist(X)\n\n## you might prefer barplot for discrete distributions\n#labels, counts = np.unique(X, return_counts=True)\n#B = plt.bar(labels, counts, align='center', width=0.2)\n\n\ncomplete before execution\n\n\nSome ideas:\n\nChange parameters\nchange size\nchose another distribution\nbarplot instead of histogram\n\nSome suggestions can be found here: https://en.wikipedia.org/wiki/List_of_probability_distributions#Discrete_distributions\nTheir Python/numpy implementations are here: https://numpy.org/doc/1.16/reference/routines.random.html\nTip:\nTry “Shift-Tab” to activate Jupyter help (next to your module of interest), or try “np.random?”"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#sampling-from-any-distribution",
    "href": "lectures/ABS2022/MarkovChains/MC_000_Sampling.html#sampling-from-any-distribution",
    "title": "00: Markov Chain Introduction",
    "section": "Sampling from (any) distribution",
    "text": "Sampling from (any) distribution\nWe can build our own machine … For example; we could sample from a list of countries to visit\n\n\nCode\nL    = ['A',  'B',  'C']     # list of my favorite countries\nprob = [0.1, 0.2, 0.7]       # corresponding probabilities \nX = np.random.choice(a=L, p=prob, size=100)\nprint(*X, sep='')\nH = plt.hist(X)\n\n\nDiscussion: Is it useful? How many parameters does it have?\nLesson: Sampling is not confined to numbers. In general we sample from sets (“events”, “states”).\nTask (5 min): What happens if we 1. remove p=prob 2. replace p=[0.5, 0.5, 0.1]\nTry it out below and consult https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n\n\nCode\n%%script echo edit before execution\nX = [ ... your choices...]\nH = plt.hist(X)\n\n\nedit before execution"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n%matplotlib inline"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#a-strange-distribution---and-a-simple-proposal",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#a-strange-distribution---and-a-simple-proposal",
    "title": "Markov Chain Monte Carlo",
    "section": "A strange distribution - and a simple proposal",
    "text": "A strange distribution - and a simple proposal\nWe have learned how to sample values \\(x_i\\) from simple, well-known distributions (Uniform, Normal, Poisson, Binomial, …)\n\\[\nx_i \\propto p(x)\n\\]\nPython/numpy has several convenience functions, and so have many other packages and programming language: R, matlab, …\nBut imagine we want to sample from some arbitrary probability density function, \\(p(x) = f(x) /Z\\).\nWe assume that function \\(f(x)\\ge 0\\) can be calculated easily, but the normalization factor \\(Z\\) is unknown:\n\\[Z = \\int f(x) dx\\]\n\n\nCode\ndef func(x):\n  # invented some function: f(x)&gt;= 0\n  y = (1+np.sin(x))*np.exp(-np.abs(x))\n  return y\n\ndef prop(x, mu=0, sigma=1):\n  # define some well-known proposal distribution, e.g. Normal(x|mu, sigma)\n  return norm.pdf(x, loc=mu, scale=sigma)\n\nx=np.linspace(-5,5,1000)\nplt.fill_between(x, func(x),color='b',alpha=0.5, label='distribution of interest')\nplt.fill_between(x, prop(x,mu=2,sigma=1), color='r', alpha=0.5, label='proposal distribution')\nplt.legend()\nplt.show()\n\n\nSampling Goal:\nDraw samples \\(x_i\\) from \\(p(x)\\), such that the expectation of any function \\(A(x)\\) can be approximated \\[\nE[A(x)] = \\sum_x A(x) p(x) \\to \\sum_i A(x_i)\n\\]\nThere are a range of sampling methods\n\nInverse Transform Sampling\nImportance Sampling\nAccept-Reject Sampling\nMarkov Chain Monte Carlo (MCMC)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#the-metropolis-algorithm",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#the-metropolis-algorithm",
    "title": "Markov Chain Monte Carlo",
    "section": "The Metropolis Algorithm",
    "text": "The Metropolis Algorithm\nThis is the classical MCMC method (1953)\nThe idea / The goal:\nSample a sequence of \\(x_{t=1, ..., T}\\) from a Markov Chain with the desired stationary distribution \\(\\pi(x)=f(x)/Z\\): \\[\nx_t \\propto MC\n\\]\nThe Algorithm: 1. pick a random start \\(x_{t=0}\\) 2. propose next \\(x_p\\) from a simpler, feasible distribution, e.g. \\(N(x|\\mu=x_t, \\sigma^2)\\) 3. accept the proposed sample (\\(x_{t+1}=x_p\\)) with probability \\(f(x_p) / f(x_t)\\), otherwise (\\(x_{t+1}=x_t\\))\nNotice: - samples \\(x_t\\) are not iid, but are correlated by a random walk: \\(x_t \\to x_{t+1}\\)\n\na Markov chain with continuous variable\nfor illustration: \\(x_t \\in \\mathbb{R}\\); in practice: \\(x_t \\in \\mathbb{R}^n\\)\nMetropolis, Rosenbluth, Rosenbluth, Teller, Teller (J. Chem Phys, 1953) - 48k citations\nhttps://www.osti.gov/servlets/purl/4390578\nhttps://cs.gmu.edu/~henryh/483/top-10.html"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#group-task-30-min",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#group-task-30-min",
    "title": "Markov Chain Monte Carlo",
    "section": "Group Task (30 min)",
    "text": "Group Task (30 min)\n\nImplement the Metropolis algorithm as a function (below) and test it on the function \\(f(x)\\) above.\nOptional: Invent your own pdf (up to normalization)\nExplore how the initial value (\\(x\\)) and the number of samples (\\(T\\)) effect the sampled distribution\nDiscussion: How could you test or change the convergence properties of the Metropolis algorithm?\nReport your results to the other group\n\n\n\nCode\n%%script echo Edit before running cell\ndef MetropolisSampling(func, x=0, T=1000):\n  \"\"\" this is a simple implementation of the Metropolis algorithm \n  input:\n    - func: a probability density function (not necessarily normalized)\n    - x: a starting value for x (default: x=0)\n    - T: number of samples (default: T=1000)\n  output:\n    - list of sampled values x\n  \"\"\"\n  sample=[] # initialize empty list of values\n  for t in range(T):\n     # given \"state\" x, propose a new state xp \n     # chose normal distribution centered around x\n    xp = .... \n\n     # define the acceptance ratio as a ratio f(xp)/ f(x)\n    alpha = ...\n\n    # accept the proposal if alpha is bigger than a random number in (0,1)  \n    if (...):\n      x = xp\n\n    # add proposed x to sample list\n    sample.append(x)\n\n  return sample\n\nsample = MetropolisSampling(func, x=0, T=2000)\n\n# plot samples (\"traces\") and histogram \nfig, ax = plt.subplots(1, 2)\nfig.tight_layout()\nax[0].plot(sample)\nax[0].set_title('Trace Plot')\nax[1].hist(sample,100)\nax[1].set_title('Histogram')\nplt.show()"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#why-does-it-work",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#why-does-it-work",
    "title": "Markov Chain Monte Carlo",
    "section": "Why does it work?",
    "text": "Why does it work?\nClaim: We constructed an ergodic Markov chain \\(X_t \\to X_{t+1}\\)\nQuestion: What are the transition probabilities \\(P(X_{t+1}=b | X_t=a)\\)?\nWrite \\(P\\) as product of a proposal probability \\(N\\) and an acceptance probability \\(A\\) \\[\nPr(X_{t+1}=b | X_t=a) = A(X_{t+1}=b|X_t=a, X_p=b) ~ N(X_p=b|\\mu=a, \\sigma^2)\n\\]\n\\[\nPr(b | a) = A(b|a, b) ~N(b|a, \\sigma^2) \\\\\nPr(b | a) = \\min\\left(1, \\frac{f(b)}{f(a)}\\right) ~ N(b|a, \\sigma^2) = \\min\\left(1, \\frac{\\pi(b)}{\\pi(a)}\\right) ~ N(b|a, \\sigma^2)\n\\]\nDetailed balance: \\(Pr(a\\to b) = Pr(b\\to a)\\)\n\\[\n\\begin{align}\n\\pi(a)~P(b|a)  &= \\pi(a)~ \\min\\left(1, \\frac{\\pi(b)}{\\pi(a)}\\right) ~ N(b|a, \\sigma^2)\n           = \\min\\left(\\pi(a), \\pi(b)\\right) ~ N(b|a, \\sigma^2)\\\\\n           &= \\pi(b) ~ P(a|b)\n\\end{align}\n\\]\nSymmetries:\n\n\\(N(a|b) = N(b|a)\\)\n\\(\\min(\\pi(a), \\pi(b)) = \\min(\\pi(b),\\pi(a))\\)\n\nNotice: - other requirements: \\(P\\) irreducible & aperiodic. - Detailed balance also holds for other symmetric proposal distributions: \\(Q(a|b)=Q(b|a)\\) - unknown normalization \\(Z\\) cancels in the acceptance rate."
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#issues",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#issues",
    "title": "Markov Chain Monte Carlo",
    "section": "Issues",
    "text": "Issues\n\n“thermalization”: it will take some time before Markov Chain samples the desired stationary distribution\nchain onvergence:\n\ntheoretically considerations (rapid mixing), and\npractical approaches (run multiple chains)\n\nautocorrelation: try to make large steps (e.g. \\(\\sigma^2\\) large)\ncurse of dimensionality: small steps (move rapidly away from high probability regions in \\(D\\) dimensions)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#many-developments",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#many-developments",
    "title": "Markov Chain Monte Carlo",
    "section": "Many Developments",
    "text": "Many Developments\n\nasymmetric proposal distributions: Metropolis-Hastings\nGibbs sampler: less random sampling\nHamiltonian Monte Carlo: model dynamics in parameter space (introduce momentum)\nTools: Stan, pyStan, pymc (3/4)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#tools-pymc-arviz",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#tools-pymc-arviz",
    "title": "Markov Chain Monte Carlo",
    "section": "Tools: Pymc / Arviz",
    "text": "Tools: Pymc / Arviz\n\nPymc provides the above functionality (and much more).\nArviz adds statistical analysis and diagnostics.\n\n\n\nCode\n%%script echo Install only once (if necessary)\npip install pymc arviz\n\n\n\n\nCode\nimport pymc as pm\nimport arviz as az\n\nprint(pm.__version__)\n\n\nNotice: Make sure to use version 4 or higher. There are differences with pymc3, which uses different data formats and defaults."
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#metropolis-sampling",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#metropolis-sampling",
    "title": "Markov Chain Monte Carlo",
    "section": "Metropolis Sampling",
    "text": "Metropolis Sampling\n\n\nCode\nT=5000 # number of time steps (samples)\nnc=2   # number of chains\n\n# first define log(p) of the function (p&gt;0) to sample from\ndef log_func(x):\n  return np.log(func(x))\n\n# with context manager\nwith pm.Model():\n  pm.DensityDist('my_x', logp=log_func)     # define custom distribution\n  trace = pm.sample(T, chains=nc, step=pm.Metropolis())\n\npt=pm.plot_trace(trace)\n\n\n\nAutocorrelation\n\n\nCode\n# access autocorrelation\nac=pm.autocorr(np.array(trace.posterior.my_x))\nplt.plot(ac[0,:100]) # chain=0\nplt.plot(ac[1,:100]) # chain=1\nplt.show()\n\n# plot autocorrelation (for each chain)\nap=pm.plot_autocorr(trace)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#references",
    "href": "lectures/ABS2022/MarkovChains/MC_004_MCMC.html#references",
    "title": "Markov Chain Monte Carlo",
    "section": "References",
    "text": "References\n\nMCMC Theory (from pymc perspective): https://pymcmc.readthedocs.io/en/latest/theory.html\nMetropolis-Hastings: https://stephens999.github.io/fiveMinuteStats/MH_intro.html"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html",
    "title": "Analysing Markov chain properties",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#preparations",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#preparations",
    "title": "Analysing Markov chain properties",
    "section": "Preparations",
    "text": "Preparations\nImport Required Modules and Define Convenience Functions\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_sequence(P, s=0, T=100):\n\n  assert P.shape[0]==P.shape[1],         \"generate_sequence: P should be a squared matrix\"\n  assert np.allclose( P.sum(axis=1), 1), \"generate_sequence: P should be a stochastic matrix\"\n\n  ns = P.shape[0] # number of states\n  L = [s]         # initial state\n\n  # loop for T time steps\n  for t in range(T):\n    s = np.random.choice( ns, p = P[s, :] )\n    L.append(s)\n  return L\n\n\nLet’s make sure we start from the same model (transition probabilities)\n\n\nCode\nP = np.array(\n    [[0.8, 0.2], \n     [0.1, 0.9]]\n)\nX=generate_sequence(P)\nprint(*X, sep='')"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#probability-of-a-sequence",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#probability-of-a-sequence",
    "title": "Analysing Markov chain properties",
    "section": "Probability of a Sequence",
    "text": "Probability of a Sequence\nThe probability of an observed state sequence \\(X=(X_1, X_2, \\ldots X_T)=X_{1:T}\\) is an important quantity:\n\\[\nPr(X|P) = Pr(X_1) Pr(X_2|X_1) Pr(X_3|X_2) \\cdots Pr(X_T|X_{T-1})\n\\]\nIt may be used to classify a given observation \\(X\\) as more or less likely to emerge from different models\n\\[\nPr(X|P_1) &lt; Pr(X|P_2)\n\\]\nGroup Task (20 min): Strange coins\nYour favorite casino seems to use very strange coins and you try 2 different Markov models to model the occurence of heads (0) and tails (1). The transition probabilities read \\[\nP_1 = \\begin{bmatrix}\n0.2   &  0.8 \\\\\n0.8   &  0.2\n\\end{bmatrix}\n\\]\n\n\n\\[\nP_2 = \\begin{bmatrix}\n0.8   &  0.2 \\\\\n0.8   &  0.2\n\\end{bmatrix}\n\\]\nYou also assume that the initial probability for observing heads (0) or tails (1) is 50% in both cases.\n\nGive a verbal account of the model and contrast this with a fair coin.\nComplete the code block below to calculate the probability of the sequence \\(X=0010000011\\) under your model.\nCan you calculate the probability for a fair coin analytically (with a simple formula)?\nExchange the results with the other group and decide which model is more likely.\n\n\n\nCode\n%%script echo Edit before executing\nX = [0,0,1,0,0,0,0,0,1,1]\nP = ...\n\nprob = ...                     # initial probability: P(x_1=0)\n# loop over all edges in chain \nfor t in range( ... ):\n    x1= ...                   # current state\n    x2= ...                   # next state\n    prob = prob* ...          # probability update\n\n    # you might want to print for debugging\n    print('t={}, x1={}, x2={} P[s1,s2]={}'.format(t,x1,x2, P[x1,x2]) )\n\nprint('prob (model)     =', prob)\nprint('prob (fair coin) =', ... formula ? ...)\n\n\nFollow up: Duplicate (repeat) the sequence \\(X\\) to obtain a new \\(X\\) with twice the length? What can you say about the probability?\nMessages - Markov Models are probabilistic models: given the transition matrix \\(P\\) we can calculate many interesting probabilities - More specifically: the probability of a given sequences. Decreases with sequence length - better use logs for long sequences! - Two approaches: Mathematical Treatment of stochastic matrices vs Numerical Simulations (algorithmic efficiency)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#counting-occurrences",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#counting-occurrences",
    "title": "Analysing Markov chain properties",
    "section": "counting occurrences",
    "text": "counting occurrences\n\n\nCode\n# define new transition matrix\nP = np.array( [[0.8, 0.2], [0.1, 0.9]] )\nnp.random.seed(42)\nX = generate_sequence(P, s=0)\n\nstates, counts = np.unique(X, return_counts=True)\nB = plt.bar(states.astype(str), counts, align='center', width=0.2)\n\n\nTask (5 min):\nRepeat the above plot for different initial states s and different length of sequences.\nWhat do you observe?"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#calculating-higher-order-probabilities",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#calculating-higher-order-probabilities",
    "title": "Analysing Markov chain properties",
    "section": "Calculating higher order probabilities",
    "text": "Calculating higher order probabilities\nA Markov Model is defined by all 1-step transition probabilities: \\(P_{ij}\\) (for \\(i \\to j\\)).\nWhat is the probability for transitions \\(i \\to j\\) in 2 steps, 3 steps, …t-steps?\n\\[P_{ij}(t) = P(X_t = j | X_0 = i)\\]\nGroup task (15 min): Consider a specific case for the given \\(P\\) above. Starting at \\(t=0\\) in state i=0, what are the probabilities at time \\(t\\)\nGroup 1: to be in state j=0?\nGroup 2: to be in state j=1?\nIn other words: Group 1 \\(\\to\\) Calculate \\(P_{00}(t)\\). Group 2 \\(\\to\\) Calculate \\(P_{01}(t)\\).\n\nat time t=1 ?\nat time t=2 ?\n\nHint: Consider all paths that go from \\(i \\to j\\)\n\n\nCode\n%%script echo Edit before executing\nprint('P01(t=1): ', ... )\nprint('P01(t=2): ', ... )\nprint('P00(t=1): ', ... ) \nprint('P00(t=2): ', ... ) \n\n\nFor larger \\(t\\), these calculations can fast become cumbersome …\n\nGeneralization and Simplification\nRequired Tools: 1. Matrix Multiplication: \\(C_{ij} = \\sum_k A_{ik}B_{kj}\\) 2. Marginalization: \\(P(A) = \\sum_B P(A|B) P(B)\\)\n\nInitialization: \\(P_{ij}(t=0) = \\delta_{ij}\\)\n1-step probabilties: \\(P_{ij}(t=1) = P_{ij}\\) (Definition of Markov Chain)\nRecursion: (marginalization over all states at time \\(t\\))\n\n\\[\n\\begin{align}\nPr(X_{t+1}=j|X_0=i) &= \\sum_k Pr(X_{t+1}=j | X_t=k, X_0=i) \\cdot Pr(X_t=k|X_0=i) \\\\\n&= \\sum_k Pr(X_{t+1}=j | X_t=k) \\cdot Pr(X_t=k|X_0=i) \\\\\nP_{ij}(t+1) &= \\sum_k P_{kj} \\cdot P_{ik}(t) = (P^{t+1})_{ij}\n\\end{align}\n\\]\n\\(\\sum_k\\) is the sum over all connections from state \\(k\\) to \\(j\\), and \\(p_{ik}(t)\\) is the previous result over all possible pathways from \\(i\\) to \\(k\\)\nThe above is for the t-step transition matrix. For the probability distribution over states we have:\n\\[\n\\begin{align}\nPr(X_{t}=j) &= \\sum_k Pr(X_t=j | X_o=k) \\cdot Pr(X_0=k) \\\\\n             &= \\sum_k P_{kj}(t) \\cdot Pr(X_0=k)\n\\end{align}\n\\]\nSimplification: (in vector notation)\n\\[\\pi(t) = \\pi(0) \\cdot P^t\\]\nTask (5min): Matrix Power\nExplore the function matrix_power() and apply this function to \\(P\\) with an increasing value of \\(t\\)\n\n\nCode\n%%script echo Edit before executing\nfrom numpy.linalg import matrix_power\nmatrix_power( ... )\n\n\n\nTask (10 min): Deja-vu\nStarting in state 0 at t=0, calculate the probabilities to be in state 1\n\nat time t=1 ?\nat time t=2 ?\nat time t=20 ?\n\nSolve it using matrix multiplication: https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_power.html\n\n\n\nCode\n%%script echo Edit before executing\nfrom numpy.linalg import matrix_power\npi_0 =        # initial probability\npi_t =        # dot product of initial probability with P^k\n\n\n\nTask (5 min):\nRepeat the above question with state 1 at t=0.\n\nat time t=1 ?\nat time t=2 ?\nat time t=20 ?\n\nExtra: Repeat the above with an uncertain state \\(\\pi=[0.5, 0.5]\\)\n\nQuestion: What is different?\nLessons:\n\nImportance of recursions\nMatrix multiplication is powerful tool (and easy with Python/Numpy)!\nMatrix powers of \\((P^t)_{ij}\\) give the t-step transition probabilities from \\(i\\) to \\(j\\)\n\\(\\pi(t) = \\pi(0) \\cdot P^t\\) gives the state probability distribution at time \\(t\\)\nImportant role of linear algebra, efficient algorithms and tools\nConvergence and Independence of Initial State"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#long-time-properties",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#long-time-properties",
    "title": "Analysing Markov chain properties",
    "section": "Long-time Properties",
    "text": "Long-time Properties\nGiven this image of a Markov Chain, can you speculate on its long-time properties?\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/MC_AbsorbingGraph.jpg”, align=left width=“800”&gt;\n\nTask (5 min):\nLet’s test it numerically! Define the corresponding transition matrix \\(P\\) and run the code cell below.\n\n\nCode\n%%script echo Edit before executing\nP= ....\n\nX = generate_sequence(P)\nprint(*X, sep='')\n\nstates, counts = np.unique(X, return_counts=True)\nB = plt.bar(states.astype(str), counts, align='center', width=0.2)\n\n\nTask (5 min): What was the initial state? Modify the above code to chose a different initial state. How does the output change?\nLesson: Some Markov Chains (transition matrices) can have absorbing states. More generally there could be multiple absorbing states."
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#visualizating-matrix-powers",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#visualizating-matrix-powers",
    "title": "Analysing Markov chain properties",
    "section": "Visualizating Matrix Powers",
    "text": "Visualizating Matrix Powers\nLet’s make the temporal analysis more visually pleasing.\n\n\nCode\nfrom numpy.linalg import matrix_power\nfig, ax = plt.subplots(nrows=1, ncols=5, figsize=(10,5))\ni=0\nfor t in [0,1,2,10,100]:\n  S=matrix_power(P,t)\n  im=ax[i].imshow(S, cmap=plt.cm.Blues)\n  ax[i].set_title(t)\n  i = i + 1\n\ncax = fig.add_axes([0.1, 0.2, 0.8, 0.05])\nfig.colorbar(im, cax=cax, orientation='horizontal')\nplt.show()\n\n\nDiscussion: What do we see here?\nI’m proud of the above visualization, so let’s make it a (slightly more flexible) function.\n\n\nCode\ndef plot_transition_matrix(P, tmax=100):\n\n  # some sanity checks\n  assert P.shape[0]==P.shape[1],         \"P should be a squared matrix\"\n  assert np.allclose( P.sum(axis=1), 1), \"P should be a stochastic matrix\"\n\n  fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(10,5))\n  i=0\n  for t in [0,1,2,10,tmax]:\n    S=matrix_power(P,t)\n    im=ax[i].imshow(S, cmap=plt.cm.Blues)\n    ax[i].set_title(t)\n    i = i + 1\n  cax = fig.add_axes([0.1, 0.2, 0.8, 0.05])\n  fig.colorbar(im, cax=cax, orientation='horizontal')\n  plt.show()"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#convergence-rate",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#convergence-rate",
    "title": "Analysing Markov chain properties",
    "section": "Convergence Rate",
    "text": "Convergence Rate\nWe saw that the Markov chain above converges to some asymptotic state - let’s explore and modify this behaviour.\nGroup Tasks (30min): Adjust the parameters in \\(P\\) to\n\ndelay the convergence to the absorbing state 3.\ngenerates two absorbing states\navoid absorbing states\n\n\n\nCode\n%%script echo Edit before eexeecution\nP1=np.array(...)\nP2=np.array(...)\nP3=np.array(...)\n\nplot_transition_matrix(P1, 100)\nplot_transition_matrix(P2, 100)\nplot_transition_matrix(P3, 100)\n\n\nLessons: The long time behaviour depends on model structure and parameters\n\nParameters may control convergence rate\nstructural aspects (state accessibilty)may cause multiple absorbing states. The Markov chain will still converge. But \\(\\to\\) initial state sensitivity\nsome (interesting) Markov chains have a unique stationary state that is independent from the initial state.\n\nNotice:\n\nstationary distribution (sometimes called steady state distribution) does not mean that the states don’t change, but that the probability distribution is constant over time (and independent of the initial state)\nthe power method is a simple but expensive way to determine the stationary distribution \\(\\pi\\), but there are more effcient ways using the eigenvalue formulation \\[\\pi = \\pi \\cdot P\\]\nthis formula also exends the idea of the stationary distribution beyond the long-term behaviour of a Markov chain; a stationary distribution is a distribution that stays fixed after application of \\(P\\) (regardless of whether it can be reached through an asymptotic process)\na Markov chain can have several stationary distributions\nMany Markov chains have a unique stationary distribution and they are usually the most interesting/important ones. However, we need to be aware of - and protect against - other possibilities (see below)"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#periodic-states",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#periodic-states",
    "title": "Analysing Markov chain properties",
    "section": "Periodic States",
    "text": "Periodic States\nOften we want to avoid periodicity as another “pathological” condition.\n\n\nCode\nP=np.array([[0.0,1.0,0.0],\n            [0.5,0.0,0.5],\n            [0.0,1.0,0.0]])\n\nplot_transition_matrix(P, 11)\n\n\nNotice: any self-transition (\\(P_{ii} \\ne 0\\)) will result in an aperiodic Markov chain."
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#summary",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#summary",
    "title": "Analysing Markov chain properties",
    "section": "Summary",
    "text": "Summary\nWe have seen different possible behaviours of Markov chains 1. convergence to unique state (stationary or absorbing) 2. convergence to non-unique state: inital state sensitivity 3. periodic behaviour\n“Nice” Markov chains are “ergodic” (everything that can happen will eventually happen) and “non-periodic”"
  },
  {
    "objectID": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#expected-times-convergence-and-return",
    "href": "lectures/ABS2022/MarkovChains/MC_002_Properties.html#expected-times-convergence-and-return",
    "title": "Analysing Markov chain properties",
    "section": "Expected Times: Convergence and Return",
    "text": "Expected Times: Convergence and Return\n\nConvergence Time\nThe convergence of a Markov chain \\(P\\) to its stationary distribution \\(\\pi\\) is a very important property.\nStarting from state \\(\\pi_0=(1, 0)\\) the approximation to the stationary distribution is given by\n\\[\n\\pi_t = \\pi_0 P^t\n\\]\nThe distance from the stationary distribution is given by the Euclidean norm \\[\n\\delta(t) = ||\\pi_t - \\pi||\n\\]\nsee also numpy: np.linalg.norm()\nTask (10 min):\nAsymptotically the Markov chain defined by \\(P\\) (above) has stationary distribution \\(\\pi=(1/3, 2/3)\\). Calculate the distance of \\(\\delta(t)\\) and plot it logarithmically against \\(t=1 \\ldots 50\\).\n\n\nCode\n%%script echo Edit before executing\nfrom numpy.linalg import matrix_power\n\nP  = np.array([[0.8, 0.2],[0.1,0.9]])  # transition matrix\npi = np.array([1./3, 2./3])            # stationatry distribution\nT  = 50                                # maximal number of times\n\npi_0 = np.array([1,0])                 # initial distribution\n\ndist = np.zeros(T)                     # initialize T distances\nfor t in range(T):\n  pi_t =        # hint: P^t\n  dist[t] = ... # calculate distance --&gt; hint: np.linalg.norm\n#  print(pi_t, dist[t])\n\nplt.plot(dist)\nplt.yscale('log')\nplt.xlabel('time')\nplt.ylabel('distance from stationary')\nplt.show()\n\n\nMessage: The convergence is exponential. The precise rate is controlled by matrix properties of \\(P\\) (its eigenvalues).\n\n\nState Duration\nHow long is the expected stay \\(E[t_i]\\) in a given state \\(i\\)?\n\\[\nPr(X_1=i, X_2=i, \\ldots X_{t}=i, X_{t+1} \\ne i) = p_{ii}^{t-1} (1-p_{ii}) \\equiv p_i(t) \\\\ \\\\\nE[t_i] = \\sum_{t=1}^\\infty t p_i(t) = (1-p_{ii}) \\sum_t t p_{ii}^{t-1} = \\frac{1}{1-p_{ii}}\n\\]\nTask (10 min): Define your favorite Markov Model (transition matrix) and generate a (sufficiently) long sequence from it. Pick a specific state \\(i\\) and confirm the above formula. A simple helper function is provided below, but try to understand it.\n\n\nCode\ndef StateDuration(L, s=0):\n  # helper function to calculate the \"duration\" of a given element s in list L\n  # --&gt; calculate the length of consecutive occurences\n  count=0\n  res=[]\n  for e in L:\n    if e==s:\n      count +=1\n    else:\n      if (count &gt; 0):\n        res.append(count)\n      count=0\n\n  if (count&gt;0):\n    res.append(count)\n  return res\n\n\n\n\nCode\n%%script echo Edit before executing\nP = np.array( ... your choice here)  # chose transition matrix\ni=0                                   # chose states\n\nX=generate_sequence(P, T= ... )\n\nexp = ... # expected duration\nres = StateDuration(...)\nprint('expected duration: {} average: {}'.format(exp, sum(res)/len(res)))"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html",
    "title": "001 - Forward Algorithm",
    "section": "",
    "text": "Model parameters: \\(\\Theta\\), Observations: \\(X\\), Hidden States: \\(Z\\)\n\nscoring (observations): \\(Pr(X) \\longrightarrow\\) Forward Algorithm\ndecoding (hidden variables):\n\nbest posterior state: \\(argmax_i Pr(Z_t=i|X) \\longrightarrow\\) Forward-Backward Algorithm\nbest state sequence: \\(argmax_Z Pr(Z|X) \\longrightarrow\\) Viterbi Algorithm\n\nlearning (model parameters) \\(argmax_\\Theta Pr(\\Theta|X) \\longrightarrow\\) Baum-Welch Algorithm"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#third-idea-dynamic-programming-reuse-previous-calculations",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#third-idea-dynamic-programming-reuse-previous-calculations",
    "title": "001 - Forward Algorithm",
    "section": "Third idea: Dynamic Programming (reuse previous calculations)",
    "text": "Third idea: Dynamic Programming (reuse previous calculations)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#idea-and-visalization",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#idea-and-visalization",
    "title": "001 - Forward Algorithm",
    "section": "Idea and Visalization",
    "text": "Idea and Visalization\nWe don’t know any \\(Z_t\\), so we need to track all possibilities: \\(\\longrightarrow\\) Trellis graph.\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_HiddenTrellis.jpg”, width=“1000”&gt;\n\nRecursion again !!!\nLet’s assume that at some time \\(t\\) we already know the joint probability for the observed sequence \\(X_{1:t}\\) and the hidden state \\(Z_t\\) (for each possible value of \\(Z_t=i\\)).\nThis information is stored in the forward variable: \\(\\alpha_{ti} = Pr(X_{1:t}, Z_t=i)\\). This is a vector of joint probabilities that will be propagated forward in time.\nThis works efficiently because of the Markov property (separation of future from past, given the present)."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#algorithmic-formulation-iteration",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#algorithmic-formulation-iteration",
    "title": "001 - Forward Algorithm",
    "section": "Algorithmic Formulation: Iteration",
    "text": "Algorithmic Formulation: Iteration\n\n1. Initialization (\\(t=1\\))\n\\[\n\\alpha_{1i} = Pr(X_1, Z_1=i) = Pr(X_1|Z_1=i) Pr(Z_1=i)\n\\] - \\(\\to\\) element-wise multiplication of a row from emission matrix with initial state distributions - \\(Pr(X_1|Z_1=i)\\) is one element of the emission matrix \\(E_{ik}\\) (row i = state, column k = k(1) observed valuee of \\(X_1\\)). - \\(Pr(Z_1=i)\\) is the i-th element of the initial state distribution \\(\\pi_i\\).\n\n\n2. Induction (\\(t \\to t+1\\)): state transition + new observation\n\n2.1 state transition (\\(Z_t \\to Z_{t+1}\\)): Consider all possible Markov transitions and sum them up. \\[\n\\begin{align}\nPr(Z_{t+1}=i, X_{1:t}) &= \\sum_k Pr(Z_{t+1}=i, Z_t=k, X_{1:t}) \\\\\n&=\\sum_k Pr(Z_{t+1}=i|Z_t=k, X_{1:t}) Pr(Z_t=k,X_{1:t}) \\\\\n&= \\sum_{k} P_{ki} \\alpha_{tk}  = \\sum_{k} \\alpha_{tk} P_{ki}\n\\end{align}\n\\] \\(\\to\\) matrix multiplcation of row vector \\(\\alpha_{tk}\\) with transition matrix\n2.2 new observation (\\(X_{t+1}\\)): consider emission probability resulting in observation \\(X_{t+1}\\) \\[\nPr(Z_{t+1}=i, X_{1:t}, X_{t+1}) = Pr(X_{t+1}|Z_{t+1}=i) Pr(Z_{t+1}=i, X_{1:t})\n\\] \\(\\to\\) element-wise multiplication of a row from emission matrix with\n\n\n\n3. Termination (\\(T=T\\)): Marginalization\n\\[\nPr(X) = Pr(X_{1:T}) = \\sum_i Pr(X_{1:T}, Z_T=i) = \\sum_i \\alpha_{Ti}\n\\]"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#graphical-summary-2-steps",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#graphical-summary-2-steps",
    "title": "001 - Forward Algorithm",
    "section": "Graphical Summary: 2 Steps",
    "text": "Graphical Summary: 2 Steps\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Forward.jpg”, width=“800”&gt;\n\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Forward_summary.jpg”, width=“800”&gt;\n\n\nThe Markov Model pushes the state \\(Z\\) forward in time \\(\\longrightarrow\\) matrix multiplication with \\(P\\)\nEmission probabilities: take into account the state-specifc (time-dependent) probabilities for observation \\(X_{t+1}\\) \\(\\longrightarrow\\) element-wise multiplication with proper column of \\(E\\)"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#notice-1",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#notice-1",
    "title": "001 - Forward Algorithm",
    "section": "Notice",
    "text": "Notice\n\nMarginalization: \\(Pr(X_{1:t}) = \\sum_i Pr(X_{1:t}, Z_t = i) = \\sum_i \\alpha_{ti} \\ne 1\\). In fact, it is much smaller than 1 for large \\(t\\) !\nRecursion Efficiency: Forward Propagation of \\(\\alpha_{tk}\\) is fast (linear in sequence length \\(T\\))\n\nCalculation of \\(Pr(X)\\) requires \\(T N^2\\) calculations \\(\\ll N^T\\)\nExample: \\((N,T) = (2, 100) \\longrightarrow 400 \\ll 2^{100}\\)\n\n\nEmission matrix \\(E_{ik}\\) serves as lookup table for given observation \\(X_t=k\\) at time \\(t\\). (\\(k=f(t)\\))"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#group-task-30-min-a-single-step-forward-in-time",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#group-task-30-min-a-single-step-forward-in-time",
    "title": "001 - Forward Algorithm",
    "section": "Group Task (30 min): A single step forward in time",
    "text": "Group Task (30 min): A single step forward in time\n\\[Pr(Z_{t-1}=i, X_{1:t-1}) \\to Pr(Z_t=i, X_{1:t})\\]\nGiven the above HMM with 2 states (Germany=0, Switzerland=1) and a magically known joint probability \\(Pr(Z_{t-1}, X_{1:t-1})=(0.05, 0.02)\\). I made those numbers up, and it is irrelevant which history of observations resulted in these probabilities. They denotes the probability for the two states and all observations until time \\(t-1\\). Notice that this does not have to sum to 1! But (thanks to Markov) this is all you need to calculate the next step.\nCalculate the updated probability for \\(Z_t=\\) Germany (0) and that the new observation \\(X_t\\) is Bread (0), Fish (1) or Fondue (2).\n\nGroup 1: $P(Z_t=0, X_{1:t-1}, X_t=0) = $ ?\nGroup 2: $P(Z_t=0, X_{1:t-1}, X_t=1) = $ ?\nGroup 3: $P(Z_t=0, X_{1:t-1}, X_t=2) = $ ?\n\n\n\nCode\nimport numpy as np\npi=np.array( [0.75, 0.25] )                          # initial state probability\nP =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\nE =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilities\n\nalpha = np.array([0.05, 0.02])  # initial probability at time t-1  (prior)\nxt = 0                          # observation of interest at time t. (Bread = 0)\n\nalpha = alpha.dot(P)            # push prior with P from t-1 --&gt; t (state transition)\nprint('after state transition: ', alpha) \n\nLH=E[:,xt]                      # pick emission probs for observation xt\nprint('emission vector:        ', LH)\n\nalpha = LH * alpha              # elementwise multiplication pf alpha with LH\nprint('new probability         ', alpha)   # \n\n# only for calculation of conditional probability\n#alpha /= np.sum(alpha)          # normalized posterior\n#print('posterior norm:', alpha)\n\n\nafter state transition:  [0.042 0.028]\nemission vector:         [0.7 0.1]\nnew probability          [0.0294 0.0028]\nposterior norm: [0.91304348 0.08695652]"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#remarks",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_001_Forward.html#remarks",
    "title": "001 - Forward Algorithm",
    "section": "Remarks",
    "text": "Remarks\n\nMonitoring: With the same recipe we can progagate the conditional probability distribution \\[Pr(Z_{t-1}=i| X_{1:t-1}) \\to Pr(Z_t=i | X_{1:t})\\] through time - rather than the joint \\(Pr(Z_t=i, X_{1:t})\\).\nWe simply have to normalize after every time step. \\[Pr(Z_t=i|X_{1:t}) = Pr(Z_t=i,X_{1:t})~/~Pr(X_{1:t})\\]"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino_Solution.html",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_003_Casino_Solution.html",
    "title": "Set up Software",
    "section": "",
    "text": "Notice (26.10.2022):\nGoogle Colab now has hmmlearn 0.2.8\nReplaced: hmm.MultinomialHMM \\(\\to\\) hmm.CategoricalHMM\n\n\nCode\n!pip install hmmlearn\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom hmmlearn import hmm\n\n# some convenience functions\ndef HMM_inspect(Z,X,t=50):\n  z_str = ''.join(str(z) for z in Z.flatten()[0:t])\n  x_str = ''.join(str(x) for x in X.flatten()[0:t])\n  print('Z: ',z_str)\n  print('X: ',x_str)\n\n  bs=list(np.arange(0,6,0.5))\n  plt.hist(X[Z == 0], label='Z=0', alpha=0.5, bins=bs)\n  plt.hist(X[Z == 1], label='Z=1', alpha=0.5, bins=bs)\n  plt.xlabel('observation X')\n  plt.legend()\n  plt.show()\n\n# plot matrix (with title and numbers)\ndef plotMatrix(ax, mat, title, cm, normalizer):\n  ax.imshow(mat, cmap=cm, norm=normalizer)\n  for (j,i),label in np.ndenumerate(mat):\n    ax.text(i,j,np.round(label,2),ha='center',va='center')\n    ax.set_title(title)\n\n# compare two models\ndef compareHMM(model, model_fit):\n  ## Visualization ###\n  my_cm=plt.cm.Blues        # set color-map\n  normalizer=Normalize(0,1) # set common color code for trans and emission probs \n\n  fig, ax = plt.subplots(3, 2, \n      gridspec_kw={'width_ratios': [1, 3], 'height_ratios': [1, 1, 0.1]},\n      figsize=(10,7))\n\n  plotMatrix(ax[0,0], model.transmat_, 'trans - orig', my_cm, normalizer)\n  plotMatrix(ax[0,1], model.emissionprob_, 'emissions - orig', my_cm, normalizer)\n  plotMatrix(ax[1,0], model_fit.transmat_, 'trans - fit', my_cm, normalizer)\n  plotMatrix(ax[1,1], model_fit.emissionprob_, 'emissions - fit', my_cm, normalizer)\n\n  # add colorbar with common color scale (set by im)\n  im = plt.cm.ScalarMappable( cmap=my_cm, norm=normalizer) \n  fig.colorbar(im, cax=ax[2,0], orientation='horizontal')\n  fig.colorbar(im, cax=ax[2,1], orientation='horizontal')\n  plt.show()\n\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: hmmlearn in /usr/local/lib/python3.7/dist-packages (0.2.8)\nRequirement already satisfied: numpy&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.21.6)\nRequirement already satisfied: scipy&gt;=0.19 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.7.3)\nRequirement already satisfied: scikit-learn&gt;=0.16 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.0.2)\nRequirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.16-&gt;hmmlearn) (1.1.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.16-&gt;hmmlearn) (3.1.0)\n\n\n\nThe Ingredients\n\nStates \\(Z\\): { 0 = fair, 1 = biased };\nObservations \\(X\\): { 0, 1, 2, 3, 4, 5 }\nParameters \\(\\Theta\\):\n\n\\[\\begin{align}\n    P(Z_0) &= \\begin{bmatrix} 0.5 & 0.5  \\end{bmatrix} \\\\ \\\\\n    P(Z_t | Z_{t-1}) & = \\begin{bmatrix} 0.95 & 0.05 \\\\ 0.25 & 0.75 \\end{bmatrix} \\\\ \\\\\n    P(X_t | Z_t) & =  \\begin{bmatrix} 1/6 & 1/6 &  1/6 & 1/6 & 1/6 & 1/6 \\\\ 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.5 \\end{bmatrix} \\\\\n\\end{align}\\]\n\n\nGenerating states and observations\n\n\nCode\npf = [1./6 ] *6           # fair emission probabilities\npb = [1./10] *5 + [1/2]   # biased emission probabilities\n\nnp.random.seed(42)\nmodel_gen = hmm.CategoricalHMM(n_components=2)\nmodel_gen.startprob_ = np.array([0.5, 0.5])                     # initial state prob\nmodel_gen.transmat_  = np.array([[0.95, 0.05], [0.25, 0.75]])  # transition prob\nmodel_gen.emissionprob_ =  np.array([pf, pb])                  # emission prob\n\nX,Z = model_gen.sample(5000)\n\n\n\n\nFit the data: several start points\n\n\nCode\nnp.random.seed(42)\nbest_model, best_score = None, None\n\nfor i in range(10):\n  model_fit = hmm.CategoricalHMM( \n      n_components=2,     # number of states\n      n_iter=1000,        # number of iterations per initialization\n      init_params='ste'   # which parameters to initialize automatically \n      )\n  \n  # manual initializations (optional)\n  #model_fit.transmat_ = np.array([ [0.9, 0.1], [0.1, 0.9]])\n\n  model_fit.fit(X)\n  score = model_fit.score(X)\n\n  if (best_model is None or score &gt; best_score):\n    best_score = score\n    best_model = model_fit\n    print('new best: ', i, best_score)\n\ncompareHMM(model_gen, best_model)\n\n\nnew best:  0 -8907.84168625088\nnew best:  2 -8894.97766303\nnew best:  3 -8894.449120233869\n\n\n\n\n\n\n\nCode\n_, Zg = model_gen.decode(X)\n_, Zb = best_model.decode(X)\n\nprint('differences (Zg-Z):  ', np.sum(Zg != Z))\nprint('differences (Zb-Z):  ', np.sum(Zb != Z))\n\n\ndifferences (Zg-Z):   795\ndifferences (Zb-Z):   763\n\n\nMessages:\n\nLearning = Fitting is computationally the most demanding step\nThe Baum-Welch Algorithm only delivers local maxima - run it several times to explore other initial conditions\nhyperparameters: number of iterations, initial estimates\nThe number of components needs to be chosen wisely (limits of unsupervised learning)\nMore data helps, but fitting will take longer\nThe state labels may swap\nAlthough state transitions and emission probabilities may be predicted quite accurately, predicting the correct state path is much harder."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html",
    "title": "000 - HMM Introduction",
    "section": "",
    "text": "Even if the “states of the world” are Markovian, they are often hidden from us, and we only observe some measurements.\nA traveling analogy\n\nI frequently commute between two states: Germany and Switzerland. Let’s assume my travels can be modelled as a Markov Process, as described in the previous section. But now I only communicate my dinner plans with the world. Therefore dinner is an observable variable, but my current state (the country) variable is hidden. We might hope that something could still be learned about the states visited from the observation on food consumption.\n\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/HMM_CountryFood.jpg”, width=“1000”&gt;\n\nThis is a Hidden Markov Model (HMM). An HMM is characterized by three ingredients:\n\ninitial distribution: \\(P(Z_0=i)=\\pi_i\\) ( \\(\\to 1 x N\\) matrix = row vector )\ntransition matrix: \\(P(Z_t=j|Z_{t-1}=i) = P_{ij}\\) ( \\(\\to N \\times N\\) matrix )\nemission matrix: \\(P(X_t=k|Z_t=i) = E_{ik}\\) ( $ N M$ matrix )\n\nThe emission probabilities are dependent on the state, but constant over time.\nFor simplicity we will assume that both states and observables are discrete. To be specific, the Hidden Markov Model with 2 states \\(Z \\in\\) {Germany=0, Switzerland=1} and observations with 3 possible observations \\(X \\in\\) {Bread=0, Fish=1, Fondue=2} may read:\n\\[\n\\begin{align}\n    P(Z_0) &= \\begin{bmatrix} 0.75 & 0.25  \\end{bmatrix} \\\\ \\\\\n    P(Z_t | Z_{t-1}) & = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.1 & 0.9 \\end{bmatrix} \\\\ \\\\\n    P(X_t | Z_t) & =  \\begin{bmatrix} 0.7 & 0.2 &  0.1 \\\\ 0.1 & 0.1 & 0.8 \\end{bmatrix} \\\\\n\\end{align}\n\\]\nDiscussion: Give an interpretation of the numbers as they relate to the graph above.\nNotice: all rows are non-negative and they sum to 1 (stochastic matrices)\nThe cell below specifies all these parameters in Python/Numpy.\n\n\nCode\nimport numpy as np\npi=np.array( [0.75, 0.25] )                          # initial state probability\nP =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\nE =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilities\n\n\nGroup Task (30 min): Discuss and simulate the above Hidden Markov Model.\nComplete the following function and generate observations from a Hidden Markov Model defined above. You might want to refer back to the first lecture on simple Markov Models\n\n\nCode\n# notice the similarities with generate_sequence() from the plain Markov Model\ndef generate_HMM(P, pi, E, T=50):\n  assert P.shape[0]==P.shape[1],         \"generate_HMM: P should be a squared matrix\"\n  assert E.shape[0]==P.shape[0],         \"generate_HMM: E and P should have the same number of rows (states)\"\n  assert np.allclose( P.sum(axis=1), 1), \"generate_HMM: P should be a stochastic matrix\"\n  assert np.allclose( E.sum(axis=1), 1), \"generate_HMM: E should be a stochastic matrix\"\n  assert np.isclose( pi.sum(), 1),       \"generate_HMM: pi should sum to 1\"\n  \n  \n  # first define two list (states = integers, emissions = letters)\n  ns = ...                          # number of states\n  ne = ...                          # number of outputs (#observables)\n  states= list(range(ns))           # state labels as integers\n  emissions=list(range(ne))         # observation labels as integers\n\n  # chose first state and emission\n  z = np.random.choice( states,    p = ... )\n  x = np.random.choice( emissions, p = ... )\n\n  # add state and observation to history\n  state_hist = [z]\n  emit_hist = [x]\n  \n  # loop for T time steps\n  for t in range(T):\n    z = np.random.choice( states,    p = ... )\n    x = np.random.choice( emissions, p = ... )\n\n    # collect history with state and emission labels\n    state_hist.append(z)\n    emit_hist.append(x)\n  return state_hist, emit_hist\n\n\nTest: If done correctly, the function should return output such as\n\n\nCode\n%%script echo ensure that function generate_HMM() is defined\nnp.random.seed(42)\nZ, X = generate_HMM(P,pi,E, T=50)\nprint('states Z       =',*Z)\nprint('observations X =',*X)\n\n\nstates Z       = 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0\nobservations X = 2 0 0 1 1 2 2 1 2 2 1 2 2 2 0 1 2 2 0 2 2 2 0 0 0 2 2 2 0 0 0 2 2 2 2 0 1 1 0 0 2 0 2 2 2 2 2 2 2 0 0\n\n\nDiscussion: Do these sequences make sense? Can you give an interpretation of the observation?"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#joint-co",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#joint-co",
    "title": "000 - HMM Introduction",
    "section": "Joint & Co",
    "text": "Joint & Co\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/JointConditionalMarginal.jpg”, width=“1000”&gt;\n\nKnowing the joint distribution \\(P(X,Z)\\) is the best we can hope for, since everything else can (in principle) be calculated from it.\nHowever:\n\nremember that we are about to hide all state variables \\(Z\\).\ncalculations maybe very hard - analytically and computationally. For example, even if we knew a joint distribtuions such as\n\n\\[\nP(X_1, X_2, \\ldots, X_T, Z_1, \\ldots Z_T)\n\\]\nmany computational task would become very difficult (combinatorics!) - unless the problem has some structure (such as a Markov Property).\nIn many situations we may be more interested in specific subsets of variables:\n\nconditional distributions: some variables are known or fixed,\nmarginal distributions some variables are uninteresting \\(\\longrightarrow\\) average over."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#bayes-theorem",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#bayes-theorem",
    "title": "000 - HMM Introduction",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/BayesEquation.jpg”, width=“1000”&gt;"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#example-diagnostic-tests",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#example-diagnostic-tests",
    "title": "000 - HMM Introduction",
    "section": "Example: Diagnostic Tests",
    "text": "Example: Diagnostic Tests\n\n&lt;img src=“https://github.com/thomasmanke/ABS/raw/main/figures/DiagnosticTest.jpg”, width=“1000”&gt;"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#group-tasks-20-min-bayesian-reasoning",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#group-tasks-20-min-bayesian-reasoning",
    "title": "000 - HMM Introduction",
    "section": "Group Tasks (20 min): Bayesian Reasoning",
    "text": "Group Tasks (20 min): Bayesian Reasoning\nFor the following assume that all HMM parameters are known: \\(\\pi, P, E\\).\n\nIs the initial distribution the same as the stationary distribution?\nLet’s assume that I sent you my (first ever) message, saying that I just had Fondue for dinner. What is the (posterior) probability that I am in Germany?\n\n\n5.8%\n50.0%\n75.0%\n\n\n\nCode\n%%script echo edit before execution\n# 1. matrix powers\nfrom numpy.linalg import matrix_power\npi = np.array([1.0, 0.0])         \nstat_dist = ...   # independent of pi\nprint('stat_dist = ', stat_dist)\n\n# 2.  Bayesian analysis\nsum  = ...\nprob = ... / sum\nprint('sum = ', sum)\nprint('answer = ', prob)\n\n\nLater you will learn how to incorporate all observations \\(X\\) systematically to derive probabilitic statements for \\(Z\\)."
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#discussion-10-min",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#discussion-10-min",
    "title": "000 - HMM Introduction",
    "section": "Discussion (10 min):",
    "text": "Discussion (10 min):\n\nWhy would we want to know the joint distribution \\(P(X,Z)\\) ? Why will it be difficult ?\nHow many possible sequences are there for a) observations \\(X\\) and b) hidden states \\(Z\\)?\nWhat happens to \\(Pr(X,Z)\\) if the sequence \\(Z\\) contains forbidden transitions?"
  },
  {
    "objectID": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#group-task-20-min",
    "href": "lectures/ABS2022/HiddenMarkovModels/HMM_000_Intro.html#group-task-20-min",
    "title": "000 - HMM Introduction",
    "section": "Group Task (20 min)",
    "text": "Group Task (20 min)\nGiven the HMM parameters and an observed sequence \\(X=(0,0,2)\\). You have not observed the corresponding sequence \\(Z=(z_1, z_2, z_3)\\), but given the HMM parameters you can calculate the probability of all possible hidden state paths \\(Z\\) - Group 1: all paths starting with \\(G\\) - Group 2: all paths starting with \\(S\\)\nReport back the path with the highest probability and compare. What happens if you sum all collected probabilities?\n\n\nCode\n%%script echo edit before execution\nprint('0 0 0: ', pi[0]*E[...]]*P[...]*E[...]*...)\n...\n\n# alternatively write a loop over Z\nX = [0,0,2]\n\n\n\nedit before execution"
  },
  {
    "objectID": "lectures/ABS2022/index.html",
    "href": "lectures/ABS2022/index.html",
    "title": "Applied Biostatistics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n00: Jupyter Notebooks Basics\n\n\n\nJupyter\n\n\nNotebooks\n\n\n\n\n\n\n\nThomas Manke\n\n\nSep 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00: Markov Chain Introduction\n\n\n\nMarkov Chains\n\n\nMC Intro\n\n\nSampling\n\n\n\n\n\n\n\nThomas Manke\n\n\nSep 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chain Sampling\n\n\n\nMarkov Chains\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysing Markov chain properties\n\n\n\nMarkov Chains\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chain Applications\n\n\n\nMarkov Chains\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chain Monte Carlo\n\n\n\nMarkov Chains\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n000 - HMM Introduction\n\n\n\nHidden Markov Models\n\n\n\n\n\n\n\nThomas Manke\n\n\nSep 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n001 - Forward Algorithm\n\n\n\nHidden Markov Models\n\n\n\n\n\n\n\nThomas Manke\n\n\nSep 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n002a - HMMLearn\n\n\n\nHidden Markov Models\n\n\n\n\n\n\n\nThomas Manke\n\n\nSep 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n002 - More Algorithms (Viterbi and Baum-Welch)\n\n\n\nHidden Markov Models\n\n\n\n\n\n\n\nThomas Manke\n\n\nSep 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n003 - An Application (Dishonest Casino)\n\n\n\nHidden Markov Models\n\n\n\n\n\n\n\nThomas Manke\n\n\nSep 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet up Software\n\n\n\nHidden Markov Models\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n005 - Project (Learning an HMM)\n\n\n\nHidden Markov Models\n\n\n\n\n\n\n\nThomas Manke\n\n\nSep 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n001 - Introduction to Artificial Neural Networks\n\n\n\nArtificial Neural Networks\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n002 - Logistic Regression\n\n\n\nArtificial Neural Networks\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore difficult problems\n\n\n\nArtificial Neural Networks\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification\n\n\n\nArtificial Neural Networks\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet Libraries\n\n\n\nArtificial Neural Networks\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequence Analysis with Neural Networks\n\n\n\nArtificial Neural Networks\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n007 - Outlook\n\n\n\nArtificial Neural Networks\n\n\n\n\n\n\n\nThomas Manke\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/Rintro2023/03_GettingData.html",
    "href": "lectures/Rintro2023/03_GettingData.html",
    "title": "03: Getting Data In and Out",
    "section": "",
    "text": "Goal: Ultimately we want to access our own data and write results to file."
  },
  {
    "objectID": "lectures/Rintro2023/03_GettingData.html#lecture-material",
    "href": "lectures/Rintro2023/03_GettingData.html#lecture-material",
    "title": "03: Getting Data In and Out",
    "section": "Lecture material",
    "text": "Lecture material\nThis lecture series is written in R Markdown and is publically available as github repository.\n\n\n\nGithub link. https://github.com/maxplanck-ie/Rintro.git –&gt; branch: 2023.03 !!\n\n\n\nBe aware that this course has seen various iterations - selecting the correct branch is crucial to avoid mix ups\n\nYou can access this material in various different ways:\n\ndownload as zip archive and unpack\nRstudio/Git: open a new Rproject (“New Project &gt; Version Control &gt; Git &gt; ) and provide the same link as Repository URL. Find the git tab and make sure to select the relevant branch”2023.03”\n\nNotice 1:\nThe rmd files are provided for your convenience. This should save some typing (and common errors).\nHowever, this is an interactive course, so please use the code, understand it, change it, and break it !\nNotice 2:\nWe may update this material during the course ! If you want to retain and edit your own material make sure to save it somewhere differently.\nIf you use git you might want to checkout a different local branch and pull only to the original branch if an update is announced."
  },
  {
    "objectID": "lectures/Rintro2023/03_GettingData.html#csv-files",
    "href": "lectures/Rintro2023/03_GettingData.html#csv-files",
    "title": "03: Getting Data In and Out",
    "section": "CSV files",
    "text": "CSV files\nComma-separated text files (ASCII) are both human and machine readible. Other separators may be chosen (tab or “|”). This format is frequently used for simple data, such as rows of different samples/observations and columns of multiple variables (per sample)\nImportant: Make sure that you know the precise location of your data file and provide this as filename.\nTopics:\n\nhome and working directory\nrelative and absolute path\n\n\n\nCode\ngetwd()                     # working directory\ndir()                       # display content\nfilename='data/iris.csv' # relative to wd\nd = read.csv(filename)      # file content --&gt; memory (d)\nstr(d)\n\n\nThere are many different ways to load such data into memory and to customize the loading.\nTasks: - Explore ?read.csv to get a first overview how this function can be customized. - How would you read only the first 10 lines? - Explore the data object d - Optional bonus: try your own file and brace! Is it clean enough?"
  },
  {
    "objectID": "lectures/Rintro2023/03_GettingData.html#from-url",
    "href": "lectures/Rintro2023/03_GettingData.html#from-url",
    "title": "03: Getting Data In and Out",
    "section": "From URL",
    "text": "From URL\nNotice that files do not need to be available locally, but might be provided by some URL.\nBe aware that in those cases there might be significant reduced loading speed, depending on your network connections.\n\n\nCode\nfilename='https://raw.githubusercontent.com/maxplanck-ie/Rintro/2023.03/data/iris.tsv'\nd = read.csv(filename, sep='\\t')  \nstr(d)"
  },
  {
    "objectID": "lectures/Rintro2023/03_GettingData.html#compressed-formats",
    "href": "lectures/Rintro2023/03_GettingData.html#compressed-formats",
    "title": "03: Getting Data In and Out",
    "section": "Compressed formats",
    "text": "Compressed formats\nEspecially for big data it is common to store them in compressed format (e.g. *gz) to reduced the storage footprint and speed-up data transfer. Such files are not human readable (binary) can also be read\n\n\nCode\ncmd = \"gunzip -c data/iris.tsv.gz\"   # command to uncompress\nd = read.csv(pipe(cmd), sep='\\t')       # read as pipe\nstr(d)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr  \"setosa\" \"setosa\" \"setosa\" \"setosa\" ..."
  },
  {
    "objectID": "lectures/Rintro2023/03_GettingData.html#writing-data",
    "href": "lectures/Rintro2023/03_GettingData.html#writing-data",
    "title": "03: Getting Data In and Out",
    "section": "Writing data",
    "text": "Writing data\nThere are many ways to save data to text files. One of the simplest uses write.csv.\n\n\nCode\nwrite.csv(iris, file=\"iris.csv\", row.names=FALSE, quote=FALSE)\n\n\nFor large data you may prefer to write compressed version:\n\n\nCode\nwrite.csv(iris, gzfile(\"output/iris.csv.gz\"))\n\n\nTask:\n\nChange some of the parameters (row.names, quote) and observed their effect on the resulting file\nSave only the subset of flowers where Species=“setosa” to a file setosa.tsv"
  },
  {
    "objectID": "lectures/Rintro2023/03_GettingData.html#rdata",
    "href": "lectures/Rintro2023/03_GettingData.html#rdata",
    "title": "03: Getting Data In and Out",
    "section": "RData",
    "text": "RData\nIn the context of the R-programming language, RData is a very convenient (binary) format that can be used to save multiple data structures or even whole environments It’s very efficient when you exchange your data with other R-users (or your future self)\n\nSpecific objects\n\n\nCode\nd = iris              # copy of iris data\nfn=\"output/iris.RData\"       # filename (and extension) of choice\nsave(d, file=fn)\nrm(d)                 # remove object d for illustration - and watch global env\nload(fn)              # reload object d from file - and watch global env\n\n\n\n\nTask: All objects\nSometimes we want to save all objects and variable that have accumulated in the “Global Environment” - just to be sure. This task test some jargon, familiarity with directory structure and ability to find help. Please try it yourself.\n\nCreate a new data object for the iris data set as before and additional variables for your favorite numbers and perhaps some favorite strings.\nSave the whole environment (using save.image())\nDelete the whole environment aka “workspace”; e.g. using rm(list=ls())\nReload the environment and confirm that you successfully recreated all objects\nDetermine your current working directory (&gt;getwd())\nLocate saved image on disk and inspect its size. Delete it if you prefer.\n\n\n\nCode\n# your code snippet here\n\n\nNotice: The suffix is not strictly necessary, but it is best practice and used consistently by the community."
  },
  {
    "objectID": "lectures/Rintro2023/00_Introduction.html",
    "href": "lectures/Rintro2023/00_Introduction.html",
    "title": "00: Introduction",
    "section": "",
    "text": "Overview of R-course\n\nBlock 1: Groundwork\n\nStarting RStudio: a GUI for R\nSimple functions (input, output, parameters & help)\nData: I/O, Types, Formats & Structures\n\n\n\nBlock 2: Speaking about Data\n\nData Descriptions\nData Visualization: simple plots and advanced heatmaps\nData Models: linear models\n\n\n\nBlock 3: Open the universe\n\ninstalling and using packages\nmanipulating data: cleaning, filtering, merging\n\n\n\nBlock 4: Advanced topics and case studies\n\nR scripts, R markdown & reproducible analyses\ndata exploration\n\n\n\n\n\nGoals:\nAfter the course you will be able to\n\nunderstand the basic syntax of R\nfind help for more nifty problems\ninstall and use packages for your specific questions\nsee the limitations of EXCEL\nunderstand the importance of structured data\nunderstand some bioinformatics jargon\nunderstand common difficulties in data analysis\nincrease the value of your CV\nproduce figures such as\n\n\n\n\nTypical examples from R.\n\n\n\n\n\nNon-Goals:\nThis course does not teach\n\nprogramming new tools: focus on existing tools\nbioinformatics: BSc course (3 yrs ++)\nNGS analysis: other programs (Linux, Galaxy, deepTools)\n\n\n\n\nWhy R ?\n\nStandard software for data analysis, statistics, and visualization\nfree and community support\nmany packages available (for data I/O, manipulation, high-end: expression analysis)\ngood compromise between flexibility (programming) and box-solutions (packages)\nsuitable for very large dataset (filtering, merging)\ninteractive and scripting\nnot the best solution for everything (e.g. mapping)\nfrequent requirement during (bioinformatics) hiring\n\n\n\n\nEmbrace the learning curve\n\nsteep (but rewarding)\nFirst hurdles:\n\nit’s a new language: vocabulary & grammar\nchoices: many ways to say (to program) the same things\nmore choices: many (redundant) packages\ncomputers are stubborn and stupid: they do not think along\n\nMore hurdles\n\ncase sensitivity: \\(a \\ne A\\)\nparameter sensitivity\nsofware dependencies\n\n\n\n\n\nStart Rstudio"
  },
  {
    "objectID": "lectures/Rintro2023/01_FirstSteps.html",
    "href": "lectures/Rintro2023/01_FirstSteps.html",
    "title": "01: First Steps",
    "section": "",
    "text": "Create R project\nBest practice:\nEach R project should have their dedicated directory. This directory will hold code, data, results etc.\nTask: Open a new project with “File &gt; New Project …”\nNotice the following distinction:\n\nWhen running Rstudio locally, the project directory will reside on your local computer.\nWhen accessing a web server, the project directory will also be on that server.\n\nTask: Familiarize yourself with the layout and the various panels in Rstudio.\nQuery: Utilization: https://pollev.com/thomasmanke101\n\n\nConsole and Commands\nFor now, the most important panel is the so-called “Console” with the prompt (“&gt;”).\nThis is where the first interactions with the R-software will take place.\nThe other panels are for: output, help and other information.\nTask: In the Console try to repeat and understand the following operations\n\n\nCode\n2+2\n\n\n[1] 4\n\n\nCode\n2^3        # This is a comment: try also 2**3\n\n\n[1] 8\n\n\nCode\n1+2+3/6    # beware of precedence rules\n\n\n[1] 3.5\n\n\nCode\nsqrt(2)    # functions\n\n\n[1] 1.414214\n\n\nCode\nsin(pi/2)  # functions and built-in constants\n\n\n[1] 1\n\n\nCode\n# Special values\n1/0        # R knows infinity! Try 1/Inf and 1/inf\n\n\n[1] Inf\n\n\nCode\n0/0        # Not a number!     Try sqrt(-1)\n\n\n[1] NaN\n\n\n\n\nCommand History\nAll R-commands are tracked in a history. It maybe accessed using the cursors (or the history panel in RStudio). This is extremely useful to navigate to previous commands, repeat them, or change them slightly.\nTask: Using the history, determine \\(\\sqrt 3\\).\n\n\nFirst help\nThere are various different ways to get help\n\n\nCode\n?sin\n\n\nUse help to find out more about: sqrt, exp or anything else of interest.\n\n\nCommunity help\n\nonline courses: http://software-carpentry.org/lessons, Coursera, Udacity, …\nbootcamp: https://github.com/jknowles/r_tutorial_ed\nQuickR: http://www.statmethods.net\nhttp://stackoverflow.com, http://www.r-bloggers.com/\n\n\n\nSessionInfo\nEvery language operates in a certain context and is context-dependent.\nSoftware depends on other software and packages.\nIt is important to be aware of those (often complex) dependencies.\nOne way to communicate this context is with the output from sessionInfo().\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.3 (2023-03-15)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/manke/miniconda3/envs/web/lib/libopenblasp-r0.3.21.dylib\n\nlocale:\n[1] C/UTF-8/C/C/C/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.2.3    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.2.3       htmltools_0.5.5   yaml_2.3.7        rmarkdown_2.14   \n [9] knitr_1.42        jsonlite_1.8.4    xfun_0.38         digest_0.6.31    \n[13] rlang_1.1.0       evaluate_0.20    \n\n\n\n\n\nAssigning values to objects\nGoal: store results of computation in new variables\n\n\nCode\nx &lt;- 2*pi # pre-defined constant pi\nx         # see value of object x = show(x) \n\n\n[1] 6.283185\n\n\nCode\nx &lt; -2    # Careful: what's going on here?\n\n\n[1] FALSE\n\n\nCode\nx\n\n\n[1] 6.283185\n\n\nCode\nx = 2  # \"=\" as in most other languages.  \nx      # x was overwritten\n\n\n[1] 2\n\n\nCode\nx==3   # another logical comparison (x unchanged)\n\n\n[1] FALSE\n\n\nCode\nx+x    # passing objects to functions (+)\n\n\n[1] 4\n\n\nTask: look at the following objects and understand the differences\n\n\nCode\npi \nhi\n\"hi\"\n\n\n\n\n\nSimple Vectors\nVectors illustrate how complex data structures can be built from smaller blocks. Here we learn how to create and inspect vectors.\n\n\nCode\nv=c(1,2)            # combine arguments into vector\nv                   # display v\n\n\n[1] 1 2\n\n\nCode\n# functions for vectors\nstr(v)              # structure of v (especially useful for long and structured objects)\n\n\n num [1:2] 1 2\n\n\nCode\ntypeof(v)           # type of v ( ~ storage mode)\n\n\n[1] \"double\"\n\n\nCode\nclass(v)            # class of v (determines how certain functions will work with v)\n\n\n[1] \"numeric\"\n\n\nCode\nlength(v)\n\n\n[1] 2\n\n\nCode\nsum(v)\n\n\n[1] 3\n\n\nCode\nsummary(v)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.25    1.50    1.50    1.75    2.00 \n\n\n\n\nAccessing Vectors\nFrequently we need to access specific elements from a vector\n\n\nCode\nv[1]\n\n\n[1] 1\n\n\nTask: access the following vector elements: v[1], v[2], v[3], v[1:3], v[-1]\n\n\nVector Generation\n\n\nCode\nv=c(1,2,3,4,5)    # combining many elements can get cumbersome\nv=1:5             # more efficient\n\nv=seq(from=1, to=5, by=1) # same as before but more flexibilty\nv=seq(1,5,0.5)            # short-hand defaults\n\n\nTasks:\n\nExplore the seq() function and inspect other parameter options.\nCreate a vector with 5 numbers equally spaced from +2 to -1.9\n\n\n\n[1]  2.000  1.025  0.050 -0.925 -1.900\n\n\nTask: Understand the difference between the two vectors v1 and v2\n\n\nCode\nv1=1:10-1\nv2=1:(10-1)\n\n\n\n\nVector Operations\n\n\nCode\nv1=1:3      # = c(1,2,3)\nv2=rep(2,3) # = c(2,2,2)\n\nv1+v2      # elementwise addition\nv1*v2      # ... multiplication\nv1 &gt; v2    # ... comparisons\nv1 %*% v2  # scalar product\n\n\nTask: Define your own vectors and explore some numerical operations\n\n\nMisc: Concepts and Pecularities\n\n\nCode\nv=1:4                    # Vector Definition\n\nv + 10                   # Recycling\nv + c(10,20)             # Recycling\nv + c(10,20,30)          # Warning != Error\n\n#v %*% c(10,20,30)        # Error = Error\n\nletters                  # built-in vector. Try LETTERS \ntypeof(letters)          # there is more than numbers\n\nnames(v)=letters[1:4]    # naming of vector elements (for convenience)\nv[\"b\"]                   # index by name\n\nrev(v)                   # reverse vectors\n\n\n\n\n\nLearning Curve:\n\nRstudio: Starting an R project\nwork with console\nsimple functions (input, output), logical operations and parameters\ncreating simple objects and assigning values\ninspecting objects: v, str(v), typeof(v), length(v)\nflexible creation of vectors\nvector indices and subsetting\nvector operations and recycling\nspecial values: Inf, NaN, NA\nfunctions can behave differently on different input: summary()\nsee warning messages and errors\ngetting help: ?sqrt\nsoftware dependencies and the importance of sesssionInfo()"
  },
  {
    "objectID": "lectures/Rintro2023/index.html",
    "href": "lectures/Rintro2023/index.html",
    "title": "Rintro",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n00: Introduction\n\n\n\nRintro2023\n\n\nIntroduction\n\n\nR\n\n\n\n\n\n\n\nThomas Manke\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01: First Steps\n\n\n\nRintro2023\n\n\nR\n\n\nvectors\n\n\nsessionInfo\n\n\nhelp\n\n\n\n\n\n\n\nThomas Manke\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n02: Higher Dimensional Data\n\n\n\nRintro2023\n\n\nR\n\n\niris\n\n\n\n\n\n\n\nThomas Manke\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n03: Getting Data In and Out\n\n\n\nRintro2023\n\n\nI/O\n\n\nR scripts\n\n\n\n\n\n\n\nThomas Manke\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n04: Data Descriptions and Visualizations\n\n\n\nRintro2023\n\n\nDescriptive Staistics\n\n\nFactors\n\n\nCorrelations\n\n\n\n\n\n\n\nThomas Manke\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n05: Data Modeling\n\n\n\nRintro2023\n\n\nlinear model\n\n\nclass\n\n\nanova\n\n\nfactors\n\n\n\n\n\n\n\nThomas Manke\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n06: Data Visualization\n\n\n\nRintro2023\n\n\nScaling\n\n\npheatmap\n\n\nPCA\n\n\nsave figures\n\n\n\n\n\n\n\nThomas Manke\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]