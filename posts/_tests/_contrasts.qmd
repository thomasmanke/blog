---
title: "Contrasts"
date: 2024/03/27
date-modified: last-modified
categories: 
  - iris
  - contrasts

#image: gapminder.png

format:
  html: default
---

```{r}
#| label: setup
library(tidyverse)
```

# Anova
```{r}
#| label: overview
plot(Petal.Width ~ Species, data=iris, color=Species)
summary(aov(Petal.Width ~ Species, data=iris))
```

# Linear Model

$$
\begin{array}{ll}
y &= X \cdot \beta + \epsilon \\
(n \times 1) &= (n \times p)~(p \times 1) + (n \times 1)
\end{array}
$$

- $y_i \in R$: dependent variable (response) $n \times 1$ vector
- $X_{ij} \in R$: **design matrix** ($p$ independent variables) $n \times p$ matrix
- $\beta \in R$: model parameters $p \times 1$ vector
- $\epsilon_i \propto N(0, \sigma)$ iid noise

## Example: Numerical Variables
For a linear regression with two independent numerical variables
$$ 
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}
$$

the design matrix is:
$$
M = 
\left(
\begin{array}{ccc}
  1 & x_{11} & x_{12} \\
  \vdots & \vdots & \vdots \\
  1 & x_{n1} & x_{n2} \\
\end{array}
\right)
$$

Notice 1: there are $p=2+1=3$ parameters including the intersect.
Therefore the design matrix has 3 columns where the first column $x_{i0}=1 ~~~\forall i$

Notice 2: for a linear regression the optimal parameters (by OLS) can be estimated explicitly by simple matrix multiplication. 

$$
\begin{array}{l}
 \hat{\beta} & = (X^T X)^{-1}~~X^T ~ y \\
 (p \times 1) & = (p \times p)~(p\times n)~(n \times 1)
\end{array}
$$



## Example: Factorial Variables
For a *factorial* variables (such as Species), the different levels can be encoded as binary dummy variables. There are many possible choices, but a simple design matrix is
$$
X = 
\left(
\begin{array}{ccc}
L_1 & L_2 & L_3 \\
  1 & 0 & 0 \\
  1 & 0 & 0 \\
  \vdots & \vdots & \vdots \\
  0 & 1 & 0 \\
  0 & 1 & 0 \\
  \vdots & \vdots & \vdots \\
  0 & 0 & 1 \\
\end{array}
\right)
$$

$$ 
y_i = \mu_0 X_{i0} + \mu_1 X_{i1} + \mu_2 X_{i2}
$$

Each $X_{ik} \in \{0,1\}$ is binary indicator variable denoting the
specific factor level (e.g. Species=setosa) for a given observation $i$.
Since $\sum_k X_{ik}=1$ the estimates will be made for each 
$y_i \in (\mu_0, \mu_1, \mu_2)$


Alternatively, and in analogy with the intersect, one may also chose one specific reference level (e.g $L_1$) and encode differences with respect to this level

$$
X = 
\left(
\begin{array}{ccc}
L_1 & L_2 & L_3 \\
  1 & 0 & 0 \\
  1 & 0 & 0 \\
  \vdots & \vdots & \vdots \\
  1 & 1 & 0 \\
  1 & 1 & 0 \\
  \vdots & \vdots & \vdots \\
  1 & 0 & 1 \\
\end{array}
\right)
$$

This corresponds to a different interpretation of the parameters 
$$ 
\begin{array}{ll}
y_i &= \mu_0 + (\mu_1-\mu_0) X_{i1} + (\mu_2 - \mu_0) X_{i2} \\
    &=\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2}
\end{array}
$$

In fact, this is the default in R. Fitted parameters $\beta$ then correspond to the differences (the 'contrasts') with respect to some reference level. 
Often (but not always) this is more interesting than the estimated levels themselves.

More generally we can apply any reversable transformation $B$ to $X$ which will change the interpretation of the fitted parameters.

$$
y = X \cdot \mu = X \times B \times B^{-1} \mu = X^\prime \beta\\
$$

$B$ is non-singular and often called the *coding matrix* ($p \times p$).
It transforms both $X \to X^\prime$ and $\mu \to \beta$.

Notice 1: When $B=[1_p, B^\ast]$ then $X^\prime = [p_n, X B^\ast]$.
This amounts to the common scenario where we define a reference level.
In these cases $B^\ast$ is also called the coding matrix ($p \times (p-1)$.

These are the coding matrices defined in R
```{r}
contr.treatment(3) # default
contr.sum(3)
```

**Summary:** 
The dummy encoding of factorial levels is somewhat arbitrary and changes the interpretation of the parameters.

Averaging vector: $\sum_i^p v_i = v^T 1_p = 1$

Contrast vector: $\sum_i^p v_i =  v^T 1_p = 0$

```{r}
#| label: anova_lm
lm_res=lm(Petal.Width ~ Species, data=iris)
aov_res=aov(Petal.Width ~ Species, data=iris)

# notice that Std.error on Speciesversicolor=Speciesvriginical (by homoscedastity assumption)
summary(lm_res) 
anova(lm_res)
summary(aov_res)
```

Species ist a factor variable with $k=3$ levels.
Therefore there are $k-1=2$ orthogonal contrasts.
```{r}
#| label: contrasts1
data(iris) # ensure to start with default iris

class(iris$Species)
summary(iris$Species)
levels(iris$Species)
# contrasts are a property of factorial variable
contrasts(iris$Species)

# re-sorting level order
iris$Species = relevel(iris$Species, "virginica")
contrasts(iris$Species)
```

## Two factorial
The above generalizes to multiple factors and level combinations 
Let's assume that the iris data was collected at two different locations $(A,B)$.
(This is not the case, but I want to introduce a simple second factor).
In this case we have $k_1=3$ levels for factor Species and $k_B=2$ levels for factor location. In total there will be $p=k_A k_B = 6$ level combinations and the expectations for each combinations may be visualized like this

$$
Ey = 
\left(
\begin{array}{lccc}
           & A & B \\
Setosa     & \mu_{00} & \mu_{01} \\
Versicolor & \mu_{10} & \mu_{11} \\
Virginica  & \mu_{20} & \mu_{21} \\
\end{array}
\right)
$$

```{r}
#| label: 2way_rand
df = iris

# define vector of length nrow(iris) with nr regions
nr=2
regions=rep(LETTERS[1:nr], nrow(iris)/nr)
df$Region=sample(regions)                  # randomize regions

summary(aov(Petal.Width ~ Species + Region + Species*Region, data=df))
```

fake association of region with length
```{r}
#| label: 2way_assoc
nr=2
df = iris %>% group_by(Species) %>% arrange(Petal.Width, .by_group=TRUE)
df$Region = rep(rep(LETTERS[1:nr],each=50/nr),3)

df %>% ggplot(aes(x=Species, y=Petal.Width, col=Species, size=Region)) + geom_point()
summary(aov(Petal.Width ~ Species + Region + Species*Region, data=df))
```


## Reference

- https://cran.r-project.org/web/packages/codingMatrices/vignettes/codingMatrices.pdf